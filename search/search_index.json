{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"applybn","text":"<p>applybn \u2014 \u044d\u0442\u043e \u043c\u043d\u043e\u0433\u043e\u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a \u0441 \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u043c \u043a\u043e\u0434\u043e\u043c, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043d\u0430 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u044f\u0445 \u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445. \u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0438\u0434\u0435\u044f \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u0445 \u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439 \u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. </p>"},{"location":"api/anomaly_detection/scores/","title":"\u041e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u043a \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0435 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u0430","text":"<p>\u0414\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u044b - \u044d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440\u044b \u043d\u0430\u0434 \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u043c\u0438 \u043e\u0446\u0435\u043d\u043e\u043a. \u041e\u043d\u0438 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u0441\u043f\u043e\u0441\u043e\u0431\u044b \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0435.</p> <p>Warning</p> <p>\u041e\u0446\u0435\u043d\u043a\u0438 \u0438\u043c\u0435\u044e\u0442 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u0443\u044e \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443 \u0442\u0438\u043f\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445.</p> <p><code>applybn</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0434\u0432\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0433\u0440\u0443\u043f\u043f\u044b \u043e\u0446\u0435\u043d\u043e\u043a: \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u0438.</p> <p>\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0432 \u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</p>"},{"location":"api/anomaly_detection/scores/#_2","title":"\u041e\u0446\u0435\u043d\u043a\u0430","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for implementing scoring mechanisms.</p> Source code in <code>applybn/anomaly_detection/scores/score.py</code> <pre><code>class Score(ABC):\n    \"\"\"\n    An abstract base class for implementing scoring mechanisms.\n    \"\"\"\n\n    def __init__(self, verbose: int = 1):\n        \"\"\"\n        Initializes the Score object.\n\n        Args:\n            verbose: The verbosity level for logging. Default is 1.\n        \"\"\"\n        self.verbose = verbose\n\n    @abstractmethod\n    def score(self, X: pd.DataFrame):\n        \"\"\"\n        Abstract method to compute scores for the given input data.\n\n        Args:\n            X: The input data to be scored.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.score.Score.__init__","title":"<code>__init__(verbose=1)</code>","text":"<p>Initializes the Score object.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>int</code> <p>The verbosity level for logging. Default is 1.</p> <code>1</code> Source code in <code>applybn/anomaly_detection/scores/score.py</code> <pre><code>def __init__(self, verbose: int = 1):\n    \"\"\"\n    Initializes the Score object.\n\n    Args:\n        verbose: The verbosity level for logging. Default is 1.\n    \"\"\"\n    self.verbose = verbose\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.score.Score.score","title":"<code>score(X)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to compute scores for the given input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data to be scored.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>applybn/anomaly_detection/scores/score.py</code> <pre><code>@abstractmethod\ndef score(self, X: pd.DataFrame):\n    \"\"\"\n    Abstract method to compute scores for the given input data.\n\n    Args:\n        X: The input data to be scored.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/anomaly_detection/scores/#_3","title":"\u041e\u0446\u0435\u043d\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u0438","text":""},{"location":"api/anomaly_detection/scores/#_4","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432","text":"<p>               Bases: <code>ProximityBasedScore</code></p> <p>A class for computing outlier scores using the Local Outlier Factor (LOF) algorithm.</p> Source code in <code>applybn/anomaly_detection/scores/proximity_based.py</code> <pre><code>class LocalOutlierScore(ProximityBasedScore):\n    \"\"\"\n    A class for computing outlier scores using the Local Outlier Factor (LOF) algorithm.\n    \"\"\"\n\n    def __init__(self, proximity_steps: int = 5, verbose: int = 1, **kwargs):\n        \"\"\"\n        Initializes the LocalOutlierScore object.\n\n        Args:\n            proximity_steps: The number of proximity steps to perform. Default is 5.\n            verbose: The verbosity level for logging. Default is 1.\n            **kwargs: Additional parameters for the Local Outlier Factor algorithm.\n        \"\"\"\n        super().__init__(proximity_steps=proximity_steps, verbose=verbose)\n        self.params = kwargs\n\n    def local_score(self, X: pd.DataFrame):\n        \"\"\"\n        Computes the local outlier scores for the given data using the LOF algorithm.\n\n        Args:\n            X: The input data.\n\n        Returns:\n            np.ndarray: An array of negative outlier factors, where higher values indicate more abnormal data points.\n        \"\"\"\n        clf = LocalOutlierFactor(**self.params)\n        clf.fit(X)\n        # The higher the value, the more abnormal the data point\n        return np.negative(clf.negative_outlier_factor_)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.proximity_based.LocalOutlierScore.__init__","title":"<code>__init__(proximity_steps=5, verbose=1, **kwargs)</code>","text":"<p>Initializes the LocalOutlierScore object.</p> <p>Parameters:</p> Name Type Description Default <code>proximity_steps</code> <code>int</code> <p>The number of proximity steps to perform. Default is 5.</p> <code>5</code> <code>verbose</code> <code>int</code> <p>The verbosity level for logging. Default is 1.</p> <code>1</code> <code>**kwargs</code> <p>Additional parameters for the Local Outlier Factor algorithm.</p> <code>{}</code> Source code in <code>applybn/anomaly_detection/scores/proximity_based.py</code> <pre><code>def __init__(self, proximity_steps: int = 5, verbose: int = 1, **kwargs):\n    \"\"\"\n    Initializes the LocalOutlierScore object.\n\n    Args:\n        proximity_steps: The number of proximity steps to perform. Default is 5.\n        verbose: The verbosity level for logging. Default is 1.\n        **kwargs: Additional parameters for the Local Outlier Factor algorithm.\n    \"\"\"\n    super().__init__(proximity_steps=proximity_steps, verbose=verbose)\n    self.params = kwargs\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.proximity_based.LocalOutlierScore.local_score","title":"<code>local_score(X)</code>","text":"<p>Computes the local outlier scores for the given data using the LOF algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: An array of negative outlier factors, where higher values indicate more abnormal data points.</p> Source code in <code>applybn/anomaly_detection/scores/proximity_based.py</code> <pre><code>def local_score(self, X: pd.DataFrame):\n    \"\"\"\n    Computes the local outlier scores for the given data using the LOF algorithm.\n\n    Args:\n        X: The input data.\n\n    Returns:\n        np.ndarray: An array of negative outlier factors, where higher values indicate more abnormal data points.\n    \"\"\"\n    clf = LocalOutlierFactor(**self.params)\n    clf.fit(X)\n    # The higher the value, the more abnormal the data point\n    return np.negative(clf.negative_outlier_factor_)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#isolation-forest","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 Isolation Forest","text":"<p>               Bases: <code>ProximityBasedScore</code></p> <p>A class for computing outlier scores using the Isolation Forest algorithm.</p> Source code in <code>applybn/anomaly_detection/scores/proximity_based.py</code> <pre><code>class IsolationForestScore(ProximityBasedScore):\n    \"\"\"\n    A class for computing outlier scores using the Isolation Forest algorithm.\n    \"\"\"\n\n    def __init__(self, proximity_steps: int = 5, verbose: int = 1, **kwargs):\n        \"\"\"\n        Initializes the IsolationForestScore object.\n\n        Args:\n            **kwargs: Additional parameters for the Isolation Forest algorithm.\n        \"\"\"\n        super().__init__(verbose=verbose, proximity_steps=proximity_steps)\n        self.params = kwargs\n\n    def local_score(self, X: pd.DataFrame):\n        \"\"\"\n        Computes the outlier scores for the given data using the Isolation Forest algorithm.\n\n        Args:\n            X: The input data.\n\n        Returns:\n            np.ndarray: An array of negative decision function values, where higher values indicate more abnormal data points.\n        \"\"\"\n        clf = IsolationForest(**self.params)\n        clf.fit(X)\n        return np.negative(clf.decision_function(X))\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.proximity_based.IsolationForestScore.__init__","title":"<code>__init__(proximity_steps=5, verbose=1, **kwargs)</code>","text":"<p>Initializes the IsolationForestScore object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional parameters for the Isolation Forest algorithm.</p> <code>{}</code> Source code in <code>applybn/anomaly_detection/scores/proximity_based.py</code> <pre><code>def __init__(self, proximity_steps: int = 5, verbose: int = 1, **kwargs):\n    \"\"\"\n    Initializes the IsolationForestScore object.\n\n    Args:\n        **kwargs: Additional parameters for the Isolation Forest algorithm.\n    \"\"\"\n    super().__init__(verbose=verbose, proximity_steps=proximity_steps)\n    self.params = kwargs\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.proximity_based.IsolationForestScore.local_score","title":"<code>local_score(X)</code>","text":"<p>Computes the outlier scores for the given data using the Isolation Forest algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: An array of negative decision function values, where higher values indicate more abnormal data points.</p> Source code in <code>applybn/anomaly_detection/scores/proximity_based.py</code> <pre><code>def local_score(self, X: pd.DataFrame):\n    \"\"\"\n    Computes the outlier scores for the given data using the Isolation Forest algorithm.\n\n    Args:\n        X: The input data.\n\n    Returns:\n        np.ndarray: An array of negative decision function values, where higher values indicate more abnormal data points.\n    \"\"\"\n    clf = IsolationForest(**self.params)\n    clf.fit(X)\n    return np.negative(clf.decision_function(X))\n</code></pre>"},{"location":"api/anomaly_detection/scores/#_5","title":"\u041e\u0446\u0435\u043d\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","text":"<p>               Bases: <code>Score</code></p> <p>A generic score class that computes scores based on a provided model. Model must implement the predict_proba method.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>class ModelBasedScore(Score):\n    \"\"\"\n    A generic score class that computes scores based on a provided model.\n    Model must implement the predict_proba method.\n    \"\"\"\n\n    def __init__(self, model):\n        \"\"\"\n        Initializes the ModelBasedScore object.\n\n        Args:\n            model: The model used to compute probabilities for scoring.\n        \"\"\"\n        super().__init__()\n        self.model = model\n\n    def score(self, X) -&gt; np.ndarray:\n        \"\"\"\n        Computes the score for the input data using the model's predicted probabilities.\n\n        Args:\n            X: The input data to be scored.\n\n        Returns:\n            np.ndarray: The predicted probabilities for the input data.\n        \"\"\"\n        if not hasattr(self.model, \"predict_proba\"):\n            raise AttributeError(\"The model does not have a predict_proba method.\")\n        probas = self.model.predict_proba(X)\n\n        if isinstance(probas, pd.Series):\n            return probas.values\n        if isinstance(probas, np.ndarray):\n            return probas\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.ModelBasedScore.__init__","title":"<code>__init__(model)</code>","text":"<p>Initializes the ModelBasedScore object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model used to compute probabilities for scoring.</p> required Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def __init__(self, model):\n    \"\"\"\n    Initializes the ModelBasedScore object.\n\n    Args:\n        model: The model used to compute probabilities for scoring.\n    \"\"\"\n    super().__init__()\n    self.model = model\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.ModelBasedScore.score","title":"<code>score(X)</code>","text":"<p>Computes the score for the input data using the model's predicted probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input data to be scored.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The predicted probabilities for the input data.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def score(self, X) -&gt; np.ndarray:\n    \"\"\"\n    Computes the score for the input data using the model's predicted probabilities.\n\n    Args:\n        X: The input data to be scored.\n\n    Returns:\n        np.ndarray: The predicted probabilities for the input data.\n    \"\"\"\n    if not hasattr(self.model, \"predict_proba\"):\n        raise AttributeError(\"The model does not have a predict_proba method.\")\n    probas = self.model.predict_proba(X)\n\n    if isinstance(probas, pd.Series):\n        return probas.values\n    if isinstance(probas, np.ndarray):\n        return probas\n</code></pre>"},{"location":"api/anomaly_detection/scores/#bn","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 BN","text":"<p>               Bases: <code>Score</code></p> <p>A score class based on a Bayesian network (BN).</p> <p>Attributes:</p> Name Type Description <code>bn</code> <p>The Bayesian network used for scoring.</p> <code>encoding</code> <p>The encoding for discrete variables.</p> <code>child_nodes</code> <p>The child nodes in the Bayesian network.</p> <code>verbose</code> <p>The verbosity level for logging.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>class BNBasedScore(Score):\n    \"\"\"\n    A score class based on a Bayesian network (BN).\n\n    Attributes:\n        bn: The Bayesian network used for scoring.\n        encoding: The encoding for discrete variables.\n        child_nodes: The child nodes in the Bayesian network.\n        verbose: The verbosity level for logging.\n    \"\"\"\n\n    def __init__(self, bn: bamt_network, encoding: dict, verbose: int = 1):\n        \"\"\"\n        Initializes the BNBasedScore object.\n\n        Args:\n            bn: The Bayesian network used for scoring.\n            encoding: The encoding for discrete variables.\n            verbose: The verbosity level for logging.\n        \"\"\"\n        super().__init__(verbose=verbose)\n        self.encoding = encoding\n        self.bn = bn\n\n        child_nodes = []\n        for column in bn.nodes_names:\n            if self.bn[column].disc_parents + self.bn[column].cont_parents:\n                child_nodes.append(column)\n\n        self.child_nodes = child_nodes\n\n    def local_score(self, X: pd.DataFrame, node_name: str):\n        \"\"\"\n        Computes the local score for a specific node in the Bayesian network.\n\n        Args:\n            X: The input data.\n            node_name : The name of the node to compute the score for.\n\n        Returns:\n            np.ndarray: An array of local scores for the specified node.\n        \"\"\"\n        node = self.bn[node_name]\n        diff = []\n        parents = node.cont_parents + node.disc_parents\n        parent_dtypes = X[parents].dtypes.to_dict()\n\n        for i in X.index:\n            node_value = X.loc[i, node_name]\n            row_df = X.loc[[i], parents].astype(parent_dtypes)\n            pvalues = row_df.to_dict(\"records\")[0]\n            cond_dist = self.bn.get_dist(node_name, pvals=pvalues)\n\n            if \"gaussian\" in cond_dist.node_type:\n                cond_mean, std = cond_dist.get()\n            else:\n                probs, classes = cond_dist.get()\n                match self.bn.descriptor[\"types\"][node_name]:\n                    case \"disc_num\":\n                        classes_ = [int(class_name) for class_name in classes]\n                    case \"disc\":\n                        classes_ = np.asarray(\n                            [\n                                self.encoding[node_name][class_name]\n                                for class_name in classes\n                            ]\n                        )\n                cond_mean = classes_ @ np.asarray(probs).T\n\n            match self.bn.descriptor[\"types\"][node_name]:\n                case \"disc_num\":\n                    diff.append((node_value - cond_mean))\n                case \"disc\":\n                    diff.append(self.encoding[node_name][node_value] - cond_mean)\n                case \"cont\":\n                    diff.append((node_value - cond_mean) / std)\n\n        return np.asarray(diff).reshape(-1, 1)\n\n    def score(self, X: pd.DataFrame):\n        \"\"\"\n        Computes the scores for all child nodes in the Bayesian network.\n\n        Args:\n            X: The input data.\n\n        Returns:\n            np.ndarray: A 2D array of scores for all child nodes.\n        \"\"\"\n        if self.verbose &gt;= 1:\n            model_iterator = tqdm(self.child_nodes, desc=\"Model\")\n        else:\n            model_iterator = self.child_nodes\n\n        model_factors = []\n        for child_node in model_iterator:\n            model_factors.append(self.local_score(X, child_node))\n\n        return np.hstack(model_factors)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.BNBasedScore.__init__","title":"<code>__init__(bn, encoding, verbose=1)</code>","text":"<p>Initializes the BNBasedScore object.</p> <p>Parameters:</p> Name Type Description Default <code>bn</code> <code>bamt_network</code> <p>The Bayesian network used for scoring.</p> required <code>encoding</code> <code>dict</code> <p>The encoding for discrete variables.</p> required <code>verbose</code> <code>int</code> <p>The verbosity level for logging.</p> <code>1</code> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def __init__(self, bn: bamt_network, encoding: dict, verbose: int = 1):\n    \"\"\"\n    Initializes the BNBasedScore object.\n\n    Args:\n        bn: The Bayesian network used for scoring.\n        encoding: The encoding for discrete variables.\n        verbose: The verbosity level for logging.\n    \"\"\"\n    super().__init__(verbose=verbose)\n    self.encoding = encoding\n    self.bn = bn\n\n    child_nodes = []\n    for column in bn.nodes_names:\n        if self.bn[column].disc_parents + self.bn[column].cont_parents:\n            child_nodes.append(column)\n\n    self.child_nodes = child_nodes\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.BNBasedScore.local_score","title":"<code>local_score(X, node_name)</code>","text":"<p>Computes the local score for a specific node in the Bayesian network.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <code>node_name</code> <p>The name of the node to compute the score for.</p> required <p>Returns:</p> Type Description <p>np.ndarray: An array of local scores for the specified node.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def local_score(self, X: pd.DataFrame, node_name: str):\n    \"\"\"\n    Computes the local score for a specific node in the Bayesian network.\n\n    Args:\n        X: The input data.\n        node_name : The name of the node to compute the score for.\n\n    Returns:\n        np.ndarray: An array of local scores for the specified node.\n    \"\"\"\n    node = self.bn[node_name]\n    diff = []\n    parents = node.cont_parents + node.disc_parents\n    parent_dtypes = X[parents].dtypes.to_dict()\n\n    for i in X.index:\n        node_value = X.loc[i, node_name]\n        row_df = X.loc[[i], parents].astype(parent_dtypes)\n        pvalues = row_df.to_dict(\"records\")[0]\n        cond_dist = self.bn.get_dist(node_name, pvals=pvalues)\n\n        if \"gaussian\" in cond_dist.node_type:\n            cond_mean, std = cond_dist.get()\n        else:\n            probs, classes = cond_dist.get()\n            match self.bn.descriptor[\"types\"][node_name]:\n                case \"disc_num\":\n                    classes_ = [int(class_name) for class_name in classes]\n                case \"disc\":\n                    classes_ = np.asarray(\n                        [\n                            self.encoding[node_name][class_name]\n                            for class_name in classes\n                        ]\n                    )\n            cond_mean = classes_ @ np.asarray(probs).T\n\n        match self.bn.descriptor[\"types\"][node_name]:\n            case \"disc_num\":\n                diff.append((node_value - cond_mean))\n            case \"disc\":\n                diff.append(self.encoding[node_name][node_value] - cond_mean)\n            case \"cont\":\n                diff.append((node_value - cond_mean) / std)\n\n    return np.asarray(diff).reshape(-1, 1)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.BNBasedScore.score","title":"<code>score(X)</code>","text":"<p>Computes the scores for all child nodes in the Bayesian network.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: A 2D array of scores for all child nodes.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def score(self, X: pd.DataFrame):\n    \"\"\"\n    Computes the scores for all child nodes in the Bayesian network.\n\n    Args:\n        X: The input data.\n\n    Returns:\n        np.ndarray: A 2D array of scores for all child nodes.\n    \"\"\"\n    if self.verbose &gt;= 1:\n        model_iterator = tqdm(self.child_nodes, desc=\"Model\")\n    else:\n        model_iterator = self.child_nodes\n\n    model_factors = []\n    for child_node in model_iterator:\n        model_factors.append(self.local_score(X, child_node))\n\n    return np.hstack(model_factors)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#iqr","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 IQR","text":"<p>               Bases: <code>BNBasedScore</code></p> <p>A score class that uses the Interquartile Range (IQR) for anomaly detection.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>class IQRBasedScore(BNBasedScore):\n    \"\"\"\n    A score class that uses the Interquartile Range (IQR) for anomaly detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        bn: bamt_network,\n        encoding: dict,\n        iqr_sensivity: float = 1.0,\n        verbose: int = 1,\n    ):\n        \"\"\"\n        Initializes the IQRBasedScore object.\n\n        Args:\n            bn: The Bayesian network used for scoring.\n            encoding: The encoding for discrete variables.\n            iqr_sensivity: The sensitivity factor for IQR-based scoring.\n            verbose: The verbosity level for logging.\n        \"\"\"\n        super().__init__(bn=bn, encoding=encoding, verbose=verbose)\n        self.iqr_sensivity = iqr_sensivity\n\n    @staticmethod\n    def score_iqr(\n        upper: float, lower: float, y: float, max_distance: float, min_distance: float\n    ):\n        \"\"\"\n        Computes the IQR-based score for a given value.\n\n        Args:\n            upper: The upper bound of the IQR.\n            lower: The lower bound of the IQR.\n            y: The value to score.\n            max_distance: The maximum distance for scaling.\n            min_distance: The minimum distance for scaling.\n\n        Raises:\n            ValueError: If the closest value does not match either upper or lower bound.\n\n        Returns:\n            float: The IQR-based score.\n        \"\"\"\n        if lower &lt; y &lt;= upper:\n            return 0\n\n        closest_value = min([upper, lower], key=lambda x: abs(x - y))\n        current_distance = abs(closest_value - y)\n\n        if closest_value == upper:\n            ref_distance = max_distance\n        elif closest_value == lower:\n            ref_distance = min_distance\n        else:\n            raise ValueError(\n                \"Unexpected state: closest_value does not match either upper or lower bound.\"\n            )\n\n        return min(1, current_distance / abs(ref_distance))\n\n    def local_score(self, X: pd.DataFrame, node_name: str):\n        \"\"\"\n        Computes the local IQR-based score for a specific node.\n\n        Args:\n            X: The input data.\n            node_name : The name of the node to compute the score for.\n\n        Returns:\n            np.ndarray: An array of local scores for the specified node.\n        \"\"\"\n        node = self.bn[node_name]\n        parents = node.cont_parents + node.disc_parents\n        parent_dtypes = X[parents].dtypes.to_dict()\n\n        scores = []\n        for i in X.index:\n            row_df = X.loc[[i], parents].astype(parent_dtypes)\n            pvalues = row_df.to_dict(\"records\")[0]\n            dist = self.bn.get_dist(node_name, pvals=pvalues).get(with_gaussian=True)\n\n            X_value = X.loc[i, node_name]\n            q25 = dist.ppf(0.25)\n            q75 = dist.ppf(0.75)\n            iqr = q75 - q25\n\n            lower_bound = q25 - iqr * self.iqr_sensivity\n            upper_bound = q75 + iqr * self.iqr_sensivity\n\n            scores.append(\n                self.score_iqr(\n                    upper_bound,\n                    lower_bound,\n                    X_value,\n                    max_distance=1 * X[node_name].max(),\n                    min_distance=1 * X[node_name].min(),\n                )\n            )\n\n        return np.asarray(scores).reshape(-1, 1)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.IQRBasedScore.__init__","title":"<code>__init__(bn, encoding, iqr_sensivity=1.0, verbose=1)</code>","text":"<p>Initializes the IQRBasedScore object.</p> <p>Parameters:</p> Name Type Description Default <code>bn</code> <code>bamt_network</code> <p>The Bayesian network used for scoring.</p> required <code>encoding</code> <code>dict</code> <p>The encoding for discrete variables.</p> required <code>iqr_sensivity</code> <code>float</code> <p>The sensitivity factor for IQR-based scoring.</p> <code>1.0</code> <code>verbose</code> <code>int</code> <p>The verbosity level for logging.</p> <code>1</code> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def __init__(\n    self,\n    bn: bamt_network,\n    encoding: dict,\n    iqr_sensivity: float = 1.0,\n    verbose: int = 1,\n):\n    \"\"\"\n    Initializes the IQRBasedScore object.\n\n    Args:\n        bn: The Bayesian network used for scoring.\n        encoding: The encoding for discrete variables.\n        iqr_sensivity: The sensitivity factor for IQR-based scoring.\n        verbose: The verbosity level for logging.\n    \"\"\"\n    super().__init__(bn=bn, encoding=encoding, verbose=verbose)\n    self.iqr_sensivity = iqr_sensivity\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.IQRBasedScore.local_score","title":"<code>local_score(X, node_name)</code>","text":"<p>Computes the local IQR-based score for a specific node.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <code>node_name</code> <p>The name of the node to compute the score for.</p> required <p>Returns:</p> Type Description <p>np.ndarray: An array of local scores for the specified node.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def local_score(self, X: pd.DataFrame, node_name: str):\n    \"\"\"\n    Computes the local IQR-based score for a specific node.\n\n    Args:\n        X: The input data.\n        node_name : The name of the node to compute the score for.\n\n    Returns:\n        np.ndarray: An array of local scores for the specified node.\n    \"\"\"\n    node = self.bn[node_name]\n    parents = node.cont_parents + node.disc_parents\n    parent_dtypes = X[parents].dtypes.to_dict()\n\n    scores = []\n    for i in X.index:\n        row_df = X.loc[[i], parents].astype(parent_dtypes)\n        pvalues = row_df.to_dict(\"records\")[0]\n        dist = self.bn.get_dist(node_name, pvals=pvalues).get(with_gaussian=True)\n\n        X_value = X.loc[i, node_name]\n        q25 = dist.ppf(0.25)\n        q75 = dist.ppf(0.75)\n        iqr = q75 - q25\n\n        lower_bound = q25 - iqr * self.iqr_sensivity\n        upper_bound = q75 + iqr * self.iqr_sensivity\n\n        scores.append(\n            self.score_iqr(\n                upper_bound,\n                lower_bound,\n                X_value,\n                max_distance=1 * X[node_name].max(),\n                min_distance=1 * X[node_name].min(),\n            )\n        )\n\n    return np.asarray(scores).reshape(-1, 1)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.IQRBasedScore.score_iqr","title":"<code>score_iqr(upper, lower, y, max_distance, min_distance)</code>  <code>staticmethod</code>","text":"<p>Computes the IQR-based score for a given value.</p> <p>Parameters:</p> Name Type Description Default <code>upper</code> <code>float</code> <p>The upper bound of the IQR.</p> required <code>lower</code> <code>float</code> <p>The lower bound of the IQR.</p> required <code>y</code> <code>float</code> <p>The value to score.</p> required <code>max_distance</code> <code>float</code> <p>The maximum distance for scaling.</p> required <code>min_distance</code> <code>float</code> <p>The minimum distance for scaling.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the closest value does not match either upper or lower bound.</p> <p>Returns:</p> Name Type Description <code>float</code> <p>The IQR-based score.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>@staticmethod\ndef score_iqr(\n    upper: float, lower: float, y: float, max_distance: float, min_distance: float\n):\n    \"\"\"\n    Computes the IQR-based score for a given value.\n\n    Args:\n        upper: The upper bound of the IQR.\n        lower: The lower bound of the IQR.\n        y: The value to score.\n        max_distance: The maximum distance for scaling.\n        min_distance: The minimum distance for scaling.\n\n    Raises:\n        ValueError: If the closest value does not match either upper or lower bound.\n\n    Returns:\n        float: The IQR-based score.\n    \"\"\"\n    if lower &lt; y &lt;= upper:\n        return 0\n\n    closest_value = min([upper, lower], key=lambda x: abs(x - y))\n    current_distance = abs(closest_value - y)\n\n    if closest_value == upper:\n        ref_distance = max_distance\n    elif closest_value == lower:\n        ref_distance = min_distance\n    else:\n        raise ValueError(\n            \"Unexpected state: closest_value does not match either upper or lower bound.\"\n        )\n\n    return min(1, current_distance / abs(ref_distance))\n</code></pre>"},{"location":"api/anomaly_detection/scores/#_6","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439","text":"<p>               Bases: <code>BNBasedScore</code></p> <p>A score class that uses conditional probability ratios for anomaly detection.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>class CondRatioScore(BNBasedScore):\n    \"\"\"\n    A score class that uses conditional probability ratios for anomaly detection.\n    \"\"\"\n\n    def __init__(self, bn: bamt_network, encoding: dict, verbose: int = 1):\n        \"\"\"\n        Initializes the CondRatioScore object.\n\n        Args:\n            bn: The Bayesian network used for scoring.\n            encoding: The encoding for discrete variables.\n            verbose: The verbosity level for logging.\n        \"\"\"\n        super(CondRatioScore, self).__init__(bn=bn, encoding=encoding, verbose=verbose)\n\n    def local_score(self, X: pd.DataFrame, node_name: str):\n        \"\"\"\n        Computes the local conditional ratio score for a specific node.\n\n        Args:\n            X: The input data.\n            node_name: The name of the node to compute the score for.\n\n        Returns:\n            np.ndarray: An array of local scores for the specified node.\n        \"\"\"\n        node = self.bn[node_name]\n        diff = []\n        parents = node.cont_parents + node.disc_parents\n        parent_dtypes = X[parents].dtypes.to_dict()\n\n        for i in X.index:\n            row_df = X.loc[[i], parents].astype(parent_dtypes)\n            pvalues = row_df.to_dict(\"records\")[0]\n            node_value = X.loc[i, node_name]\n            cond_dist = self.bn.get_dist(node_name, pvals=pvalues).get()\n\n            diff.append(self.score_proba_ratio(X[node_name], node_value, cond_dist))\n\n        return np.asarray(diff).reshape(-1, 1)\n\n    @staticmethod\n    def score_proba_ratio(sample: pd.Series, X_value: str, cond_dist: tuple):\n        \"\"\"\n        Computes the conditional probability ratio score.\n\n        Args:\n            sample: The sample data.\n            X_value: The value to score.\n            cond_dist: The conditional distribution.\n\n        Returns:\n            float: The conditional probability ratio score.\n        \"\"\"\n        cond_probs, values = cond_dist\n        marginal_prob = sample.value_counts(normalize=True)[X_value]\n\n        index = values.index(str(X_value))\n        cond_prob = cond_probs[index]\n\n        if not np.isfinite(marginal_prob / cond_prob):\n            return np.nan\n\n        return min(1, marginal_prob / cond_prob)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.CondRatioScore.__init__","title":"<code>__init__(bn, encoding, verbose=1)</code>","text":"<p>Initializes the CondRatioScore object.</p> <p>Parameters:</p> Name Type Description Default <code>bn</code> <code>bamt_network</code> <p>The Bayesian network used for scoring.</p> required <code>encoding</code> <code>dict</code> <p>The encoding for discrete variables.</p> required <code>verbose</code> <code>int</code> <p>The verbosity level for logging.</p> <code>1</code> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def __init__(self, bn: bamt_network, encoding: dict, verbose: int = 1):\n    \"\"\"\n    Initializes the CondRatioScore object.\n\n    Args:\n        bn: The Bayesian network used for scoring.\n        encoding: The encoding for discrete variables.\n        verbose: The verbosity level for logging.\n    \"\"\"\n    super(CondRatioScore, self).__init__(bn=bn, encoding=encoding, verbose=verbose)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.CondRatioScore.local_score","title":"<code>local_score(X, node_name)</code>","text":"<p>Computes the local conditional ratio score for a specific node.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <code>node_name</code> <code>str</code> <p>The name of the node to compute the score for.</p> required <p>Returns:</p> Type Description <p>np.ndarray: An array of local scores for the specified node.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def local_score(self, X: pd.DataFrame, node_name: str):\n    \"\"\"\n    Computes the local conditional ratio score for a specific node.\n\n    Args:\n        X: The input data.\n        node_name: The name of the node to compute the score for.\n\n    Returns:\n        np.ndarray: An array of local scores for the specified node.\n    \"\"\"\n    node = self.bn[node_name]\n    diff = []\n    parents = node.cont_parents + node.disc_parents\n    parent_dtypes = X[parents].dtypes.to_dict()\n\n    for i in X.index:\n        row_df = X.loc[[i], parents].astype(parent_dtypes)\n        pvalues = row_df.to_dict(\"records\")[0]\n        node_value = X.loc[i, node_name]\n        cond_dist = self.bn.get_dist(node_name, pvals=pvalues).get()\n\n        diff.append(self.score_proba_ratio(X[node_name], node_value, cond_dist))\n\n    return np.asarray(diff).reshape(-1, 1)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.CondRatioScore.score_proba_ratio","title":"<code>score_proba_ratio(sample, X_value, cond_dist)</code>  <code>staticmethod</code>","text":"<p>Computes the conditional probability ratio score.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Series</code> <p>The sample data.</p> required <code>X_value</code> <code>str</code> <p>The value to score.</p> required <code>cond_dist</code> <code>tuple</code> <p>The conditional distribution.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The conditional probability ratio score.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>@staticmethod\ndef score_proba_ratio(sample: pd.Series, X_value: str, cond_dist: tuple):\n    \"\"\"\n    Computes the conditional probability ratio score.\n\n    Args:\n        sample: The sample data.\n        X_value: The value to score.\n        cond_dist: The conditional distribution.\n\n    Returns:\n        float: The conditional probability ratio score.\n    \"\"\"\n    cond_probs, values = cond_dist\n    marginal_prob = sample.value_counts(normalize=True)[X_value]\n\n    index = values.index(str(X_value))\n    cond_prob = cond_probs[index]\n\n    if not np.isfinite(marginal_prob / cond_prob):\n        return np.nan\n\n    return min(1, marginal_prob / cond_prob)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#iqr_1","title":"\u041a\u043e\u043c\u0431\u0438\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 IQR \u0438 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439","text":"<p>               Bases: <code>BNBasedScore</code></p> <p>A score class that combines IQR-based scoring and probability ratio scoring for anomaly detection.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>class CombinedIQRandProbRatioScore(BNBasedScore):\n    \"\"\"\n    A score class that combines IQR-based scoring and probability ratio scoring for anomaly detection.\n    \"\"\"\n\n    def __init__(\n        self, bn: bamt_network, encoding: dict, scores: dict, verbose: int = 1\n    ):\n        \"\"\"\n        Initializes the CombinedIQRandProbRatioScore object.\n\n        Args:\n            bn: The Bayesian network used for scoring.\n            encoding: The encoding for discrete variables.\n            scores: A dictionary containing scoring objects for continuous and discrete variables.\n            verbose: The verbosity level for logging.\n        \"\"\"\n        super(CombinedIQRandProbRatioScore, self).__init__(\n            bn=bn, encoding=encoding, verbose=verbose\n        )\n        self.scores = scores\n\n    def local_score(self, X: pd.DataFrame, node_name: str):\n        \"\"\"\n        Computes the local score for a specific node by combining IQR-based and probability ratio scoring.\n\n        Args:\n            X: The input data.\n            node_name: The name of the node to compute the score for.\n\n        Returns:\n            np.ndarray: An array of local scores for the specified node.\n        \"\"\"\n        node = self.bn[node_name]\n        iqr_sensivity = self.scores[\"cont\"].iqr_sensivity\n        parents = node.cont_parents + node.disc_parents\n        parent_dtypes = X[parents].dtypes.to_dict()\n\n        scores = []\n        for i in X.index:\n            row_df = X.loc[[i], parents].astype(parent_dtypes)\n            pvalues = row_df.to_dict(\"records\")[0]\n            X_value = X.loc[i, node_name]\n            dist = self.bn.get_dist(node_name, pvals=pvalues)\n\n            if \"gaussian\" in dist.node_type:\n                dist = dist.get(with_gaussian=True)\n                if dist.kwds[\"scale\"] == 0:\n                    scores.append(0)\n                    continue\n\n                q25 = dist.ppf(0.25)\n                q75 = dist.ppf(0.75)\n                iqr = q75 - q25\n\n                lower_bound = q25 - iqr * iqr_sensivity\n                upper_bound = q75 + iqr * iqr_sensivity\n\n                scores.append(\n                    self.scores[\"cont\"].score_iqr(\n                        upper_bound,\n                        lower_bound,\n                        X_value,\n                        max_distance=1 * X[node_name].max(),\n                        min_distance=1 * X[node_name].min(),\n                    )\n                )\n            else:\n                dist = dist.get()\n                scores.append(\n                    self.scores[\"disc\"].score_proba_ratio(X[node_name], X_value, dist)\n                )\n\n        return np.asarray(scores).reshape(-1, 1)\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.CombinedIQRandProbRatioScore.__init__","title":"<code>__init__(bn, encoding, scores, verbose=1)</code>","text":"<p>Initializes the CombinedIQRandProbRatioScore object.</p> <p>Parameters:</p> Name Type Description Default <code>bn</code> <code>bamt_network</code> <p>The Bayesian network used for scoring.</p> required <code>encoding</code> <code>dict</code> <p>The encoding for discrete variables.</p> required <code>scores</code> <code>dict</code> <p>A dictionary containing scoring objects for continuous and discrete variables.</p> required <code>verbose</code> <code>int</code> <p>The verbosity level for logging.</p> <code>1</code> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def __init__(\n    self, bn: bamt_network, encoding: dict, scores: dict, verbose: int = 1\n):\n    \"\"\"\n    Initializes the CombinedIQRandProbRatioScore object.\n\n    Args:\n        bn: The Bayesian network used for scoring.\n        encoding: The encoding for discrete variables.\n        scores: A dictionary containing scoring objects for continuous and discrete variables.\n        verbose: The verbosity level for logging.\n    \"\"\"\n    super(CombinedIQRandProbRatioScore, self).__init__(\n        bn=bn, encoding=encoding, verbose=verbose\n    )\n    self.scores = scores\n</code></pre>"},{"location":"api/anomaly_detection/scores/#applybn.anomaly_detection.scores.model_based.CombinedIQRandProbRatioScore.local_score","title":"<code>local_score(X, node_name)</code>","text":"<p>Computes the local score for a specific node by combining IQR-based and probability ratio scoring.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <code>node_name</code> <code>str</code> <p>The name of the node to compute the score for.</p> required <p>Returns:</p> Type Description <p>np.ndarray: An array of local scores for the specified node.</p> Source code in <code>applybn/anomaly_detection/scores/model_based.py</code> <pre><code>def local_score(self, X: pd.DataFrame, node_name: str):\n    \"\"\"\n    Computes the local score for a specific node by combining IQR-based and probability ratio scoring.\n\n    Args:\n        X: The input data.\n        node_name: The name of the node to compute the score for.\n\n    Returns:\n        np.ndarray: An array of local scores for the specified node.\n    \"\"\"\n    node = self.bn[node_name]\n    iqr_sensivity = self.scores[\"cont\"].iqr_sensivity\n    parents = node.cont_parents + node.disc_parents\n    parent_dtypes = X[parents].dtypes.to_dict()\n\n    scores = []\n    for i in X.index:\n        row_df = X.loc[[i], parents].astype(parent_dtypes)\n        pvalues = row_df.to_dict(\"records\")[0]\n        X_value = X.loc[i, node_name]\n        dist = self.bn.get_dist(node_name, pvals=pvalues)\n\n        if \"gaussian\" in dist.node_type:\n            dist = dist.get(with_gaussian=True)\n            if dist.kwds[\"scale\"] == 0:\n                scores.append(0)\n                continue\n\n            q25 = dist.ppf(0.25)\n            q75 = dist.ppf(0.75)\n            iqr = q75 - q25\n\n            lower_bound = q25 - iqr * iqr_sensivity\n            upper_bound = q75 + iqr * iqr_sensivity\n\n            scores.append(\n                self.scores[\"cont\"].score_iqr(\n                    upper_bound,\n                    lower_bound,\n                    X_value,\n                    max_distance=1 * X[node_name].max(),\n                    min_distance=1 * X[node_name].min(),\n                )\n            )\n        else:\n            dist = dist.get()\n            scores.append(\n                self.scores[\"disc\"].score_proba_ratio(X[node_name], X_value, dist)\n            )\n\n    return np.asarray(scores).reshape(-1, 1)\n</code></pre>"},{"location":"api/anomaly_detection/tDBN_data_formatter/","title":"\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 tDBN","text":"<p>TemporalDBNTransformer \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e \u043d\u0430\u0440\u0435\u0437\u043a\u0443 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432. \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u044e\u0431\u043e\u0439 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432.</p> <p>\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0432 \u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</p>"},{"location":"api/anomaly_detection/tDBN_data_formatter/#_1","title":"\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformer for creating a temporal windowed representation of tabular data for use with dynamic Bayesian networks (DBNs) or other time-dependent models.</p> <p>This transformer assumes that:</p> <ul> <li>The input data has already been discretized (e.g., using KBinsDiscretizer).</li> <li>Each row represents a time step for a given subject or unit.</li> <li>The data is ordered correctly in time.</li> </ul> <p>Example:</p> <pre><code>For input data:\n    f1   f2\n    0    10\n    1    11\n    2    12\n\nWith window=2, the output will be:\n    subject_id f1__0  f2__0  f1__1  f2__1\n        0        0      10     1      11\n        1        1      11     2      12\n</code></pre> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/data_formatter.py</code> <pre><code>class TemporalDBNTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer for creating a temporal windowed representation of tabular data\n    for use with dynamic Bayesian networks (DBNs) or other time-dependent models.\n\n    This transformer assumes that:\n\n    - The input data has already been discretized (e.g., using KBinsDiscretizer).\n    - Each row represents a time step for a given subject or unit.\n    - The data is ordered correctly in time.\n\n    Example:\n\n        For input data:\n            f1   f2\n            0    10\n            1    11\n            2    12\n\n        With window=2, the output will be:\n            subject_id f1__0  f2__0  f1__1  f2__1\n                0        0      10     1      11\n                1        1      11     2      12\n\n    \"\"\"\n\n    def __init__(\n        self,\n        window: float = 100,\n        include_label: bool = True,\n        stride: int = 1,\n        gathering_strategy: None | Literal[\"any\"] = \"any\",\n    ):\n        \"\"\"\n        Initialize the transformer.\n\n        Args:\n            window: If &lt; 1, the size of the sliding temporal window. If &gt; 1, number of rows in window.\n            stride: The size of the sliding temporal stride.\n            include_label: Whether to include the label (`y`) column in the transformed output.\n        \"\"\"\n        self.window = window\n        self.include_label = include_label\n        self.gathering_strategy = gathering_strategy\n        self.stride = stride\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        \"\"\"\n        Fit does nothing but is required by scikit-learn.\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Transforms the input DataFrame into a windowed representation with an optional stride.\n\n        Args:\n            X: Input features. Each row is a time step.\n            y: Labels corresponding to each row of X (e.g., anomaly labels). Must be the same length as X.\n\n        Returns:\n            A DataFrame where each row is a flattened sliding window of the input.\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input X must be a pandas DataFrame.\")\n\n        if self.include_label:\n            if y is None:\n                raise ValueError(\"Labels must be provided when include_label=True.\")\n            if len(X) != len(y):\n                raise ValueError(\"X and y must have the same number of rows.\")\n            X = X.copy()\n            X[\"anomaly\"] = y.values\n\n        values = X.values\n        n_rows, n_features = values.shape\n        if n_rows &lt; self.window:\n            raise ValueError(f\"Input data must have at least {self.window} rows.\")\n\n        if self.window &lt; 1:\n            window_size = int(self.window * n_rows)\n        else:\n            window_size = self.window\n\n        num_windows = (n_rows - window_size) // self.stride + 1\n\n        dfs = []\n        for i in range(0, num_windows * self.stride, self.stride):\n            window = values[i : i + window_size]\n            window_flat = window.flatten()\n            col_names = [\n                f\"{col}__{j}\" for j in range(1, window_size + 1) for col in X.columns\n            ]\n            part_df = pd.DataFrame([window_flat], columns=col_names)\n            dfs.append(part_df)\n\n        final_df = pd.concat(dfs, axis=0, ignore_index=True).reset_index(\n            names=[\"subject_id\"]\n        )\n\n        if self.gathering_strategy and self.include_label:\n            final_df = self.aggregate_anomalies(final_df)\n\n        return final_df\n\n    def aggregate_anomalies(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        This function aggregates anomalies. After transform there are a lot of cols with anomalies and\n        gathering them into one target vector is required.\n        Args:\n            X: Sliced data\n\n        Returns:\n            Dataframe with target vector\n        \"\"\"\n        X_ = X.copy()\n        match self.gathering_strategy:\n            case \"any\":\n                anomaly_cols_names = [\n                    col for col in X.columns if col.startswith(\"anomaly\")\n                ]\n                anomalies_cols = X[anomaly_cols_names]\n\n                aggregated = np.any(anomalies_cols, axis=1)\n                X_.drop(anomaly_cols_names, axis=1, inplace=True)\n                X_[\"anomaly\"] = aggregated.astype(int)\n                return X_\n            case _:\n                raise ValueError(\n                    f\"Unknown gathering strategy {self.gathering_strategy}.\"\n                )\n</code></pre>"},{"location":"api/anomaly_detection/tDBN_data_formatter/#applybn.anomaly_detection.dynamic_anomaly_detector.data_formatter.TemporalDBNTransformer.__init__","title":"<code>__init__(window=100, include_label=True, stride=1, gathering_strategy='any')</code>","text":"<p>Initialize the transformer.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>float</code> <p>If &lt; 1, the size of the sliding temporal window. If &gt; 1, number of rows in window.</p> <code>100</code> <code>stride</code> <code>int</code> <p>The size of the sliding temporal stride.</p> <code>1</code> <code>include_label</code> <code>bool</code> <p>Whether to include the label (<code>y</code>) column in the transformed output.</p> <code>True</code> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/data_formatter.py</code> <pre><code>def __init__(\n    self,\n    window: float = 100,\n    include_label: bool = True,\n    stride: int = 1,\n    gathering_strategy: None | Literal[\"any\"] = \"any\",\n):\n    \"\"\"\n    Initialize the transformer.\n\n    Args:\n        window: If &lt; 1, the size of the sliding temporal window. If &gt; 1, number of rows in window.\n        stride: The size of the sliding temporal stride.\n        include_label: Whether to include the label (`y`) column in the transformed output.\n    \"\"\"\n    self.window = window\n    self.include_label = include_label\n    self.gathering_strategy = gathering_strategy\n    self.stride = stride\n</code></pre>"},{"location":"api/anomaly_detection/tDBN_data_formatter/#applybn.anomaly_detection.dynamic_anomaly_detector.data_formatter.TemporalDBNTransformer.aggregate_anomalies","title":"<code>aggregate_anomalies(X)</code>","text":"<p>This function aggregates anomalies. After transform there are a lot of cols with anomalies and gathering them into one target vector is required. Args:     X: Sliced data</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with target vector</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/data_formatter.py</code> <pre><code>def aggregate_anomalies(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function aggregates anomalies. After transform there are a lot of cols with anomalies and\n    gathering them into one target vector is required.\n    Args:\n        X: Sliced data\n\n    Returns:\n        Dataframe with target vector\n    \"\"\"\n    X_ = X.copy()\n    match self.gathering_strategy:\n        case \"any\":\n            anomaly_cols_names = [\n                col for col in X.columns if col.startswith(\"anomaly\")\n            ]\n            anomalies_cols = X[anomaly_cols_names]\n\n            aggregated = np.any(anomalies_cols, axis=1)\n            X_.drop(anomaly_cols_names, axis=1, inplace=True)\n            X_[\"anomaly\"] = aggregated.astype(int)\n            return X_\n        case _:\n            raise ValueError(\n                f\"Unknown gathering strategy {self.gathering_strategy}.\"\n            )\n</code></pre>"},{"location":"api/anomaly_detection/tDBN_data_formatter/#applybn.anomaly_detection.dynamic_anomaly_detector.data_formatter.TemporalDBNTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit does nothing but is required by scikit-learn.</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/data_formatter.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n    \"\"\"\n    Fit does nothing but is required by scikit-learn.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/anomaly_detection/tDBN_data_formatter/#applybn.anomaly_detection.dynamic_anomaly_detector.data_formatter.TemporalDBNTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transforms the input DataFrame into a windowed representation with an optional stride.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input features. Each row is a time step.</p> required <code>y</code> <code>Optional[Series]</code> <p>Labels corresponding to each row of X (e.g., anomaly labels). Must be the same length as X.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where each row is a flattened sliding window of the input.</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/data_formatter.py</code> <pre><code>def transform(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the input DataFrame into a windowed representation with an optional stride.\n\n    Args:\n        X: Input features. Each row is a time step.\n        y: Labels corresponding to each row of X (e.g., anomaly labels). Must be the same length as X.\n\n    Returns:\n        A DataFrame where each row is a flattened sliding window of the input.\n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input X must be a pandas DataFrame.\")\n\n    if self.include_label:\n        if y is None:\n            raise ValueError(\"Labels must be provided when include_label=True.\")\n        if len(X) != len(y):\n            raise ValueError(\"X and y must have the same number of rows.\")\n        X = X.copy()\n        X[\"anomaly\"] = y.values\n\n    values = X.values\n    n_rows, n_features = values.shape\n    if n_rows &lt; self.window:\n        raise ValueError(f\"Input data must have at least {self.window} rows.\")\n\n    if self.window &lt; 1:\n        window_size = int(self.window * n_rows)\n    else:\n        window_size = self.window\n\n    num_windows = (n_rows - window_size) // self.stride + 1\n\n    dfs = []\n    for i in range(0, num_windows * self.stride, self.stride):\n        window = values[i : i + window_size]\n        window_flat = window.flatten()\n        col_names = [\n            f\"{col}__{j}\" for j in range(1, window_size + 1) for col in X.columns\n        ]\n        part_df = pd.DataFrame([window_flat], columns=col_names)\n        dfs.append(part_df)\n\n    final_df = pd.concat(dfs, axis=0, ignore_index=True).reset_index(\n        names=[\"subject_id\"]\n    )\n\n    if self.gathering_strategy and self.include_label:\n        final_df = self.aggregate_anomalies(final_df)\n\n    return final_df\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/","title":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432 \u0432 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<p>\u0414\u0435\u0442\u0435\u043a\u0442\u043e\u0440 \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u0435\u0437 \u0443\u0447\u0438\u0442\u0435\u043b\u044f. \u042d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u044e\u0442 \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u043c\u0438 scores, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0432 \u044d\u0442\u043e\u043c \u043c\u0435\u0442\u043e\u0434\u0435 \u043a\u0430\u0436\u0434\u0430\u044f \u0441\u0442\u0440\u043e\u043a\u0430 \u0438\u043c\u0435\u0435\u0442 \u0441\u0432\u043e\u044e \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e \u043e\u0446\u0435\u043d\u043a\u0443 \u0430\u043d\u043e\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438.</p> <p>\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0432 \u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</p>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#_2","title":"\u0414\u0435\u0442\u0435\u043a\u0442\u043e\u0440","text":"<p>A tabular detector for anomaly detection.</p> <p>This class provides methods for fitting a pipeline, scoring data, and predicting anomalies in tabular datasets. It supports multiple scoring methods, including mixed, proximity-based, and model-based scoring.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>class TabularDetector:\n    \"\"\"\n    A tabular detector for anomaly detection.\n\n    This class provides methods for fitting a pipeline, scoring data, and predicting anomalies\n    in tabular datasets. It supports multiple scoring methods, including mixed, proximity-based,\n    and model-based scoring.\n    \"\"\"\n\n    _parameter_constraints = {\n        \"target_name\": [str, None],\n        \"score\": StrOptions({\"mixed\", \"proximity\", \"model\"}),\n        \"additional_score\": Options(options={StrOptions({\"LOF\"}), None}, type=str),\n        \"thresholding_strategy\": Options(\n            options={StrOptions({\"best_from_range\"}), None}, type=str\n        ),\n        \"model_estimation_method\": [dict],\n        \"verbose\": [int],\n    }\n\n    _scores = {\n        \"mixed\": ODBPScore,\n        \"proximity\": LocalOutlierScore,\n        \"model\": ModelBasedScore,\n    }\n\n    def __init__(\n        self,\n        target_name: None | str = None,\n        score: Literal[\"mixed\", \"proximity\", \"model\"] = \"mixed\",\n        additional_score: None | str = \"LOF\",\n        thresholding_strategy: None | str = \"best_from_range\",\n        model_estimation_method: (\n            None\n            | str\n            | dict[\n                Literal[\"cont\", \"disc\"],\n                Literal[\"original_modified\", \"iqr\", \"cond_ratio\"],\n            ]\n        ) = None,\n        verbose: int = 1,\n    ):\n        \"\"\"\n        Initializes the TabularDetector object.\n\n        Args:\n            target_name: The name of the target column in the dataset.\n            score: The scoring method to use (\"mixed\", \"proximity\", or \"model\").\n            additional_score: The additional proximity-based scoring method (e.g., \"LOF\").\n            thresholding_strategy: The strategy for thresholding scores (e.g., \"best_from_range\").\n            model_estimation_method: The method for model-based scoring, specified separately\n                for continuous and discrete variables.\n            verbose: The verbosity level for logging. Default is 1.\n        \"\"\"\n        if model_estimation_method is None:\n            model_estimation_method = {\"cont\": \"iqr\", \"disc\": \"cond_ratio\"}\n\n        self.target_name = target_name\n        self.score = score\n        self.additional_score = additional_score\n        self.thresholding = thresholding_strategy\n        self.model_estimation_method = model_estimation_method\n        self.y_ = None\n        self.verbose = verbose\n\n    @property\n    def impacts(self):\n        return {\n            \"proximity\": self.pipeline_.scorer.proximity_impact,\n            \"model\": self.pipeline_.scorer.model_impact,\n        }\n\n    def _is_fitted(self):\n        \"\"\"\n        Checks whether the detector is fitted or not.\n\n        Returns:\n            bool: True if the detector is fitted, False otherwise.\n        \"\"\"\n        return True if \"pipeline_\" in self.__dict__ else False\n\n    def __getattr__(self, attr: str):\n        \"\"\"\n        Delegates attribute access to the pipeline if the attribute is not found.\n\n        Args:\n            attr: The name of the attribute.\n\n        Returns:\n            Any: The value of the attribute.\n\n        Raises:\n            NotFittedError: If the pipeline is not fitted.\n        \"\"\"\n        try:\n            return object.__getattribute__(self, attr)\n        except AttributeError:\n            if self._is_fitted():\n                return getattr(self.pipeline_, attr)\n            else:\n                raise NotFittedError(\"BN Estimator has not been fitted.\")\n\n    def construct_score(self, **scorer_args):\n        \"\"\"\n        Constructs a scoring object based on the selected scoring method.\n\n        Args:\n            **scorer_args: Additional arguments for the scoring object.\n\n        Returns:\n            Score: The constructed scoring object.\n        \"\"\"\n        score_class = self._scores[self.score]\n        score_obj = score_class(**scorer_args)\n        return score_obj\n\n    def _validate_methods(self):\n        \"\"\"\n        Validates that the model estimation method matches the data types.\n\n        Raises:\n            ValueError: If the estimation method is unknown.\n            TypeError: If the estimation method is incompatible with the data types.\n        \"\"\"\n        if isinstance(self.model_estimation_method, dict):\n            return  # Custom methods are allowed\n\n        method = self.model_estimation_method\n        node_types = set(self.descriptor[\"types\"].values())\n\n        # Define method compatibility\n        method_compatibility = {\n            \"iqr\": {\"cont\"},  # IQR only works with continuous data\n            \"cond_ratio\": {\n                \"disc\",\n                \"disc_num\",\n            },  # Conditional ratio only works with discrete data\n            \"original_modified\": {\"disc\", \"disc_num\", \"cont\"},\n        }\n\n        # Check if method is known\n        if method not in method_compatibility:\n            raise ValueError(f\"Unknown estimation method: {method}\")\n\n        # Check for incompatible data types\n        incompatible_types = node_types - method_compatibility[method]\n        if incompatible_types:\n            raise TypeError(\n                f\"Method '{method}' cannot work with {', '.join(incompatible_types)} data types. \"\n                f\"Compatible types: {', '.join(method_compatibility[method])}\"\n            )\n\n    def _validate_target_name(self, X):\n        if self.target_name is not None:\n            if self.target_name not in X.columns:\n                raise KeyError(\n                    f\"Target name '{self.target_name}' is not present in {X.columns.tolist()}.\"\n                )\n            else:\n                return True\n        return False\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Fits the anomaly detection pipeline to the data.\n\n        Args:\n            X: The input data.\n            y: The target values. Not used.\n\n        Returns:\n            TabularDetector: The fitted detector.\n\n        Raises:\n            KeyError: If the target column is not found in the input data.\n        \"\"\"\n        X_ = X.copy()\n        if self._validate_target_name(X):\n            self.y_ = X_.pop(self.target_name)\n\n        factory = EstimatorPipelineFactory(task_type=\"classification\")\n        factory.estimator_ = TabularEstimator()\n        pipeline = factory()\n\n        ad_pipeline = AnomalyDetectionPipeline.from_core_pipeline(pipeline)\n\n        ad_pipeline.fit(X_)\n\n        self.pipeline_ = ad_pipeline\n        return self\n\n    def decision_function(self, X: pd.DataFrame):\n        \"\"\"\n        Computes the anomaly scores for the input data.\n\n        Args:\n            X: The input data.\n\n        Returns:\n            np.ndarray: The computed anomaly scores.\n        \"\"\"\n        self._validate_methods()\n        score_obj = self.construct_score(\n            bn=self.pipeline_.bn_,\n            model_estimation_method=self.model_estimation_method,\n            proximity_estimation_method=self.additional_score,\n            model_scorer_args=dict(\n                encoding=self.pipeline_.encoding, verbose=self.verbose\n            ),\n            additional_scorer_args=dict(verbose=self.verbose),\n        )\n        self.pipeline_.set_params(bn_estimator__scorer=score_obj)\n        scores = self.pipeline_.score(X)\n        return scores\n\n    @staticmethod\n    def threshold_search_supervised(y: np.ndarray, y_pred: np.ndarray):\n        \"\"\"\n        Searches for the best threshold to maximize the F1 score.\n\n        Args:\n            y : The true labels.\n            y_pred: The predicted scores.\n\n        Returns:\n            float: The best threshold.\n        \"\"\"\n        thresholds = np.linspace(1, y_pred.max(), 100)\n        eval_scores = []\n\n        for t in thresholds:\n            outlier_scores_thresholded = np.where(y_pred &lt; t, 0, 1)\n            eval_scores.append(f1_score(y, outlier_scores_thresholded))\n\n        return thresholds[np.argmax(eval_scores)]\n\n    def predict_scores(self, X: pd.DataFrame):\n        \"\"\"\n        Predicts the anomaly scores for the input data.\n\n        Args:\n            X: The input data.\n\n        Returns:\n            np.ndarray: The predicted anomaly scores.\n        \"\"\"\n        check_is_fitted(self)\n        return self.decision_function(X)\n\n    def predict(self, X: pd.DataFrame):\n        \"\"\"\n        Predicts whether each data point is an anomaly or not.\n\n        Args:\n            X: The input data.\n\n        Returns:\n            np.ndarray: An array of binary predictions (1 for anomaly, 0 for normal).\n\n        Raises:\n            NotImplementedError: If unsupervised thresholding is not implemented.\n        \"\"\"\n        check_is_fitted(self)\n        X_ = X.copy()\n        if self._validate_target_name(X):\n            X_.drop(columns=[self.target_name], inplace=True)\n\n        D = self.decision_function(X_)\n        if self.y_ is not None:\n            best_threshold = self.threshold_search_supervised(self.y_, D)\n        else:\n            raise NotImplementedError(\n                \"Unsupervised thresholding is not implemented yet.\"\n                \"Please specify a target column to use supervised thresholding or use predict_scores.\"\n            )\n\n        return np.where(D &gt; best_threshold, 1, 0)\n\n    def plot_result(self, predicted: np.ndarray | pd.Series):\n        \"\"\"\n        Plots the results of the anomaly detection.\n\n        Args:\n            predicted: The predicted labels.\n        \"\"\"\n        result_display = ResultsDisplay(predicted, self.y_)\n        result_display.show()\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.__getattr__","title":"<code>__getattr__(attr)</code>","text":"<p>Delegates attribute access to the pipeline if the attribute is not found.</p> <p>Parameters:</p> Name Type Description Default <code>attr</code> <code>str</code> <p>The name of the attribute.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>The value of the attribute.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the pipeline is not fitted.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def __getattr__(self, attr: str):\n    \"\"\"\n    Delegates attribute access to the pipeline if the attribute is not found.\n\n    Args:\n        attr: The name of the attribute.\n\n    Returns:\n        Any: The value of the attribute.\n\n    Raises:\n        NotFittedError: If the pipeline is not fitted.\n    \"\"\"\n    try:\n        return object.__getattribute__(self, attr)\n    except AttributeError:\n        if self._is_fitted():\n            return getattr(self.pipeline_, attr)\n        else:\n            raise NotFittedError(\"BN Estimator has not been fitted.\")\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.__init__","title":"<code>__init__(target_name=None, score='mixed', additional_score='LOF', thresholding_strategy='best_from_range', model_estimation_method=None, verbose=1)</code>","text":"<p>Initializes the TabularDetector object.</p> <p>Parameters:</p> Name Type Description Default <code>target_name</code> <code>None | str</code> <p>The name of the target column in the dataset.</p> <code>None</code> <code>score</code> <code>Literal['mixed', 'proximity', 'model']</code> <p>The scoring method to use (\"mixed\", \"proximity\", or \"model\").</p> <code>'mixed'</code> <code>additional_score</code> <code>None | str</code> <p>The additional proximity-based scoring method (e.g., \"LOF\").</p> <code>'LOF'</code> <code>thresholding_strategy</code> <code>None | str</code> <p>The strategy for thresholding scores (e.g., \"best_from_range\").</p> <code>'best_from_range'</code> <code>model_estimation_method</code> <code>None | str | dict[Literal['cont', 'disc'], Literal['original_modified', 'iqr', 'cond_ratio']]</code> <p>The method for model-based scoring, specified separately for continuous and discrete variables.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>The verbosity level for logging. Default is 1.</p> <code>1</code> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def __init__(\n    self,\n    target_name: None | str = None,\n    score: Literal[\"mixed\", \"proximity\", \"model\"] = \"mixed\",\n    additional_score: None | str = \"LOF\",\n    thresholding_strategy: None | str = \"best_from_range\",\n    model_estimation_method: (\n        None\n        | str\n        | dict[\n            Literal[\"cont\", \"disc\"],\n            Literal[\"original_modified\", \"iqr\", \"cond_ratio\"],\n        ]\n    ) = None,\n    verbose: int = 1,\n):\n    \"\"\"\n    Initializes the TabularDetector object.\n\n    Args:\n        target_name: The name of the target column in the dataset.\n        score: The scoring method to use (\"mixed\", \"proximity\", or \"model\").\n        additional_score: The additional proximity-based scoring method (e.g., \"LOF\").\n        thresholding_strategy: The strategy for thresholding scores (e.g., \"best_from_range\").\n        model_estimation_method: The method for model-based scoring, specified separately\n            for continuous and discrete variables.\n        verbose: The verbosity level for logging. Default is 1.\n    \"\"\"\n    if model_estimation_method is None:\n        model_estimation_method = {\"cont\": \"iqr\", \"disc\": \"cond_ratio\"}\n\n    self.target_name = target_name\n    self.score = score\n    self.additional_score = additional_score\n    self.thresholding = thresholding_strategy\n    self.model_estimation_method = model_estimation_method\n    self.y_ = None\n    self.verbose = verbose\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.construct_score","title":"<code>construct_score(**scorer_args)</code>","text":"<p>Constructs a scoring object based on the selected scoring method.</p> <p>Parameters:</p> Name Type Description Default <code>**scorer_args</code> <p>Additional arguments for the scoring object.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Score</code> <p>The constructed scoring object.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def construct_score(self, **scorer_args):\n    \"\"\"\n    Constructs a scoring object based on the selected scoring method.\n\n    Args:\n        **scorer_args: Additional arguments for the scoring object.\n\n    Returns:\n        Score: The constructed scoring object.\n    \"\"\"\n    score_class = self._scores[self.score]\n    score_obj = score_class(**scorer_args)\n    return score_obj\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.decision_function","title":"<code>decision_function(X)</code>","text":"<p>Computes the anomaly scores for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The computed anomaly scores.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def decision_function(self, X: pd.DataFrame):\n    \"\"\"\n    Computes the anomaly scores for the input data.\n\n    Args:\n        X: The input data.\n\n    Returns:\n        np.ndarray: The computed anomaly scores.\n    \"\"\"\n    self._validate_methods()\n    score_obj = self.construct_score(\n        bn=self.pipeline_.bn_,\n        model_estimation_method=self.model_estimation_method,\n        proximity_estimation_method=self.additional_score,\n        model_scorer_args=dict(\n            encoding=self.pipeline_.encoding, verbose=self.verbose\n        ),\n        additional_scorer_args=dict(verbose=self.verbose),\n    )\n    self.pipeline_.set_params(bn_estimator__scorer=score_obj)\n    scores = self.pipeline_.score(X)\n    return scores\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the anomaly detection pipeline to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <code>y</code> <p>The target values. Not used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TabularDetector</code> <p>The fitted detector.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the target column is not found in the input data.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Fits the anomaly detection pipeline to the data.\n\n    Args:\n        X: The input data.\n        y: The target values. Not used.\n\n    Returns:\n        TabularDetector: The fitted detector.\n\n    Raises:\n        KeyError: If the target column is not found in the input data.\n    \"\"\"\n    X_ = X.copy()\n    if self._validate_target_name(X):\n        self.y_ = X_.pop(self.target_name)\n\n    factory = EstimatorPipelineFactory(task_type=\"classification\")\n    factory.estimator_ = TabularEstimator()\n    pipeline = factory()\n\n    ad_pipeline = AnomalyDetectionPipeline.from_core_pipeline(pipeline)\n\n    ad_pipeline.fit(X_)\n\n    self.pipeline_ = ad_pipeline\n    return self\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.plot_result","title":"<code>plot_result(predicted)</code>","text":"<p>Plots the results of the anomaly detection.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>ndarray | Series</code> <p>The predicted labels.</p> required Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def plot_result(self, predicted: np.ndarray | pd.Series):\n    \"\"\"\n    Plots the results of the anomaly detection.\n\n    Args:\n        predicted: The predicted labels.\n    \"\"\"\n    result_display = ResultsDisplay(predicted, self.y_)\n    result_display.show()\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.predict","title":"<code>predict(X)</code>","text":"<p>Predicts whether each data point is an anomaly or not.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: An array of binary predictions (1 for anomaly, 0 for normal).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If unsupervised thresholding is not implemented.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def predict(self, X: pd.DataFrame):\n    \"\"\"\n    Predicts whether each data point is an anomaly or not.\n\n    Args:\n        X: The input data.\n\n    Returns:\n        np.ndarray: An array of binary predictions (1 for anomaly, 0 for normal).\n\n    Raises:\n        NotImplementedError: If unsupervised thresholding is not implemented.\n    \"\"\"\n    check_is_fitted(self)\n    X_ = X.copy()\n    if self._validate_target_name(X):\n        X_.drop(columns=[self.target_name], inplace=True)\n\n    D = self.decision_function(X_)\n    if self.y_ is not None:\n        best_threshold = self.threshold_search_supervised(self.y_, D)\n    else:\n        raise NotImplementedError(\n            \"Unsupervised thresholding is not implemented yet.\"\n            \"Please specify a target column to use supervised thresholding or use predict_scores.\"\n        )\n\n    return np.where(D &gt; best_threshold, 1, 0)\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.predict_scores","title":"<code>predict_scores(X)</code>","text":"<p>Predicts the anomaly scores for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The predicted anomaly scores.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>def predict_scores(self, X: pd.DataFrame):\n    \"\"\"\n    Predicts the anomaly scores for the input data.\n\n    Args:\n        X: The input data.\n\n    Returns:\n        np.ndarray: The predicted anomaly scores.\n    \"\"\"\n    check_is_fitted(self)\n    return self.decision_function(X)\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#applybn.anomaly_detection.static_anomaly_detector.tabular_detector.TabularDetector.threshold_search_supervised","title":"<code>threshold_search_supervised(y, y_pred)</code>  <code>staticmethod</code>","text":"<p>Searches for the best threshold to maximize the F1 score.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <p>The true labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted scores.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The best threshold.</p> Source code in <code>applybn/anomaly_detection/static_anomaly_detector/tabular_detector.py</code> <pre><code>@staticmethod\ndef threshold_search_supervised(y: np.ndarray, y_pred: np.ndarray):\n    \"\"\"\n    Searches for the best threshold to maximize the F1 score.\n\n    Args:\n        y : The true labels.\n        y_pred: The predicted scores.\n\n    Returns:\n        float: The best threshold.\n    \"\"\"\n    thresholds = np.linspace(1, y_pred.max(), 100)\n    eval_scores = []\n\n    for t in thresholds:\n        outlier_scores_thresholded = np.where(y_pred &lt; t, 0, 1)\n        eval_scores.append(f1_score(y, outlier_scores_thresholded))\n\n    return thresholds[np.argmax(eval_scores)]\n</code></pre>"},{"location":"api/anomaly_detection/tabular_anomaly_detector/#_3","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<pre><code>from applybn.anomaly_detection.static_anomaly_detector.tabular_detector import TabularDetector\n\nX = load_data() # \u043b\u044e\u0431\u044b\u043c \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u043c\ndetector = TabularDetector()\ndetector.fit(X)\n\ndetector.predict_scores(X)\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/","title":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445","text":"<p>FastTimeSeriesDetector \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445 \u0431\u0435\u0437 \u0443\u0447\u0438\u0442\u0435\u043b\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430 Java. \u0414\u043b\u044f \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438 \u044d\u0442\u043e\u0433\u043e \u043c\u043e\u0434\u0443\u043b\u044f \u0431\u044b\u043b\u0430 \u0432\u043d\u0435\u0441\u0435\u043d\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430\u044f \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f.</p> <p>\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0432 \u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</p>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#_2","title":"\u0414\u0435\u0442\u0435\u043a\u0442\u043e\u0440","text":"<p>A time-series anomaly detection model based on Dynamic Bayesian Network (DBN) structure learning implemented in Java and accessed via JPype.</p> <p>This class supports both pre-sliced DBN data and automatic transformation of raw tabular time-series data using a sliding window mechanism.</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/fast_time_series_detector.py</code> <pre><code>class FastTimeSeriesDetector:\n    \"\"\"\n    A time-series anomaly detection model based on Dynamic Bayesian Network (DBN) structure learning\n    implemented in Java and accessed via JPype.\n\n    This class supports both pre-sliced DBN data and automatic transformation of raw tabular time-series data\n    using a sliding window mechanism.\n    \"\"\"\n\n    def __init__(\n        self,\n        abs_threshold: float = -4.5,\n        rel_threshold: float = 0.8,\n        num_parents: int = 3,\n        artificial_slicing: bool = False,\n        artificial_slicing_params: dict = None,\n        scoring_function: str = \"ll\",\n        markov_lag: int = 1,\n        non_stationary: bool = False,\n    ):\n        \"\"\"\n        Initializes the FastTimeSeriesDetector.\n\n        Args:\n            abs_threshold: Absolute score below which values are flagged as anomalies.\n            rel_threshold: Fraction of features with anomaly scores needed to flag the full sample.\n            num_parents: Maximum number of parents allowed in the DBN structure.\n            artificial_slicing: Whether to apply window-based transformation on the input data.\n            artificial_slicing_params: Parameters for the TemporalDBNTransformer (e.g., window size).\n            scoring_function: Scoring function used by the Java DBN learner ('ll' or 'MDL').\n            markov_lag: The Markov lag (time distance) for DBN learning.\n            non_stationary: Learn separate models for each transition instead of one shared model.\n        \"\"\"\n        self.args = [\n            \"-p\",\n            str(num_parents),\n            \"-s\",\n            scoring_function,\n            \"-m\",\n            str(markov_lag),\n            \"-ns\",\n            str(non_stationary),\n            \"-pm\",\n        ]\n        base = os.path.join(os.path.dirname(os.path.abspath(__file__)))\n        module_path = os.path.join(base, \"dbnod_modified.jar\")\n\n        if not jpype.isJVMStarted():\n            jpype.startJVM(\n                classpath=[\n                    module_path,\n                ]\n            )\n\n        self.abs_threshold = abs_threshold\n        self.rel_threshold = rel_threshold\n        self.artificial_slicing = artificial_slicing\n        self.artificial_slicing_params = artificial_slicing_params\n\n    def _is_fitted(self):\n        return True if \"scores_\" in self.__dict__ else False\n\n    @staticmethod\n    def _validate_data(X):\n        \"\"\"\n        Ensures the input DataFrame contains a 'subject_id' column and that all other\n        column names follow the expected '__' naming convention for DBN inputs.\n\n        Raises:\n            TypeError: If required format is not met.\n        \"\"\"\n        if \"subject_id\" not in X.columns:\n            raise TypeError(\"subject_id column not found in data.\")\n\n        if not all(\"__\" in col_name for col_name in X.columns.drop(\"subject_id\")):\n            raise TypeError(\n                \"Data type error. Column names must contain '__' characters.\"\n            )\n\n    def fit(self, X: pd.DataFrame):\n        \"\"\"\n        Trains the DBN model using input data. If artificial slicing is enabled,\n        performs time-window transformation before training.\n\n        Args:\n            X: Input data (time-series features).\n\n        Returns:\n            np.ndarray: Anomaly labels (0 for normal, 1 for anomalous).\n        \"\"\"\n        if not self.artificial_slicing:\n            self._validate_data(X)\n        else:\n            transformer = TemporalDBNTransformer(**self.artificial_slicing_params)\n            X = transformer.fit_transform(X)\n\n        self.scores_ = self.decision_function(X)\n\n        return self\n\n    def predict_scores(self, X: pd.DataFrame = None):\n        \"\"\"\n        Computes raw anomaly scores from the trained DBN.\n\n        Args:\n            X: Input data. Not used in this implementation.\n\n        Returns:\n            np.ndarray: Raw scores.\n        \"\"\"\n        if not self._is_fitted():\n            raise NotFittedError(\"DBN model has not been fitted.\")\n\n        return self.scores_\n\n    def calibrate(\n        self,\n        y_true: pd.Series | np.ndarray,\n        calibration_bounds: dict | None = None,\n        verbose: int = 1,\n        calibration_params: dict = None,\n    ):\n        \"\"\"\n        A method to calibrate the DBN. Calibration means finding absolute and relative thresholds.\n        Utilizes bayesian optimization.\n\n        Args:\n            y_true: values to calibrate on\n            calibration_bounds: bound of calibration values. Must contain abs_thrs and rel_thrs keys.\n            verbose: verbosity level.\n            calibration_params: calibration parameters for optimization.\n\n        \"\"\"\n\n        def func_to_optimize(abs_thrs, rel_thrs):\n            self.abs_threshold = abs_thrs\n            self.rel_threshold = rel_thrs\n            preds = self.predict()\n            return f1_score(y_true, preds)\n\n        if calibration_params is None:\n            calibration_params = dict(init_points=10, n_iter=100)\n\n        if calibration_bounds is None:\n            pbounds = {\"abs_thrs\": (-8, -2), \"rel_thrs\": (0.2, 0.95)}\n        else:\n            pbounds = calibration_bounds\n\n        optimizer = BayesianOptimization(\n            f=func_to_optimize, pbounds=pbounds, verbose=verbose\n        )\n\n        optimizer.maximize(**calibration_params)\n\n        self.abs_threshold, self.rel_threshold = (\n            optimizer.max[\"params\"][\"abs_thrs\"],\n            optimizer.max[\"params\"][\"rel_thrs\"],\n        )\n        return self\n\n    def predict(self, X: pd.DataFrame = None):\n        \"\"\"\n        Trains the model and applies anomaly decision logic.\n\n        Args:\n            X: Input features. Not used.\n\n        Returns:\n            np.ndarray: Binary anomaly labels (1 = anomalous).\n        \"\"\"\n        if not self._is_fitted():\n            raise NotFittedError(\"DBN model has not been fitted.\")\n\n        thresholded = np.where((self.scores_ &lt; self.abs_threshold), 1, 0)\n\n        # Aggregate per-sample anomaly flags and compare against relative threshold\n        self.anom_fractions_ = thresholded.mean(axis=0)\n        return np.where(self.anom_fractions_ &gt; self.rel_threshold, 1, 0)\n\n    def decision_function(self, X: pd.DataFrame):\n        \"\"\"\n        Calls the Java backend to score transitions using DBN inference.\n\n        Args:\n            X: Preprocessed DBN-compatible DataFrame.\n\n        Returns:\n            np.ndarray: 2D array of log-likelihood scores from the Java model.\n        \"\"\"\n        from com.github.tDBN.cli import LearnFromFile\n\n        # Write data to disk and call Java scoring\n        with tempfile.NamedTemporaryFile() as tmpfile:\n            X.to_csv(tmpfile, index=False)\n            self.args.extend([\"-i\", tmpfile.name])\n            result = LearnFromFile.ComputeScores(JArray(JString)(self.args))\n\n            outlier_indexes, scores = result\n\n            # Convert Java 2D double array into numpy\n            py_2d_array = []\n            for i in range(len(scores)):\n                py_2d_array.append(list(scores[i]))\n\n            scores = np.asarray(py_2d_array)\n            return scores\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector.FastTimeSeriesDetector.__init__","title":"<code>__init__(abs_threshold=-4.5, rel_threshold=0.8, num_parents=3, artificial_slicing=False, artificial_slicing_params=None, scoring_function='ll', markov_lag=1, non_stationary=False)</code>","text":"<p>Initializes the FastTimeSeriesDetector.</p> <p>Parameters:</p> Name Type Description Default <code>abs_threshold</code> <code>float</code> <p>Absolute score below which values are flagged as anomalies.</p> <code>-4.5</code> <code>rel_threshold</code> <code>float</code> <p>Fraction of features with anomaly scores needed to flag the full sample.</p> <code>0.8</code> <code>num_parents</code> <code>int</code> <p>Maximum number of parents allowed in the DBN structure.</p> <code>3</code> <code>artificial_slicing</code> <code>bool</code> <p>Whether to apply window-based transformation on the input data.</p> <code>False</code> <code>artificial_slicing_params</code> <code>dict</code> <p>Parameters for the TemporalDBNTransformer (e.g., window size).</p> <code>None</code> <code>scoring_function</code> <code>str</code> <p>Scoring function used by the Java DBN learner ('ll' or 'MDL').</p> <code>'ll'</code> <code>markov_lag</code> <code>int</code> <p>The Markov lag (time distance) for DBN learning.</p> <code>1</code> <code>non_stationary</code> <code>bool</code> <p>Learn separate models for each transition instead of one shared model.</p> <code>False</code> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/fast_time_series_detector.py</code> <pre><code>def __init__(\n    self,\n    abs_threshold: float = -4.5,\n    rel_threshold: float = 0.8,\n    num_parents: int = 3,\n    artificial_slicing: bool = False,\n    artificial_slicing_params: dict = None,\n    scoring_function: str = \"ll\",\n    markov_lag: int = 1,\n    non_stationary: bool = False,\n):\n    \"\"\"\n    Initializes the FastTimeSeriesDetector.\n\n    Args:\n        abs_threshold: Absolute score below which values are flagged as anomalies.\n        rel_threshold: Fraction of features with anomaly scores needed to flag the full sample.\n        num_parents: Maximum number of parents allowed in the DBN structure.\n        artificial_slicing: Whether to apply window-based transformation on the input data.\n        artificial_slicing_params: Parameters for the TemporalDBNTransformer (e.g., window size).\n        scoring_function: Scoring function used by the Java DBN learner ('ll' or 'MDL').\n        markov_lag: The Markov lag (time distance) for DBN learning.\n        non_stationary: Learn separate models for each transition instead of one shared model.\n    \"\"\"\n    self.args = [\n        \"-p\",\n        str(num_parents),\n        \"-s\",\n        scoring_function,\n        \"-m\",\n        str(markov_lag),\n        \"-ns\",\n        str(non_stationary),\n        \"-pm\",\n    ]\n    base = os.path.join(os.path.dirname(os.path.abspath(__file__)))\n    module_path = os.path.join(base, \"dbnod_modified.jar\")\n\n    if not jpype.isJVMStarted():\n        jpype.startJVM(\n            classpath=[\n                module_path,\n            ]\n        )\n\n    self.abs_threshold = abs_threshold\n    self.rel_threshold = rel_threshold\n    self.artificial_slicing = artificial_slicing\n    self.artificial_slicing_params = artificial_slicing_params\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector.FastTimeSeriesDetector.calibrate","title":"<code>calibrate(y_true, calibration_bounds=None, verbose=1, calibration_params=None)</code>","text":"<p>A method to calibrate the DBN. Calibration means finding absolute and relative thresholds. Utilizes bayesian optimization.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Series | ndarray</code> <p>values to calibrate on</p> required <code>calibration_bounds</code> <code>dict | None</code> <p>bound of calibration values. Must contain abs_thrs and rel_thrs keys.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>verbosity level.</p> <code>1</code> <code>calibration_params</code> <code>dict</code> <p>calibration parameters for optimization.</p> <code>None</code> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/fast_time_series_detector.py</code> <pre><code>def calibrate(\n    self,\n    y_true: pd.Series | np.ndarray,\n    calibration_bounds: dict | None = None,\n    verbose: int = 1,\n    calibration_params: dict = None,\n):\n    \"\"\"\n    A method to calibrate the DBN. Calibration means finding absolute and relative thresholds.\n    Utilizes bayesian optimization.\n\n    Args:\n        y_true: values to calibrate on\n        calibration_bounds: bound of calibration values. Must contain abs_thrs and rel_thrs keys.\n        verbose: verbosity level.\n        calibration_params: calibration parameters for optimization.\n\n    \"\"\"\n\n    def func_to_optimize(abs_thrs, rel_thrs):\n        self.abs_threshold = abs_thrs\n        self.rel_threshold = rel_thrs\n        preds = self.predict()\n        return f1_score(y_true, preds)\n\n    if calibration_params is None:\n        calibration_params = dict(init_points=10, n_iter=100)\n\n    if calibration_bounds is None:\n        pbounds = {\"abs_thrs\": (-8, -2), \"rel_thrs\": (0.2, 0.95)}\n    else:\n        pbounds = calibration_bounds\n\n    optimizer = BayesianOptimization(\n        f=func_to_optimize, pbounds=pbounds, verbose=verbose\n    )\n\n    optimizer.maximize(**calibration_params)\n\n    self.abs_threshold, self.rel_threshold = (\n        optimizer.max[\"params\"][\"abs_thrs\"],\n        optimizer.max[\"params\"][\"rel_thrs\"],\n    )\n    return self\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector.FastTimeSeriesDetector.decision_function","title":"<code>decision_function(X)</code>","text":"<p>Calls the Java backend to score transitions using DBN inference.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Preprocessed DBN-compatible DataFrame.</p> required <p>Returns:</p> Type Description <p>np.ndarray: 2D array of log-likelihood scores from the Java model.</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/fast_time_series_detector.py</code> <pre><code>def decision_function(self, X: pd.DataFrame):\n    \"\"\"\n    Calls the Java backend to score transitions using DBN inference.\n\n    Args:\n        X: Preprocessed DBN-compatible DataFrame.\n\n    Returns:\n        np.ndarray: 2D array of log-likelihood scores from the Java model.\n    \"\"\"\n    from com.github.tDBN.cli import LearnFromFile\n\n    # Write data to disk and call Java scoring\n    with tempfile.NamedTemporaryFile() as tmpfile:\n        X.to_csv(tmpfile, index=False)\n        self.args.extend([\"-i\", tmpfile.name])\n        result = LearnFromFile.ComputeScores(JArray(JString)(self.args))\n\n        outlier_indexes, scores = result\n\n        # Convert Java 2D double array into numpy\n        py_2d_array = []\n        for i in range(len(scores)):\n            py_2d_array.append(list(scores[i]))\n\n        scores = np.asarray(py_2d_array)\n        return scores\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector.FastTimeSeriesDetector.fit","title":"<code>fit(X)</code>","text":"<p>Trains the DBN model using input data. If artificial slicing is enabled, performs time-window transformation before training.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input data (time-series features).</p> required <p>Returns:</p> Type Description <p>np.ndarray: Anomaly labels (0 for normal, 1 for anomalous).</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/fast_time_series_detector.py</code> <pre><code>def fit(self, X: pd.DataFrame):\n    \"\"\"\n    Trains the DBN model using input data. If artificial slicing is enabled,\n    performs time-window transformation before training.\n\n    Args:\n        X: Input data (time-series features).\n\n    Returns:\n        np.ndarray: Anomaly labels (0 for normal, 1 for anomalous).\n    \"\"\"\n    if not self.artificial_slicing:\n        self._validate_data(X)\n    else:\n        transformer = TemporalDBNTransformer(**self.artificial_slicing_params)\n        X = transformer.fit_transform(X)\n\n    self.scores_ = self.decision_function(X)\n\n    return self\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector.FastTimeSeriesDetector.predict","title":"<code>predict(X=None)</code>","text":"<p>Trains the model and applies anomaly decision logic.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input features. Not used.</p> <code>None</code> <p>Returns:</p> Type Description <p>np.ndarray: Binary anomaly labels (1 = anomalous).</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/fast_time_series_detector.py</code> <pre><code>def predict(self, X: pd.DataFrame = None):\n    \"\"\"\n    Trains the model and applies anomaly decision logic.\n\n    Args:\n        X: Input features. Not used.\n\n    Returns:\n        np.ndarray: Binary anomaly labels (1 = anomalous).\n    \"\"\"\n    if not self._is_fitted():\n        raise NotFittedError(\"DBN model has not been fitted.\")\n\n    thresholded = np.where((self.scores_ &lt; self.abs_threshold), 1, 0)\n\n    # Aggregate per-sample anomaly flags and compare against relative threshold\n    self.anom_fractions_ = thresholded.mean(axis=0)\n    return np.where(self.anom_fractions_ &gt; self.rel_threshold, 1, 0)\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector.FastTimeSeriesDetector.predict_scores","title":"<code>predict_scores(X=None)</code>","text":"<p>Computes raw anomaly scores from the trained DBN.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input data. Not used in this implementation.</p> <code>None</code> <p>Returns:</p> Type Description <p>np.ndarray: Raw scores.</p> Source code in <code>applybn/anomaly_detection/dynamic_anomaly_detector/fast_time_series_detector.py</code> <pre><code>def predict_scores(self, X: pd.DataFrame = None):\n    \"\"\"\n    Computes raw anomaly scores from the trained DBN.\n\n    Args:\n        X: Input data. Not used in this implementation.\n\n    Returns:\n        np.ndarray: Raw scores.\n    \"\"\"\n    if not self._is_fitted():\n        raise NotFittedError(\"DBN model has not been fitted.\")\n\n    return self.scores_\n</code></pre>"},{"location":"api/anomaly_detection/ts_anomaly_detection/#_3","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<pre><code>from applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector import FastTimeSeriesDetector\n\nX = load_data() # \u043b\u044e\u0431\u044b\u043c \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u043c\ny_true = X.pop(\"y\") # \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\ndetector = FastTimeSeriesDetector()\ndetector.fit(X)\n\ndetector.calibrate(y_true) # \u043e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\ndetector.predict(X)\n</code></pre>"},{"location":"api/core/estimators/","title":"\u041e\u0446\u0435\u043d\u0449\u0438\u043a\u0438","text":"<p>\u042d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u0438. \u041f\u043e \u0441\u0443\u0442\u0438, \u044d\u0442\u043e \u043e\u0431\u0435\u0440\u0442\u043a\u0430 \u043d\u0430\u0434 \u0441\u0435\u0442\u044f\u043c\u0438 <code>bamt</code> \u0441 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f\u043c\u0438. \u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u044d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c, \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u0432\u044b \u044f\u0432\u043b\u044f\u0435\u0442\u0435\u0441\u044c \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u043e\u043c, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d \u043d\u0438\u0437\u043a\u043e\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u044b\u0439. \u0412 \u043f\u0440\u043e\u0442\u0438\u0432\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u044b (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u044b, \u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u0435\u043b\u0438 \u0438 \u0442.\u0434.).</p> <p>\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0432 \u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f. </p>"},{"location":"api/core/estimators/#applybn.core.estimators.base_estimator.BNEstimator","title":"<code>BNEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>A Bayesian Network Estimator class that extends scikit-learn's BaseEstimator.</p> Source code in <code>applybn/core/estimators/base_estimator.py</code> <pre><code>class BNEstimator(BaseEstimator):\n    \"\"\"\n    A Bayesian Network Estimator class that extends scikit-learn's BaseEstimator.\n    \"\"\"\n\n    _parameter_constraints = {\n        \"has_logit\": [bool],\n        \"use_mixture\": [bool],\n        \"bn_type\": [str, None],\n        \"partial\": [Options(object, {False, \"parameters\", \"structure\"})],\n        \"learning_params\": [None, dict],\n    }\n\n    def __init__(\n        self,\n        has_logit: bool = False,\n        use_mixture: bool = False,\n        partial: False | Literal[\"parameters\", \"structure\"] = False,\n        bn_type: Literal[\"hybrid\", \"disc\", \"cont\"] | None = None,\n        learning_params: Unpack[ParamDict] | None = None,\n    ):\n        \"\"\"\n        Initializes the BNEstimator with the given parameters.\n\n        Args:\n            has_logit: Indicates if logit transformation is used.\n            use_mixture: Indicates if mixture model is used.\n            partial: Indicates if partial fitting is used.\n            bn_type: Type of Bayesian Network.\n            learning_params: Parameters for learning.\n        \"\"\"\n        self.has_logit = has_logit\n        self.use_mixture = use_mixture\n        self.bn_type = bn_type\n        self.partial = partial\n        self.learning_params = {} if learning_params is None else learning_params\n\n    def _is_fitted(self):\n        \"\"\"\n        Checks whether the estimator is fitted or not by checking \"bn_\" key if __dict__.\n        This has to be done because check_is_fitted(self) does not imply correct and goes into recursion because of\n        delegating strategy in getattr method.\n        \"\"\"\n        return True if \"bn_\" in self.__dict__ else False\n\n    def __getattr__(self, attr: str):\n        \"\"\"If attribute is not found in the pipeline, look in the last step of the pipeline.\"\"\"\n        try:\n            return object.__getattribute__(self, attr)\n        except AttributeError:\n            if self._is_fitted():\n                return getattr(self.bn_, attr)\n            else:\n                raise NotFittedError(\"BN Estimator has not been fitted.\")\n\n    @staticmethod\n    def detect_bn(data: pd.DataFrame) -&gt; Literal[\"hybrid\", \"disc\", \"cont\"]:\n        \"\"\"\n        Detects the type of Bayesian Network based on the data.\n        Bamt typing is used.\n\n        Args:\n            data (pd.DataFrame): The input data to analyze.\n\n        Returns:\n            bn_type: The detected type of Bayesian Network.\n\n        Raises:\n            None: an error translates into bamt logger.\n                Possible errors:\n                    \"Unsupported data type. Dtype: {dtypes}\"\n        \"\"\"\n\n        node_types = nodes_types(data)\n\n        if len(node_types.keys()) != len(data.columns):\n            diff = set(data.columns) - set(node_types.keys())\n            raise NodesAutoTypingError(diff)\n\n        nodes_types_unique = set(node_types.values())\n\n        net_types2unqiue = {\n            \"hybrid\": [\n                {\"cont\", \"disc\", \"disc_num\"},\n                {\"cont\", \"disc_num\"},\n                {\"cont\", \"disc\"},\n            ],\n            \"disc\": [{\"disc\"}, {\"disc_num\"}, {\"disc\", \"disc_num\"}],\n            \"cont\": [{\"cont\"}],\n        }\n        find_matching_key = (\n            {frozenset(s): k for k, v in net_types2unqiue.items() for s in v}\n        ).get\n        return find_matching_key(frozenset(nodes_types_unique))\n\n    def init_bn(\n        self, bn_type: Literal[\"hybrid\", \"disc\", \"cont\"]\n    ) -&gt; HybridBN | DiscreteBN | ContinuousBN:\n        \"\"\"\n        Initializes the Bayesian Network based on the type.\n\n        Args:\n            bn_type: The type of Bayesian Network to initialize.\n\n        Returns:\n            An instance of the corresponding Bayesian Network class.\n\n        Raises:\n            TypeError: Invalid bn_type.\n        \"\"\"\n        str2net = {\"hybrid\": HybridBN, \"disc\": DiscreteBN, \"cont\": ContinuousBN}\n\n        params = dict()\n        match bn_type:\n            case \"hybrid\":\n                params = dict(use_mixture=self.use_mixture, has_logit=self.has_logit)\n            case \"cont\":\n                params = dict(use_mixture=self.use_mixture)\n            case \"disc\":\n                ...\n            case _:\n                raise TypeError(f\"Invalid bn_type, obtained bn_type: {bn_type}\")\n        return str2net[bn_type](**params)\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"\n        Fits the Bayesian Network to the data.\n\n        Args:\n            X (tuple): a tuple with (X, descriptor, clean_data).\n                If partial is \"structure\", clean_data can be None (not used).\n            y (None): not used.\n\n        Returns:\n            self (BNEstimator): The fitted estimator.\n        \"\"\"\n\n        # this has to be done because scikit learn unpacking problem\n        # inside pipeline there is unpacking.\n        X, descriptor, clean_data = X\n        if not self.partial == \"parameters\":\n            if not self.bn_type in [\"hybrid\", \"disc\", \"cont\"]:\n                bn_type_ = self.detect_bn(clean_data)\n            else:\n                bn_type_ = self.bn_type\n\n            bn = self.init_bn(bn_type_)\n\n            self.bn_ = bn\n            self.bn_type = bn_type_\n\n        match self.partial:\n            case \"parameters\":\n                if not self.bn_.edges:\n                    raise NotFittedError(\n                        \"Trying to learn parameters on unfitted estimator. Call fit method first.\"\n                    )\n                self.bn_.fit_parameters(clean_data)\n            case \"structure\":\n                self.bn_.add_nodes(descriptor)\n                self.bn_.add_edges(X, progress_bar=False, **self.learning_params)\n            case False:\n                self.bn_.add_nodes(descriptor)\n                self.bn_.add_edges(X, progress_bar=False, **self.learning_params)\n                self.bn_.fit_parameters(clean_data)\n\n        return self\n</code></pre>"},{"location":"api/core/estimators/#applybn.core.estimators.base_estimator.BNEstimator.__getattr__","title":"<code>__getattr__(attr)</code>","text":"<p>If attribute is not found in the pipeline, look in the last step of the pipeline.</p> Source code in <code>applybn/core/estimators/base_estimator.py</code> <pre><code>def __getattr__(self, attr: str):\n    \"\"\"If attribute is not found in the pipeline, look in the last step of the pipeline.\"\"\"\n    try:\n        return object.__getattribute__(self, attr)\n    except AttributeError:\n        if self._is_fitted():\n            return getattr(self.bn_, attr)\n        else:\n            raise NotFittedError(\"BN Estimator has not been fitted.\")\n</code></pre>"},{"location":"api/core/estimators/#applybn.core.estimators.base_estimator.BNEstimator.__init__","title":"<code>__init__(has_logit=False, use_mixture=False, partial=False, bn_type=None, learning_params=None)</code>","text":"<p>Initializes the BNEstimator with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>has_logit</code> <code>bool</code> <p>Indicates if logit transformation is used.</p> <code>False</code> <code>use_mixture</code> <code>bool</code> <p>Indicates if mixture model is used.</p> <code>False</code> <code>partial</code> <code>False | Literal['parameters', 'structure']</code> <p>Indicates if partial fitting is used.</p> <code>False</code> <code>bn_type</code> <code>Literal['hybrid', 'disc', 'cont'] | None</code> <p>Type of Bayesian Network.</p> <code>None</code> <code>learning_params</code> <code>Unpack[ParamDict] | None</code> <p>Parameters for learning.</p> <code>None</code> Source code in <code>applybn/core/estimators/base_estimator.py</code> <pre><code>def __init__(\n    self,\n    has_logit: bool = False,\n    use_mixture: bool = False,\n    partial: False | Literal[\"parameters\", \"structure\"] = False,\n    bn_type: Literal[\"hybrid\", \"disc\", \"cont\"] | None = None,\n    learning_params: Unpack[ParamDict] | None = None,\n):\n    \"\"\"\n    Initializes the BNEstimator with the given parameters.\n\n    Args:\n        has_logit: Indicates if logit transformation is used.\n        use_mixture: Indicates if mixture model is used.\n        partial: Indicates if partial fitting is used.\n        bn_type: Type of Bayesian Network.\n        learning_params: Parameters for learning.\n    \"\"\"\n    self.has_logit = has_logit\n    self.use_mixture = use_mixture\n    self.bn_type = bn_type\n    self.partial = partial\n    self.learning_params = {} if learning_params is None else learning_params\n</code></pre>"},{"location":"api/core/estimators/#applybn.core.estimators.base_estimator.BNEstimator.detect_bn","title":"<code>detect_bn(data)</code>  <code>staticmethod</code>","text":"<p>Detects the type of Bayesian Network based on the data. Bamt typing is used.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input data to analyze.</p> required <p>Returns:</p> Name Type Description <code>bn_type</code> <code>Literal['hybrid', 'disc', 'cont']</code> <p>The detected type of Bayesian Network.</p> <p>Raises:</p> Type Description <code>None</code> <p>an error translates into bamt logger. Possible errors:     \"Unsupported data type. Dtype: {dtypes}\"</p> Source code in <code>applybn/core/estimators/base_estimator.py</code> <pre><code>@staticmethod\ndef detect_bn(data: pd.DataFrame) -&gt; Literal[\"hybrid\", \"disc\", \"cont\"]:\n    \"\"\"\n    Detects the type of Bayesian Network based on the data.\n    Bamt typing is used.\n\n    Args:\n        data (pd.DataFrame): The input data to analyze.\n\n    Returns:\n        bn_type: The detected type of Bayesian Network.\n\n    Raises:\n        None: an error translates into bamt logger.\n            Possible errors:\n                \"Unsupported data type. Dtype: {dtypes}\"\n    \"\"\"\n\n    node_types = nodes_types(data)\n\n    if len(node_types.keys()) != len(data.columns):\n        diff = set(data.columns) - set(node_types.keys())\n        raise NodesAutoTypingError(diff)\n\n    nodes_types_unique = set(node_types.values())\n\n    net_types2unqiue = {\n        \"hybrid\": [\n            {\"cont\", \"disc\", \"disc_num\"},\n            {\"cont\", \"disc_num\"},\n            {\"cont\", \"disc\"},\n        ],\n        \"disc\": [{\"disc\"}, {\"disc_num\"}, {\"disc\", \"disc_num\"}],\n        \"cont\": [{\"cont\"}],\n    }\n    find_matching_key = (\n        {frozenset(s): k for k, v in net_types2unqiue.items() for s in v}\n    ).get\n    return find_matching_key(frozenset(nodes_types_unique))\n</code></pre>"},{"location":"api/core/estimators/#applybn.core.estimators.base_estimator.BNEstimator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the Bayesian Network to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tuple</code> <p>a tuple with (X, descriptor, clean_data). If partial is \"structure\", clean_data can be None (not used).</p> required <code>y</code> <code>None</code> <p>not used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BNEstimator</code> <p>The fitted estimator.</p> Source code in <code>applybn/core/estimators/base_estimator.py</code> <pre><code>@_fit_context(prefer_skip_nested_validation=True)\ndef fit(self, X, y=None):\n    \"\"\"\n    Fits the Bayesian Network to the data.\n\n    Args:\n        X (tuple): a tuple with (X, descriptor, clean_data).\n            If partial is \"structure\", clean_data can be None (not used).\n        y (None): not used.\n\n    Returns:\n        self (BNEstimator): The fitted estimator.\n    \"\"\"\n\n    # this has to be done because scikit learn unpacking problem\n    # inside pipeline there is unpacking.\n    X, descriptor, clean_data = X\n    if not self.partial == \"parameters\":\n        if not self.bn_type in [\"hybrid\", \"disc\", \"cont\"]:\n            bn_type_ = self.detect_bn(clean_data)\n        else:\n            bn_type_ = self.bn_type\n\n        bn = self.init_bn(bn_type_)\n\n        self.bn_ = bn\n        self.bn_type = bn_type_\n\n    match self.partial:\n        case \"parameters\":\n            if not self.bn_.edges:\n                raise NotFittedError(\n                    \"Trying to learn parameters on unfitted estimator. Call fit method first.\"\n                )\n            self.bn_.fit_parameters(clean_data)\n        case \"structure\":\n            self.bn_.add_nodes(descriptor)\n            self.bn_.add_edges(X, progress_bar=False, **self.learning_params)\n        case False:\n            self.bn_.add_nodes(descriptor)\n            self.bn_.add_edges(X, progress_bar=False, **self.learning_params)\n            self.bn_.fit_parameters(clean_data)\n\n    return self\n</code></pre>"},{"location":"api/core/estimators/#applybn.core.estimators.base_estimator.BNEstimator.init_bn","title":"<code>init_bn(bn_type)</code>","text":"<p>Initializes the Bayesian Network based on the type.</p> <p>Parameters:</p> Name Type Description Default <code>bn_type</code> <code>Literal['hybrid', 'disc', 'cont']</code> <p>The type of Bayesian Network to initialize.</p> required <p>Returns:</p> Type Description <code>HybridBN | DiscreteBN | ContinuousBN</code> <p>An instance of the corresponding Bayesian Network class.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Invalid bn_type.</p> Source code in <code>applybn/core/estimators/base_estimator.py</code> <pre><code>def init_bn(\n    self, bn_type: Literal[\"hybrid\", \"disc\", \"cont\"]\n) -&gt; HybridBN | DiscreteBN | ContinuousBN:\n    \"\"\"\n    Initializes the Bayesian Network based on the type.\n\n    Args:\n        bn_type: The type of Bayesian Network to initialize.\n\n    Returns:\n        An instance of the corresponding Bayesian Network class.\n\n    Raises:\n        TypeError: Invalid bn_type.\n    \"\"\"\n    str2net = {\"hybrid\": HybridBN, \"disc\": DiscreteBN, \"cont\": ContinuousBN}\n\n    params = dict()\n    match bn_type:\n        case \"hybrid\":\n            params = dict(use_mixture=self.use_mixture, has_logit=self.has_logit)\n        case \"cont\":\n            params = dict(use_mixture=self.use_mixture)\n        case \"disc\":\n            ...\n        case _:\n            raise TypeError(f\"Invalid bn_type, obtained bn_type: {bn_type}\")\n    return str2net[bn_type](**params)\n</code></pre>"},{"location":"api/core/estimators_factory/","title":"\u0424\u0430\u0431\u0440\u0438\u043a\u0430 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u043e\u0432","text":"<p>\u042d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u043e\u0432, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0441 <code>BNEstimator</code>.</p> <p>\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0432 \u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f. </p>"},{"location":"api/core/estimators_factory/#applybn.core.estimators.estimator_factory.EstimatorPipelineFactory","title":"<code>EstimatorPipelineFactory</code>","text":"<p>Factory class to create an estimator pipeline for classification or regression tasks.</p> <p>Attributes:</p> Name Type Description <code>interfaces</code> <code>dict</code> <p>Mapping of task types to their corresponding scikit-learn mixin classes.</p> <code>task_type</code> <code>str</code> <p>The type of task ('classification' or 'regression').</p> <code>estimator_</code> <code>None | BaseEstimator</code> <p>The estimator instance.</p> Source code in <code>applybn/core/estimators/estimator_factory.py</code> <pre><code>class EstimatorPipelineFactory:\n    \"\"\"\n    Factory class to create an estimator pipeline for classification or regression tasks.\n\n    Attributes:\n        interfaces (dict): Mapping of task types to their corresponding scikit-learn mixin classes.\n        task_type (str): The type of task ('classification' or 'regression').\n        estimator_ (None | BaseEstimator): The estimator instance.\n    \"\"\"\n\n    interfaces = {\"classification\": ClassifierMixin, \"regression\": RegressorMixin}\n\n    def __init__(self, task_type: str = \"classification\"):\n        \"\"\"\n        Initializes the EstimatorPipelineFactory with the given task type.\n\n        Args:\n            task_type: The type of task ('classification' or 'regression').\n        \"\"\"\n        self.task_type = task_type\n        self.estimator_ = None\n\n    @staticmethod\n    def convert_bamt_preprocessor(preprocessor: list):\n        \"\"\"\n        Converts a BAMT preprocessor to a BamtPreprocessorWrapper.\n\n        Args:\n            preprocessor: The BAMT preprocessor to convert.\n\n        Returns:\n            BamtPreprocessorWrapper: The wrapped preprocessor.\n        \"\"\"\n        return BamtPreprocessorWrapper(preprocessor)\n\n    def __call__(\n        self, preprocessor: None | list = None, **params: Unpack[BNEstimatorParams]\n    ):\n        \"\"\"\n        Creates a pipeline with the given preprocessor and parameters.\n\n        Args:\n            preprocessor: The preprocessor to use (default is None).\n            **params: Parameters for the BNEstimator.\n\n        Returns:\n            CorePipeline: The constructed pipeline.\n        \"\"\"\n        if preprocessor is None:\n            preprocessor = self.default_preprocessor\n\n        self.estimator.set_params(**params)\n\n        wrapped_preprocessor = self.convert_bamt_preprocessor(preprocessor)\n\n        pipeline = CorePipeline(\n            [(\"preprocessor\", wrapped_preprocessor), (\"bn_estimator\", self.estimator)]\n        )\n        return pipeline\n\n    def _adjust_interface(self) -&gt; None:\n        \"\"\"\n        Adjusts the interface of the estimator based on the task type.\n        \"\"\"\n        interface = self.interfaces[self.task_type]\n        names = {\n            \"regression\": \"RegressorMixin\",\n            \"classification\": \"ClassifierMixin\",\n        }\n        new_class = type(\n            f\"BNEstimatorWith{names[self.task_type]}\", (BNEstimator, interface), {}\n        )\n        self.estimator_ = new_class()\n\n    @property\n    def estimator(self):\n        \"\"\"\n        Returns the estimator instance, creating it if necessary.\n\n        Returns:\n            BNEstimatorMixin: The estimator instance with classifier or regressor interface.\n        \"\"\"\n        if not self.estimator_:\n            self._adjust_interface()\n        return self.estimator_\n\n    @property\n    def default_preprocessor(self):\n        encoder = pp.LabelEncoder()\n        discretizer = pp.KBinsDiscretizer(\n            n_bins=5, encode=\"ordinal\", strategy=\"uniform\"\n        )\n        preprocessor = Preprocessor(\n            [(\"encoder\", encoder), (\"discretizer\", discretizer)]\n        )\n        return preprocessor\n</code></pre>"},{"location":"api/core/estimators_factory/#applybn.core.estimators.estimator_factory.EstimatorPipelineFactory.estimator","title":"<code>estimator</code>  <code>property</code>","text":"<p>Returns the estimator instance, creating it if necessary.</p> <p>Returns:</p> Name Type Description <code>BNEstimatorMixin</code> <p>The estimator instance with classifier or regressor interface.</p>"},{"location":"api/core/estimators_factory/#applybn.core.estimators.estimator_factory.EstimatorPipelineFactory.__call__","title":"<code>__call__(preprocessor=None, **params)</code>","text":"<p>Creates a pipeline with the given preprocessor and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>preprocessor</code> <code>None | list</code> <p>The preprocessor to use (default is None).</p> <code>None</code> <code>**params</code> <code>Unpack[BNEstimatorParams]</code> <p>Parameters for the BNEstimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CorePipeline</code> <p>The constructed pipeline.</p> Source code in <code>applybn/core/estimators/estimator_factory.py</code> <pre><code>def __call__(\n    self, preprocessor: None | list = None, **params: Unpack[BNEstimatorParams]\n):\n    \"\"\"\n    Creates a pipeline with the given preprocessor and parameters.\n\n    Args:\n        preprocessor: The preprocessor to use (default is None).\n        **params: Parameters for the BNEstimator.\n\n    Returns:\n        CorePipeline: The constructed pipeline.\n    \"\"\"\n    if preprocessor is None:\n        preprocessor = self.default_preprocessor\n\n    self.estimator.set_params(**params)\n\n    wrapped_preprocessor = self.convert_bamt_preprocessor(preprocessor)\n\n    pipeline = CorePipeline(\n        [(\"preprocessor\", wrapped_preprocessor), (\"bn_estimator\", self.estimator)]\n    )\n    return pipeline\n</code></pre>"},{"location":"api/core/estimators_factory/#applybn.core.estimators.estimator_factory.EstimatorPipelineFactory.__init__","title":"<code>__init__(task_type='classification')</code>","text":"<p>Initializes the EstimatorPipelineFactory with the given task type.</p> <p>Parameters:</p> Name Type Description Default <code>task_type</code> <code>str</code> <p>The type of task ('classification' or 'regression').</p> <code>'classification'</code> Source code in <code>applybn/core/estimators/estimator_factory.py</code> <pre><code>def __init__(self, task_type: str = \"classification\"):\n    \"\"\"\n    Initializes the EstimatorPipelineFactory with the given task type.\n\n    Args:\n        task_type: The type of task ('classification' or 'regression').\n    \"\"\"\n    self.task_type = task_type\n    self.estimator_ = None\n</code></pre>"},{"location":"api/core/estimators_factory/#applybn.core.estimators.estimator_factory.EstimatorPipelineFactory.convert_bamt_preprocessor","title":"<code>convert_bamt_preprocessor(preprocessor)</code>  <code>staticmethod</code>","text":"<p>Converts a BAMT preprocessor to a BamtPreprocessorWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>preprocessor</code> <code>list</code> <p>The BAMT preprocessor to convert.</p> required <p>Returns:</p> Name Type Description <code>BamtPreprocessorWrapper</code> <p>The wrapped preprocessor.</p> Source code in <code>applybn/core/estimators/estimator_factory.py</code> <pre><code>@staticmethod\ndef convert_bamt_preprocessor(preprocessor: list):\n    \"\"\"\n    Converts a BAMT preprocessor to a BamtPreprocessorWrapper.\n\n    Args:\n        preprocessor: The BAMT preprocessor to convert.\n\n    Returns:\n        BamtPreprocessorWrapper: The wrapped preprocessor.\n    \"\"\"\n    return BamtPreprocessorWrapper(preprocessor)\n</code></pre>"},{"location":"api/core/logger/","title":"\u041b\u043e\u0433\u0433\u0435\u0440","text":"<p>               Bases: <code>Logger</code></p> Source code in <code>applybn/core/logger.py</code> <pre><code>class Logger(logging.Logger):\n    _instances = {}\n\n    def __new__(cls, name, level=logging.INFO, log_file=None):\n        \"\"\"Direct implementation of singleton pattern.\"\"\"\n        if name in cls._instances:\n            return cls._instances[name]\n\n        instance = super().__new__(cls)\n        cls._instances[name] = instance\n        return instance\n\n    def __init__(self, name, level=logging.INFO, log_file=None):\n        \"\"\"Force to rebuild attributes of logger created to avoid side effects of another libs.\"\"\"\n        super().__init__(name, level)\n\n        if not self.hasHandlers():\n            console_handler = RichHandler(rich_tracebacks=True)\n            console_handler.setLevel(level)\n            self.addHandler(console_handler)\n\n            if log_file:\n                file_handler = logging.FileHandler(log_file)\n                file_handler.setLevel(level)\n                file_handler.setFormatter(\n                    logging.Formatter(\n                        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n                    )\n                )\n                self.addHandler(file_handler)\n</code></pre>"},{"location":"api/core/logger/#applybn.core.logger.Logger.__init__","title":"<code>__init__(name, level=logging.INFO, log_file=None)</code>","text":"<p>Force to rebuild attributes of logger created to avoid side effects of another libs.</p> Source code in <code>applybn/core/logger.py</code> <pre><code>def __init__(self, name, level=logging.INFO, log_file=None):\n    \"\"\"Force to rebuild attributes of logger created to avoid side effects of another libs.\"\"\"\n    super().__init__(name, level)\n\n    if not self.hasHandlers():\n        console_handler = RichHandler(rich_tracebacks=True)\n        console_handler.setLevel(level)\n        self.addHandler(console_handler)\n\n        if log_file:\n            file_handler = logging.FileHandler(log_file)\n            file_handler.setLevel(level)\n            file_handler.setFormatter(\n                logging.Formatter(\n                    \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n                )\n            )\n            self.addHandler(file_handler)\n</code></pre>"},{"location":"api/core/logger/#applybn.core.logger.Logger.__new__","title":"<code>__new__(name, level=logging.INFO, log_file=None)</code>","text":"<p>Direct implementation of singleton pattern.</p> Source code in <code>applybn/core/logger.py</code> <pre><code>def __new__(cls, name, level=logging.INFO, log_file=None):\n    \"\"\"Direct implementation of singleton pattern.\"\"\"\n    if name in cls._instances:\n        return cls._instances[name]\n\n    instance = super().__new__(cls)\n    cls._instances[name] = instance\n    return instance\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/","title":"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 CNN","text":"<p>\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> <p>Causal CNN Explainer for measuring filter importance in a convolutional neural network.</p> <p>This class extracts convolutional layers from a CNN, builds a Directed Acyclic Graph (DAG) representation of the filters, learns structural equations using linear regression, and computes filter importances. It also provides filter pruning and evaluation methods.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>The CNN model to analyze.</p> <code>device</code> <code>device</code> <p>The device (CPU or CUDA) for computations.</p> <code>conv_layers</code> <code>list</code> <p>List of (layer_name, layer_module) for all convolutional layers.</p> <code>dag</code> <code>dict</code> <p>DAG representation of the CNN filters per layer.</p> <code>filter_outputs</code> <code>dict</code> <p>Collected intermediate outputs for each convolutional layer.</p> <code>parent_outputs</code> <code>dict</code> <p>Collected intermediate outputs for parent layers.</p> <code>filter_importances</code> <code>dict</code> <p>Importance scores for the filters of each layer.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>class CausalCNNExplainer:\n    \"\"\"Causal CNN Explainer for measuring filter importance in a convolutional neural network.\n\n    This class extracts convolutional layers from a CNN, builds a Directed Acyclic Graph (DAG)\n    representation of the filters, learns structural equations using linear regression,\n    and computes filter importances. It also provides filter pruning and evaluation methods.\n\n    Attributes:\n        model (nn.Module): The CNN model to analyze.\n        device (torch.device): The device (CPU or CUDA) for computations.\n        conv_layers (list): List of (layer_name, layer_module) for all convolutional layers.\n        dag (dict): DAG representation of the CNN filters per layer.\n        filter_outputs (dict): Collected intermediate outputs for each convolutional layer.\n        parent_outputs (dict): Collected intermediate outputs for parent layers.\n        filter_importances (dict): Importance scores for the filters of each layer.\n    \"\"\"\n\n    def __init__(self, model: nn.Module, device: torch.device | None = None):\n        \"\"\"Initializes the CausalCNNExplainer object.\n\n        Args:\n            model:\n                A PyTorch CNN model.\n            device:\n                The device (CPU or CUDA) for computations. Defaults to None (CPU if not specified).\n        \"\"\"\n        if device is None:\n            device = torch.device(\"cpu\")\n        self.device = device\n        self.model = model.to(self.device)\n\n        self.conv_layers = []\n        self.dag = {}\n        self.filter_outputs = {}\n        self.parent_outputs = {}\n        self.filter_importances = {}\n\n        self._extract_conv_layers()\n        self._build_dag()\n        self._initialize_importances()\n\n    def _extract_conv_layers(self):\n        \"\"\"Extracts convolutional layers recursively from the model.\"\"\"\n        self.conv_layers = []\n\n        def recursive_extract(module, name_prefix=\"\"):\n            for name, layer in module.named_children():\n                full_name = f\"{name_prefix}.{name}\" if name_prefix else name\n                if isinstance(layer, nn.Conv2d):\n                    self.conv_layers.append((full_name, layer))\n                else:\n                    recursive_extract(layer, full_name)\n\n        recursive_extract(self.model)\n\n    def _build_dag(self):\n        \"\"\"Builds a simple DAG structure, where each layer depends on the previous one.\"\"\"\n        self.dag = {\n            idx: {\n                \"name\": name,\n                \"layer\": layer,\n                \"filters\": list(range(layer.out_channels)),\n                \"parents\": [idx - 1] if idx &gt; 0 else [],\n                \"children\": [idx + 1] if idx &lt; len(self.conv_layers) - 1 else [],\n            }\n            for idx, (name, layer) in enumerate(self.conv_layers)\n        }\n\n    def _initialize_importances(self):\n        \"\"\"Initializes filter importance arrays for each layer.\"\"\"\n        for idx in self.dag.keys():\n            num_filters = len(self.dag[idx][\"filters\"])\n            self.filter_importances[idx] = np.zeros(num_filters)\n\n    def collect_data(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        frobenius_norm_func: Callable | None = None,\n    ):\n        \"\"\"Collects output data for each convolutional layer.\n\n        Args:\n            data_loader:\n                DataLoader for the dataset whose activations are collected.\n            frobenius_norm_func:\n                A function to compute the Frobenius norm over feature maps.\n                Defaults to a built-in Frobenius norm if None is provided.\n        \"\"\"\n        if frobenius_norm_func is None:\n            frobenius_norm_func = self._frobenius_norm\n\n        # Internal dictionary to store activations\n        activation_storage = {}\n\n        def get_activation(name):\n            def hook(_, __, output):\n                activation_storage[name] = output.detach()\n\n            return hook\n\n        # Register forward hooks\n        hooks = []\n        for idx, (name, layer) in enumerate(self.conv_layers):\n            hooks.append(layer.register_forward_hook(get_activation(name)))\n\n        # Collect data by running forward passes\n        self.filter_outputs.clear()\n        self.parent_outputs.clear()\n\n        with torch.no_grad():\n            for images, _ in data_loader:\n                images = images.to(self.device)\n                _ = self.model(images)  # Forward pass\n\n                for idx, (layer_name, _) in enumerate(self.conv_layers):\n                    output = activation_storage[layer_name]\n                    transformed = frobenius_norm_func(output).cpu().numpy()\n                    if idx not in self.filter_outputs:\n                        self.filter_outputs[idx] = transformed\n                    else:\n                        self.filter_outputs[idx] = np.concatenate(\n                            (self.filter_outputs[idx], transformed), axis=0\n                        )\n\n        # Remove hooks\n        for hook in hooks:\n            hook.remove()\n\n        # Collect parent outputs\n        for idx in self.dag.keys():\n            parents = self.dag[idx][\"parents\"]\n            if parents:\n                parent_idx = parents[0]\n                self.parent_outputs[idx] = self.filter_outputs.get(parent_idx, None)\n            else:\n                # First convolutional layer has no parent\n                self.parent_outputs[idx] = None\n\n    def learn_structural_equations(self):\n        \"\"\"Learns structural equations using linear regression and updates filter importances.\"\"\"\n        for idx in self.dag.keys():\n            y = self.filter_outputs.get(idx, None)\n            X = self.parent_outputs.get(idx, None)\n\n            # Skip if there is no parent output (first layer) or data is missing\n            if X is None or y is None:\n                continue\n\n            # Flatten X and y\n            X_flat = X.reshape(-1, X.shape[-1])\n            y_flat = y.reshape(-1, y.shape[-1])\n\n            num_parent_filters = X_flat.shape[1]\n            num_child_filters = y_flat.shape[1]\n            importance_accumulator = np.zeros(num_parent_filters)\n\n            # Train regression for each child filter\n            for i in range(num_child_filters):\n                model_reg = LinearRegression()\n                model_reg.fit(X_flat, y_flat[:, i])\n                coeffs = np.abs(model_reg.coef_)\n                importance_accumulator += coeffs\n\n            # Accumulate importance to the parent filters\n            parent_idx = self.dag[idx][\"parents\"][0]\n            self.filter_importances[parent_idx] += importance_accumulator\n\n        # Normalize importance scores\n        for idx in self.filter_importances.keys():\n            importance = self.filter_importances[idx]\n            total = np.sum(importance)\n            if total != 0:\n                self.filter_importances[idx] = importance / total\n            else:\n                # Default uniform distribution if all coefficients sum to zero\n                num_filters = len(importance)\n                self.filter_importances[idx] = np.ones(num_filters) / num_filters\n\n    def get_filter_importances(self) -&gt; dict:\n        \"\"\"Returns the computed filter importances.\n\n        Returns:\n            A dictionary mapping layer index to a NumPy array of importance scores.\n        \"\"\"\n        return self.filter_importances\n\n    def prune_filters_by_importance(self, percent: float) -&gt; nn.Module:\n        \"\"\"Prunes filters with the lowest importance scores by zeroing out their weights.\n\n        Args:\n            percent:\n                Percentage of filters to prune in each convolutional layer.\n\n        Returns:\n            A copy of the model with pruned (zeroed) filters.\n        \"\"\"\n        pruned_model = copy.deepcopy(self.model)\n        filters_to_prune = {}\n\n        # Calculate which filters to prune in each layer\n        for idx in self.dag.keys():\n            importance = self.filter_importances.get(idx)\n            if importance is not None:\n                num_filters = len(importance)\n                n_prune = int(num_filters * percent / 100)\n                if n_prune == 0 and percent &gt; 0:\n                    n_prune = 1\n                prune_indices = np.argsort(importance)[:n_prune]\n                filters_to_prune[idx] = prune_indices\n            else:\n                filters_to_prune[idx] = []\n\n        # Prune by zeroing weights\n        with torch.no_grad():\n            for idx, prune_indices in filters_to_prune.items():\n                name = self.dag[idx][\"name\"]\n                layer = self._get_submodule(pruned_model, name)\n                for f_idx in prune_indices:\n                    if f_idx &lt; layer.weight.shape[0]:\n                        layer.weight[f_idx] = 0\n                        if layer.bias is not None:\n                            layer.bias[f_idx] = 0\n\n        return pruned_model\n\n    def prune_random_filters(self, percent: float) -&gt; nn.Module:\n        \"\"\"Randomly prunes a specified percentage of filters by zeroing out their weights.\n\n        Args:\n            percent:\n                Percentage of filters to prune in each convolutional layer.\n\n        Returns:\n            A copy of the model with pruned (zeroed) filters.\n        \"\"\"\n        pruned_model = copy.deepcopy(self.model)\n\n        with torch.no_grad():\n            for idx in self.dag.keys():\n                num_filters = self.dag[idx][\"layer\"].out_channels\n                n_prune = int(num_filters * percent / 100)\n                if n_prune == 0 and percent &gt; 0:\n                    n_prune = 1\n                prune_indices = random.sample(range(num_filters), n_prune)\n\n                name = self.dag[idx][\"name\"]\n                layer = self._get_submodule(pruned_model, name)\n                for f_idx in prune_indices:\n                    if f_idx &lt; layer.weight.shape[0]:\n                        layer.weight[f_idx] = 0\n                        if layer.bias is not None:\n                            layer.bias[f_idx] = 0\n\n        return pruned_model\n\n    def evaluate_model(\n        self, model: nn.Module, data_loader: torch.utils.data.DataLoader\n    ) -&gt; float:\n        \"\"\"Evaluates the accuracy of the model on a given DataLoader.\n\n        Args:\n            model:\n                The pruned or original model to evaluate.\n            data_loader:\n                The DataLoader to use for evaluation.\n\n        Returns:\n            Accuracy of the model on the provided data (0 to 1).\n        \"\"\"\n        model.eval()\n        model.to(self.device)\n\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in data_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        return correct / total\n\n    @staticmethod\n    def _frobenius_norm(tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Default Frobenius norm function that averages over spatial dimensions.\n\n        Args:\n            tensor: A 4D tensor (batch_size, channels, height, width).\n\n        Returns:\n            A 2D tensor (batch_size, channels) with the Frobenius norm computed over spatial dimensions.\n        \"\"\"\n        return torch.norm(tensor.view(tensor.size(0), tensor.size(1), -1), dim=2)\n\n    @staticmethod\n    def _get_submodule(model: nn.Module, target: str) -&gt; nn.Module:\n        \"\"\"Helper function to retrieve a submodule by hierarchical name.\"\"\"\n        names = target.split(\".\")\n        submodule = model\n        for name in names:\n            if name.isdigit():\n                submodule = submodule[int(name)]\n            else:\n                submodule = getattr(submodule, name)\n        return submodule\n\n    def visualize_heatmap_on_input(\n        self,\n        image_tensor: torch.Tensor,\n        alpha: float = 0.5,\n        cmap: str = \"viridis\",\n        figsize: tuple[int, int] = (15, 5),\n    ):\n        \"\"\"Shows original image, heatmap, and overlay side-by-side.\"\"\"\n        if image_tensor.dim() == 3:\n            image_tensor = image_tensor.unsqueeze(0)\n        image_tensor = image_tensor.to(self.device)\n\n        # Capture first-layer activations\n        first_layer_idx = 0\n        layer_name, layer = self.conv_layers[first_layer_idx]\n        activation = None\n\n        def hook(_, __, output):\n            nonlocal activation\n            activation = output.detach()\n\n        handle = layer.register_forward_hook(hook)\n        with torch.no_grad():\n            _ = self.model(image_tensor)\n        handle.remove()\n\n        # Convert importance scores to tensor\n        importances = torch.from_numpy(self.filter_importances[first_layer_idx]).to(\n            activation.device\n        )\n\n        # Compute weighted activations\n        activations = activation.squeeze(0)  # Remove batch dimension\n        weighted_activations = activations * importances[:, None, None]\n        heatmap = weighted_activations.sum(dim=0)\n\n        # Normalize and convert to numpy\n        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n        heatmap_np = heatmap.cpu().numpy()\n\n        # Process input image (denormalize)\n        img = image_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n\n        # Resize heatmap to match input size\n        heatmap_resized = cv2.resize(heatmap_np, (img.shape[1], img.shape[0]))\n\n        # Create subplots\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n\n        # Original image\n        ax1.imshow(img)\n        ax1.set_title(\"Original Image\")\n        ax1.axis(\"off\")\n\n        # Heatmap alone\n        ax2.imshow(heatmap_resized, cmap=cmap)\n        ax2.set_title(\"Filter Importance Heatmap\")\n        ax2.axis(\"off\")\n\n        # Overlay\n        ax3.imshow(img)\n        ax3.imshow(heatmap_resized, alpha=alpha, cmap=cmap)\n        ax3.set_title(\"Heatmap Overlay\")\n        ax3.axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\n    def visualize_first_layer_filters(\n        self, n_filters: int = 16, figsize: tuple[int, int] = (12, 8)\n    ):\n        \"\"\"Visualizes weights of the first convolutional layer's filters.\"\"\"\n        first_layer_idx = 0\n        layer_name, layer = self.conv_layers[first_layer_idx]\n\n        # Move weights to CPU for visualization\n        weights = layer.weight.detach().cpu().numpy()\n        importances = self.filter_importances[first_layer_idx]\n\n        n_filters = min(n_filters, weights.shape[0])\n        rows = (n_filters + 3) // 4\n\n        fig, axes = plt.subplots(rows, 4, figsize=figsize)\n        axes = axes.ravel()\n\n        for i in range(n_filters):\n            filter_weights = weights[i]\n            filter_weights = (filter_weights - filter_weights.min()) / (\n                filter_weights.max() - filter_weights.min()\n            )\n\n            if filter_weights.shape[0] == 3:\n                filter_img = filter_weights.transpose(1, 2, 0)\n            else:\n                filter_img = filter_weights[0]\n                filter_img = np.stack([filter_img] * 3, axis=-1)\n\n            axes[i].imshow(filter_img)\n            axes[i].set_title(f\"Filter {i}\\nImportance: {importances[i]:.3f}\")\n            axes[i].axis(\"off\")\n\n        for j in range(n_filters, len(axes)):\n            axes[j].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\n    def visualize_filter_tsne(\n        self, layer_idx: int = 0, figsize: tuple[int, int] = (8, 6)\n    ):\n        \"\"\"Visualizes filter weights using t-SNE (for higher-dimensional layers).\"\"\"\n        layer_name, layer = self.conv_layers[layer_idx]\n        weights = layer.weight.detach().cpu().numpy()\n        n_filters = weights.shape[0]\n\n        # Flatten filter weights\n        flat_weights = weights.reshape(n_filters, -1)\n\n        # Reduce to 2D with t-SNE\n        tsne = TSNE(n_components=2, random_state=42)\n        embeddings = tsne.fit_transform(flat_weights)\n\n        # Color by importance\n        importances = self.filter_importances[layer_idx]\n        plt.figure(figsize=figsize)\n        plt.scatter(\n            embeddings[:, 0], embeddings[:, 1], c=importances, cmap=\"viridis\", alpha=0.7\n        )\n        plt.colorbar(label=\"Filter Importance\")\n        plt.title(f\"t-SNE of Filter Weights (Layer {layer_idx})\")\n        plt.xlabel(\"t-SNE 1\")\n        plt.ylabel(\"t-SNE 2\")\n        plt.grid(True)\n        plt.show()\n\n    def plot_importance_distribution(self, figsize: tuple[int, int] = (10, 6)):\n        \"\"\"Plots boxplots of filter importance distributions across layers.\"\"\"\n        layer_indices = sorted(self.filter_importances.keys())\n        importances = [self.filter_importances[idx] for idx in layer_indices]\n\n        plt.figure(figsize=figsize)\n        plt.boxplot(importances, labels=layer_indices)\n        plt.xlabel(\"Layer Index\")\n        plt.ylabel(\"Filter Importance Score\")\n        plt.title(\"Distribution of Filter Importances Across Layers\")\n        plt.grid(True)\n        plt.show()\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.__init__","title":"<code>__init__(model, device=None)</code>","text":"<p>Initializes the CausalCNNExplainer object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>A PyTorch CNN model.</p> required <code>device</code> <code>device | None</code> <p>The device (CPU or CUDA) for computations. Defaults to None (CPU if not specified).</p> <code>None</code> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def __init__(self, model: nn.Module, device: torch.device | None = None):\n    \"\"\"Initializes the CausalCNNExplainer object.\n\n    Args:\n        model:\n            A PyTorch CNN model.\n        device:\n            The device (CPU or CUDA) for computations. Defaults to None (CPU if not specified).\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cpu\")\n    self.device = device\n    self.model = model.to(self.device)\n\n    self.conv_layers = []\n    self.dag = {}\n    self.filter_outputs = {}\n    self.parent_outputs = {}\n    self.filter_importances = {}\n\n    self._extract_conv_layers()\n    self._build_dag()\n    self._initialize_importances()\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.collect_data","title":"<code>collect_data(data_loader, frobenius_norm_func=None)</code>","text":"<p>Collects output data for each convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for the dataset whose activations are collected.</p> required <code>frobenius_norm_func</code> <code>Callable | None</code> <p>A function to compute the Frobenius norm over feature maps. Defaults to a built-in Frobenius norm if None is provided.</p> <code>None</code> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def collect_data(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    frobenius_norm_func: Callable | None = None,\n):\n    \"\"\"Collects output data for each convolutional layer.\n\n    Args:\n        data_loader:\n            DataLoader for the dataset whose activations are collected.\n        frobenius_norm_func:\n            A function to compute the Frobenius norm over feature maps.\n            Defaults to a built-in Frobenius norm if None is provided.\n    \"\"\"\n    if frobenius_norm_func is None:\n        frobenius_norm_func = self._frobenius_norm\n\n    # Internal dictionary to store activations\n    activation_storage = {}\n\n    def get_activation(name):\n        def hook(_, __, output):\n            activation_storage[name] = output.detach()\n\n        return hook\n\n    # Register forward hooks\n    hooks = []\n    for idx, (name, layer) in enumerate(self.conv_layers):\n        hooks.append(layer.register_forward_hook(get_activation(name)))\n\n    # Collect data by running forward passes\n    self.filter_outputs.clear()\n    self.parent_outputs.clear()\n\n    with torch.no_grad():\n        for images, _ in data_loader:\n            images = images.to(self.device)\n            _ = self.model(images)  # Forward pass\n\n            for idx, (layer_name, _) in enumerate(self.conv_layers):\n                output = activation_storage[layer_name]\n                transformed = frobenius_norm_func(output).cpu().numpy()\n                if idx not in self.filter_outputs:\n                    self.filter_outputs[idx] = transformed\n                else:\n                    self.filter_outputs[idx] = np.concatenate(\n                        (self.filter_outputs[idx], transformed), axis=0\n                    )\n\n    # Remove hooks\n    for hook in hooks:\n        hook.remove()\n\n    # Collect parent outputs\n    for idx in self.dag.keys():\n        parents = self.dag[idx][\"parents\"]\n        if parents:\n            parent_idx = parents[0]\n            self.parent_outputs[idx] = self.filter_outputs.get(parent_idx, None)\n        else:\n            # First convolutional layer has no parent\n            self.parent_outputs[idx] = None\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.evaluate_model","title":"<code>evaluate_model(model, data_loader)</code>","text":"<p>Evaluates the accuracy of the model on a given DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The pruned or original model to evaluate.</p> required <code>data_loader</code> <code>DataLoader</code> <p>The DataLoader to use for evaluation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Accuracy of the model on the provided data (0 to 1).</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def evaluate_model(\n    self, model: nn.Module, data_loader: torch.utils.data.DataLoader\n) -&gt; float:\n    \"\"\"Evaluates the accuracy of the model on a given DataLoader.\n\n    Args:\n        model:\n            The pruned or original model to evaluate.\n        data_loader:\n            The DataLoader to use for evaluation.\n\n    Returns:\n        Accuracy of the model on the provided data (0 to 1).\n    \"\"\"\n    model.eval()\n    model.to(self.device)\n\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in data_loader:\n            images, labels = images.to(self.device), labels.to(self.device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.get_filter_importances","title":"<code>get_filter_importances()</code>","text":"<p>Returns the computed filter importances.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping layer index to a NumPy array of importance scores.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def get_filter_importances(self) -&gt; dict:\n    \"\"\"Returns the computed filter importances.\n\n    Returns:\n        A dictionary mapping layer index to a NumPy array of importance scores.\n    \"\"\"\n    return self.filter_importances\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.learn_structural_equations","title":"<code>learn_structural_equations()</code>","text":"<p>Learns structural equations using linear regression and updates filter importances.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def learn_structural_equations(self):\n    \"\"\"Learns structural equations using linear regression and updates filter importances.\"\"\"\n    for idx in self.dag.keys():\n        y = self.filter_outputs.get(idx, None)\n        X = self.parent_outputs.get(idx, None)\n\n        # Skip if there is no parent output (first layer) or data is missing\n        if X is None or y is None:\n            continue\n\n        # Flatten X and y\n        X_flat = X.reshape(-1, X.shape[-1])\n        y_flat = y.reshape(-1, y.shape[-1])\n\n        num_parent_filters = X_flat.shape[1]\n        num_child_filters = y_flat.shape[1]\n        importance_accumulator = np.zeros(num_parent_filters)\n\n        # Train regression for each child filter\n        for i in range(num_child_filters):\n            model_reg = LinearRegression()\n            model_reg.fit(X_flat, y_flat[:, i])\n            coeffs = np.abs(model_reg.coef_)\n            importance_accumulator += coeffs\n\n        # Accumulate importance to the parent filters\n        parent_idx = self.dag[idx][\"parents\"][0]\n        self.filter_importances[parent_idx] += importance_accumulator\n\n    # Normalize importance scores\n    for idx in self.filter_importances.keys():\n        importance = self.filter_importances[idx]\n        total = np.sum(importance)\n        if total != 0:\n            self.filter_importances[idx] = importance / total\n        else:\n            # Default uniform distribution if all coefficients sum to zero\n            num_filters = len(importance)\n            self.filter_importances[idx] = np.ones(num_filters) / num_filters\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.plot_importance_distribution","title":"<code>plot_importance_distribution(figsize=(10, 6))</code>","text":"<p>Plots boxplots of filter importance distributions across layers.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def plot_importance_distribution(self, figsize: tuple[int, int] = (10, 6)):\n    \"\"\"Plots boxplots of filter importance distributions across layers.\"\"\"\n    layer_indices = sorted(self.filter_importances.keys())\n    importances = [self.filter_importances[idx] for idx in layer_indices]\n\n    plt.figure(figsize=figsize)\n    plt.boxplot(importances, labels=layer_indices)\n    plt.xlabel(\"Layer Index\")\n    plt.ylabel(\"Filter Importance Score\")\n    plt.title(\"Distribution of Filter Importances Across Layers\")\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.prune_filters_by_importance","title":"<code>prune_filters_by_importance(percent)</code>","text":"<p>Prunes filters with the lowest importance scores by zeroing out their weights.</p> <p>Parameters:</p> Name Type Description Default <code>percent</code> <code>float</code> <p>Percentage of filters to prune in each convolutional layer.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>A copy of the model with pruned (zeroed) filters.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def prune_filters_by_importance(self, percent: float) -&gt; nn.Module:\n    \"\"\"Prunes filters with the lowest importance scores by zeroing out their weights.\n\n    Args:\n        percent:\n            Percentage of filters to prune in each convolutional layer.\n\n    Returns:\n        A copy of the model with pruned (zeroed) filters.\n    \"\"\"\n    pruned_model = copy.deepcopy(self.model)\n    filters_to_prune = {}\n\n    # Calculate which filters to prune in each layer\n    for idx in self.dag.keys():\n        importance = self.filter_importances.get(idx)\n        if importance is not None:\n            num_filters = len(importance)\n            n_prune = int(num_filters * percent / 100)\n            if n_prune == 0 and percent &gt; 0:\n                n_prune = 1\n            prune_indices = np.argsort(importance)[:n_prune]\n            filters_to_prune[idx] = prune_indices\n        else:\n            filters_to_prune[idx] = []\n\n    # Prune by zeroing weights\n    with torch.no_grad():\n        for idx, prune_indices in filters_to_prune.items():\n            name = self.dag[idx][\"name\"]\n            layer = self._get_submodule(pruned_model, name)\n            for f_idx in prune_indices:\n                if f_idx &lt; layer.weight.shape[0]:\n                    layer.weight[f_idx] = 0\n                    if layer.bias is not None:\n                        layer.bias[f_idx] = 0\n\n    return pruned_model\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.prune_random_filters","title":"<code>prune_random_filters(percent)</code>","text":"<p>Randomly prunes a specified percentage of filters by zeroing out their weights.</p> <p>Parameters:</p> Name Type Description Default <code>percent</code> <code>float</code> <p>Percentage of filters to prune in each convolutional layer.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>A copy of the model with pruned (zeroed) filters.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def prune_random_filters(self, percent: float) -&gt; nn.Module:\n    \"\"\"Randomly prunes a specified percentage of filters by zeroing out their weights.\n\n    Args:\n        percent:\n            Percentage of filters to prune in each convolutional layer.\n\n    Returns:\n        A copy of the model with pruned (zeroed) filters.\n    \"\"\"\n    pruned_model = copy.deepcopy(self.model)\n\n    with torch.no_grad():\n        for idx in self.dag.keys():\n            num_filters = self.dag[idx][\"layer\"].out_channels\n            n_prune = int(num_filters * percent / 100)\n            if n_prune == 0 and percent &gt; 0:\n                n_prune = 1\n            prune_indices = random.sample(range(num_filters), n_prune)\n\n            name = self.dag[idx][\"name\"]\n            layer = self._get_submodule(pruned_model, name)\n            for f_idx in prune_indices:\n                if f_idx &lt; layer.weight.shape[0]:\n                    layer.weight[f_idx] = 0\n                    if layer.bias is not None:\n                        layer.bias[f_idx] = 0\n\n    return pruned_model\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.visualize_filter_tsne","title":"<code>visualize_filter_tsne(layer_idx=0, figsize=(8, 6))</code>","text":"<p>Visualizes filter weights using t-SNE (for higher-dimensional layers).</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def visualize_filter_tsne(\n    self, layer_idx: int = 0, figsize: tuple[int, int] = (8, 6)\n):\n    \"\"\"Visualizes filter weights using t-SNE (for higher-dimensional layers).\"\"\"\n    layer_name, layer = self.conv_layers[layer_idx]\n    weights = layer.weight.detach().cpu().numpy()\n    n_filters = weights.shape[0]\n\n    # Flatten filter weights\n    flat_weights = weights.reshape(n_filters, -1)\n\n    # Reduce to 2D with t-SNE\n    tsne = TSNE(n_components=2, random_state=42)\n    embeddings = tsne.fit_transform(flat_weights)\n\n    # Color by importance\n    importances = self.filter_importances[layer_idx]\n    plt.figure(figsize=figsize)\n    plt.scatter(\n        embeddings[:, 0], embeddings[:, 1], c=importances, cmap=\"viridis\", alpha=0.7\n    )\n    plt.colorbar(label=\"Filter Importance\")\n    plt.title(f\"t-SNE of Filter Weights (Layer {layer_idx})\")\n    plt.xlabel(\"t-SNE 1\")\n    plt.ylabel(\"t-SNE 2\")\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.visualize_first_layer_filters","title":"<code>visualize_first_layer_filters(n_filters=16, figsize=(12, 8))</code>","text":"<p>Visualizes weights of the first convolutional layer's filters.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def visualize_first_layer_filters(\n    self, n_filters: int = 16, figsize: tuple[int, int] = (12, 8)\n):\n    \"\"\"Visualizes weights of the first convolutional layer's filters.\"\"\"\n    first_layer_idx = 0\n    layer_name, layer = self.conv_layers[first_layer_idx]\n\n    # Move weights to CPU for visualization\n    weights = layer.weight.detach().cpu().numpy()\n    importances = self.filter_importances[first_layer_idx]\n\n    n_filters = min(n_filters, weights.shape[0])\n    rows = (n_filters + 3) // 4\n\n    fig, axes = plt.subplots(rows, 4, figsize=figsize)\n    axes = axes.ravel()\n\n    for i in range(n_filters):\n        filter_weights = weights[i]\n        filter_weights = (filter_weights - filter_weights.min()) / (\n            filter_weights.max() - filter_weights.min()\n        )\n\n        if filter_weights.shape[0] == 3:\n            filter_img = filter_weights.transpose(1, 2, 0)\n        else:\n            filter_img = filter_weights[0]\n            filter_img = np.stack([filter_img] * 3, axis=-1)\n\n        axes[i].imshow(filter_img)\n        axes[i].set_title(f\"Filter {i}\\nImportance: {importances[i]:.3f}\")\n        axes[i].axis(\"off\")\n\n    for j in range(n_filters, len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#applybn.explainable.nn_layers_importance.cnn_filter_importance.CausalCNNExplainer.visualize_heatmap_on_input","title":"<code>visualize_heatmap_on_input(image_tensor, alpha=0.5, cmap='viridis', figsize=(15, 5))</code>","text":"<p>Shows original image, heatmap, and overlay side-by-side.</p> Source code in <code>applybn/explainable/nn_layers_importance/cnn_filter_importance.py</code> <pre><code>def visualize_heatmap_on_input(\n    self,\n    image_tensor: torch.Tensor,\n    alpha: float = 0.5,\n    cmap: str = \"viridis\",\n    figsize: tuple[int, int] = (15, 5),\n):\n    \"\"\"Shows original image, heatmap, and overlay side-by-side.\"\"\"\n    if image_tensor.dim() == 3:\n        image_tensor = image_tensor.unsqueeze(0)\n    image_tensor = image_tensor.to(self.device)\n\n    # Capture first-layer activations\n    first_layer_idx = 0\n    layer_name, layer = self.conv_layers[first_layer_idx]\n    activation = None\n\n    def hook(_, __, output):\n        nonlocal activation\n        activation = output.detach()\n\n    handle = layer.register_forward_hook(hook)\n    with torch.no_grad():\n        _ = self.model(image_tensor)\n    handle.remove()\n\n    # Convert importance scores to tensor\n    importances = torch.from_numpy(self.filter_importances[first_layer_idx]).to(\n        activation.device\n    )\n\n    # Compute weighted activations\n    activations = activation.squeeze(0)  # Remove batch dimension\n    weighted_activations = activations * importances[:, None, None]\n    heatmap = weighted_activations.sum(dim=0)\n\n    # Normalize and convert to numpy\n    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n    heatmap_np = heatmap.cpu().numpy()\n\n    # Process input image (denormalize)\n    img = image_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n    img = np.clip(img, 0, 1)\n\n    # Resize heatmap to match input size\n    heatmap_resized = cv2.resize(heatmap_np, (img.shape[1], img.shape[0]))\n\n    # Create subplots\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n\n    # Original image\n    ax1.imshow(img)\n    ax1.set_title(\"Original Image\")\n    ax1.axis(\"off\")\n\n    # Heatmap alone\n    ax2.imshow(heatmap_resized, cmap=cmap)\n    ax2.set_title(\"Filter Importance Heatmap\")\n    ax2.axis(\"off\")\n\n    # Overlay\n    ax3.imshow(img)\n    ax3.imshow(heatmap_resized, alpha=alpha, cmap=cmap)\n    ax3.set_title(\"Heatmap Overlay\")\n    ax3.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/explainable/cnn_filter_importance/#_1","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":""},{"location":"api/explainable/concepts_causal/","title":"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432","text":"<p>\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> <p>A tool for extracting and analyzing causal concepts from tabular datasets.</p> <p>This class provides methods to cluster data, evaluate discriminability of clusters, extract concept definitions, and estimate causal effects on different outcomes.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>class ConceptCausalExplainer:\n    \"\"\"A tool for extracting and analyzing causal concepts from tabular datasets.\n\n    This class provides methods to cluster data, evaluate discriminability of clusters,\n    extract concept definitions, and estimate causal effects on different outcomes.\n    \"\"\"\n\n    @staticmethod\n    def calculate_confidence_uncertainty(\n        X: pd.DataFrame,\n        y: pd.Series | np.ndarray,\n        clf: ClassifierMixin | BaseEstimator,\n    ) -&gt; tuple:\n        \"\"\"Calculate model confidence and aleatoric uncertainty using DataIQ.\n\n        Args:\n            X: Feature matrix.\n            y: Target labels or values.\n            clf: A trained classifier that supports predict_proba or similar.\n\n        Returns:\n            A tuple (confidence, aleatoric_uncertainty) containing:\n                - confidence: Model confidence scores.\n                - aleatoric_uncertainty: Aleatoric uncertainty scores.\n        \"\"\"\n        data_iq = DataIQSKLearn(X=X, y=y)\n        data_iq.on_epoch_end(clf=clf, iteration=10)\n        confidence = data_iq.confidence\n        aleatoric_uncertainty = data_iq.aleatoric\n        return confidence, aleatoric_uncertainty\n\n    @staticmethod\n    def perform_clustering(\n        D: pd.DataFrame, num_clusters: int, random_state=42\n    ) -&gt; np.ndarray:\n        \"\"\"Perform KMeans clustering on the dataset.\n\n        Args:\n            D: The dataset for clustering (without index column).\n            num_clusters: The number of clusters to form.\n            random_state: Seed for KMeans.\n\n        Returns:\n            Array of cluster labels.\n        \"\"\"\n        kmeans = KMeans(n_clusters=num_clusters, random_state=random_state)\n        clusters = kmeans.fit_predict(D)\n        return clusters\n\n    @staticmethod\n    def evaluate_discriminability(\n        D: pd.DataFrame,\n        N: pd.DataFrame,\n        clusters: np.ndarray,\n        auc_threshold: float,\n        k_min_cluster_size: int,\n        random_state=42,\n    ) -&gt; list:\n        \"\"\"Evaluate discriminability of clusters using an SVM and AUC.\n\n        Args:\n            D: The discovery dataset, expected to include an 'index' column.\n            N: The negative (natural) dataset, expected to include an 'index' column.\n            clusters: Cluster labels from perform_clustering.\n            auc_threshold: A threshold for AUC to consider a cluster discriminative.\n            k_min_cluster_size: Minimum cluster size for evaluation.\n            random_state: Seed for splitting and SVC\n\n        Returns:\n            A list of dictionaries containing information about discriminative clusters.\n        \"\"\"\n        discriminative_clusters = []\n        cluster_labels = np.unique(clusters)\n\n        for cluster_label in cluster_labels:\n            cluster_indices = np.where(clusters == cluster_label)[0]\n            if len(cluster_indices) &gt;= k_min_cluster_size:\n                # Prepare data for SVM classifier\n                X_cluster = D.iloc[cluster_indices]\n                y_cluster = np.ones(len(cluster_indices))\n                X_N = N.drop(columns=\"index\")  # Negative class is the natural dataset N\n                y_N = np.zeros(len(X_N))\n                # Combine datasets\n                X_train = pd.concat([X_cluster.drop(columns=\"index\"), X_N], axis=0)\n                y_train = np.concatenate([y_cluster, y_N])\n                # Split into training and validation sets\n                X_train_svm, X_val_svm, y_train_svm, y_val_svm = train_test_split(\n                    X_train, y_train, test_size=0.3, random_state=random_state\n                )\n                # Train SVM\n                S_i = SVC(kernel=\"linear\", probability=True, random_state=random_state)\n                S_i.fit(X_train_svm, y_train_svm)\n                # Evaluate discriminability using AUC\n                y_scores = S_i.decision_function(X_val_svm)\n                auc_score = roc_auc_score(y_val_svm, y_scores)\n                logger.info(f\"Cluster {cluster_label}: AUC = {auc_score:.4f}\")\n                if auc_score &gt; auc_threshold:\n                    # Discriminative cluster found\n                    discriminative_clusters.append(\n                        {\n                            \"classifier\": S_i,\n                            \"cluster_label\": cluster_label,\n                            \"cluster_indices\": D.iloc[cluster_indices][\"index\"].values,\n                            \"auc_score\": auc_score,\n                        }\n                    )\n        return discriminative_clusters\n\n    def extract_concepts(\n        self,\n        D: pd.DataFrame,\n        N: pd.DataFrame,\n        auc_threshold: float = 0.7,\n        k_min_cluster_size: int = 100,\n        max_clusters: int = 10,\n        max_iterations: int = 10,\n    ) -&gt; list:\n        \"\"\"Extract concepts from a discovery dataset.\n\n        Clusters the dataset incrementally and looks for discriminative clusters.\n\n        Args:\n            D: Discovery dataset with an 'index' column.\n            N: Negative (natural) dataset with an 'index' column.\n            auc_threshold: Threshold for AUC to declare a cluster discriminative.\n            k_min_cluster_size: Minimum cluster size for evaluation.\n            max_clusters: Maximum number of clusters to attempt.\n            max_iterations: Maximum iterations for incremental clustering.\n\n        Returns:\n            A list of discriminative cluster dictionaries.\n        \"\"\"\n        svm_classifiers = []\n        cluster_concepts = []\n        no_improvement_counter = 0\n        num_clusters = 9\n        iteration = 0\n\n        while num_clusters &lt;= max_clusters and iteration &lt; max_iterations:\n            iteration += 1\n            logger.info(\n                f\"\\nIteration {iteration}: Clustering with {num_clusters} clusters\"\n            )\n            clusters = self.perform_clustering(D.drop(columns=\"index\"), num_clusters)\n            discriminative_clusters = self.evaluate_discriminability(\n                D, N, clusters, auc_threshold, k_min_cluster_size\n            )\n\n            if discriminative_clusters:\n                for concept in discriminative_clusters:\n                    svm_classifiers.append(concept[\"classifier\"])\n                    cluster_concepts.append(concept)\n                no_improvement_counter = 0\n            else:\n                no_improvement_counter += 1\n\n            if no_improvement_counter &gt;= 3:\n                logger.info(\"No significant improvement in discriminability.\")\n                break\n\n            num_clusters += 1\n\n        return cluster_concepts\n\n    @staticmethod\n    def generate_concept_space(X: pd.DataFrame, cluster_concepts: list) -&gt; pd.DataFrame:\n        \"\"\"Generate a binary concept space from the given cluster concepts.\n\n        Args:\n            X: The entire preprocessed dataset.\n            cluster_concepts: A list of discriminative cluster dictionaries.\n\n        Returns:\n            A DataFrame with binary columns indicating concept membership.\n        \"\"\"\n        A = pd.DataFrame(index=X.index)\n        for idx, concept in enumerate(cluster_concepts):\n            classifier = concept[\"classifier\"]\n            A_i_scores = classifier.decision_function(X)\n            A[f\"Concept_{idx}\"] = (A_i_scores &gt; 0).astype(int)\n        return A\n\n    @staticmethod\n    def select_features_for_concept(\n        concept_data: pd.DataFrame,\n        other_data: pd.DataFrame,\n        features: list,\n        original_data: pd.DataFrame,\n        lambda_reg: float = 0.1,\n    ) -&gt; dict:\n        \"\"\"Select features for a concept and extract value ranges or categories.\n\n        Args:\n            concept_data: Data points belonging to the concept.\n            other_data: Remaining data points not in the concept.\n            features: List of feature names in the preprocessed dataset.\n            original_data: Original dataset (before one-hot encoding).\n            lambda_reg: Regularization parameter to penalize variance or overlap.\n\n        Returns:\n            A dictionary mapping features to their type and range/categories.\n        \"\"\"\n        selected_features = {}\n        for feature in features:\n            X_i_feature = concept_data[feature]\n            X_minus_i_feature = other_data[feature]\n            # For numerical features, calculate the mean difference\n            if (\n                feature in original_data.columns\n                and original_data[feature].dtype != object\n            ):\n                mean_diff = abs(X_i_feature.mean() - X_minus_i_feature.mean())\n                var_within = X_i_feature.var()\n                score = mean_diff - lambda_reg * var_within\n                if score &gt; 0:\n                    # Get value range from original data\n                    original_feature = feature\n                    indices = concept_data.index\n                    X_i_orig_feature = original_data.loc[indices, original_feature]\n                    value_range = (X_i_orig_feature.min(), X_i_orig_feature.max())\n                    selected_features[original_feature] = {\n                        \"type\": \"numeric\",\n                        \"range\": value_range,\n                    }\n            else:\n                # For one-hot encoded categorical features\n                proportion_in_concept = X_i_feature.mean()\n                proportion_in_others = X_minus_i_feature.mean()\n                proportion_diff = proportion_in_concept - proportion_in_others\n                score = abs(proportion_diff) - lambda_reg\n                if score &gt; 0:\n                    # Map back to original feature and category\n                    if \"_\" in feature:\n                        original_feature = \"_\".join(feature.split(\"_\")[:-1])\n                        category = feature.split(\"_\")[-1]\n                        selected_features.setdefault(\n                            original_feature, {\"type\": \"categorical\", \"categories\": []}\n                        )\n                        selected_features[original_feature][\"categories\"].append(\n                            category\n                        )\n        return selected_features\n\n    def extract_concept_meanings(\n        self, D: pd.DataFrame, cluster_concepts: list, original_data: pd.DataFrame\n    ) -&gt; dict:\n        \"\"\"Extract the meanings (dominant features) of each concept.\n\n        Args:\n            D: Preprocessed discovery dataset with an 'index' column.\n            cluster_concepts: List of discriminative cluster dictionaries.\n            original_data: Original dataset (before one-hot encoding).\n\n        Returns:\n            A dictionary mapping concept names to their selected features and values.\n        \"\"\"\n        selected_features_per_concept = {}\n        features = D.drop(columns=\"index\").columns.tolist()\n\n        for idx, concept in enumerate(cluster_concepts):\n            cluster_indices = concept[\"cluster_indices\"]\n            concept_data = D.set_index(\"index\").loc[cluster_indices]\n            other_indices = D.index.difference(concept_data.index)\n            other_data = D.loc[other_indices]\n            selected_features = self.select_features_for_concept(\n                concept_data, other_data, features, original_data\n            )\n            concept_key = f\"Concept_{idx}\"\n            selected_features_per_concept[concept_key] = selected_features\n            logger.info(f\"\\n{concept_key} selected features and values:\")\n            for feature, details in selected_features.items():\n                if details[\"type\"] == \"numeric\":\n                    logger.info(f\"  {feature}: range {details['range']}\")\n                else:\n                    logger.info(f\"  {feature}: categories {details['categories']}\")\n        return selected_features_per_concept\n\n    @staticmethod\n    def estimate_causal_effects(D_c: pd.DataFrame) -&gt; dict:\n        \"\"\"Estimate the causal effect of each concept on a binary outcome.\n\n        Args:\n            D_c: DataFrame where columns are concepts plus the outcome 'L_f' (binary).\n\n        Returns:\n            Dictionary of concept names to their estimated coefficients (logistic regression).\n        \"\"\"\n        effects = {}\n        outcome = \"L_f\"\n\n        for concept in D_c.columns:\n            if concept != outcome:\n                # Prepare data\n                X = D_c[[concept]].copy()\n                # Control for other concepts\n                other_concepts = [\n                    col for col in D_c.columns if col not in [concept, outcome]\n                ]\n                if other_concepts:\n                    X[other_concepts] = D_c[other_concepts]\n                X = sm.add_constant(X)\n                y = D_c[outcome]\n                # Fit logistic regression\n                model = sm.Logit(y, X).fit(disp=0)\n                # Extract the coefficient for the concept\n                coef = model.params[concept]\n                effects[concept] = coef\n                logger.info(\n                    f\"{concept}: Estimated Coefficient (Causal Effect) = {coef:.4f}\"\n                )\n        return effects\n\n    @staticmethod\n    def estimate_causal_effects_on_continuous_outcomes(\n        D_c: pd.DataFrame, outcome_name: str\n    ) -&gt; dict:\n        \"\"\"Estimate causal effects on continuous outcomes using econML's LinearDML or CausalForestDML.\n\n        Args:\n            D_c: DataFrame where columns include concepts and a continuous outcome.\n            outcome_name: Name of the continuous outcome column.\n\n        Returns:\n            Dictionary of concept names to their estimated causal effect on the outcome.\n        \"\"\"\n        from sklearn.ensemble import RandomForestRegressor\n\n        effects = {}\n        for concept in D_c.columns:\n            if concept != outcome_name:\n                # Define treatment and outcome\n                T = D_c[[concept]].values.ravel()\n                Y = D_c[outcome_name].values\n                # Control for other concepts\n                X_controls = D_c.drop(columns=[concept, outcome_name])\n\n                # Simple heuristic for selecting estimator\n                if X_controls.shape[0] &gt; X_controls.shape[1] * 2:\n                    est = LinearDML(\n                        model_y=RandomForestRegressor(),\n                        model_t=RandomForestRegressor(),\n                        linear_first_stages=False,\n                    )\n                else:\n                    logger.info(\n                        f\"Using CausalForestDML for {concept} due to high dimensionality in controls.\"\n                    )\n                    est = CausalForestDML()\n\n                # Fit model and calculate treatment effect\n                est.fit(Y, T, X=X_controls)\n                treatment_effect = est.effect(X=X_controls)\n\n                # Store the mean effect for the current concept\n                effects[concept] = treatment_effect.mean()\n                logger.info(\n                    f\"{concept}: Estimated Causal Effect = {treatment_effect.mean():.4f}\"\n                )\n\n        return effects\n\n    @staticmethod\n    def plot_tornado(\n        effects_dict: dict,\n        title: str = \"Tornado Plot\",\n        figsize: tuple[int, int] = (10, 6),\n    ):\n        \"\"\"Visualize causal effects using a tornado plot.\n\n        Args:\n            effects_dict: Dictionary of {concept: effect_size}\n            title: Title for the plot\n            figsize: Figure dimensions\n        \"\"\"\n        # Sort effects by absolute value\n        sorted_effects = sorted(\n            effects_dict.items(), key=lambda x: abs(x[1]), reverse=True\n        )\n\n        # Prepare data for plotting\n        concepts = [k for k, v in sorted_effects]\n        values = [v for k, v in sorted_effects]\n        colors = [\n            \"#4C72B0\" if v &gt; 0 else \"#DD8452\" for v in values\n        ]  # Blue for positive, orange for negative\n\n        # Create plot\n        plt.figure(figsize=figsize)\n        y_pos = np.arange(len(concepts))\n\n        # Create horizontal bars\n        bars = plt.barh(y_pos, values, color=colors)\n\n        # Add reference line and styling\n        plt.axvline(0, color=\"black\", linewidth=0.8)\n        plt.yticks(y_pos, concepts)\n        plt.xlabel(\"Causal Effect Size\")\n        plt.title(title)\n        plt.gca().invert_yaxis()  # Largest effect at top\n\n        # Add value labels\n        for bar, value in zip(bars, values):\n            if value &gt; 0:\n                ha = \"left\"\n                xpos = min(value + 0.01, max(values) * 0.95)\n            else:\n                ha = \"right\"\n                xpos = max(value - 0.01, min(values) * 0.95)\n            plt.text(\n                xpos,\n                bar.get_y() + bar.get_height() / 2,\n                f\"{value:.3f}\",\n                ha=ha,\n                va=\"center\",\n                color=\"black\",\n            )\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.calculate_confidence_uncertainty","title":"<code>calculate_confidence_uncertainty(X, y, clf)</code>  <code>staticmethod</code>","text":"<p>Calculate model confidence and aleatoric uncertainty using DataIQ.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Feature matrix.</p> required <code>y</code> <code>Series | ndarray</code> <p>Target labels or values.</p> required <code>clf</code> <code>ClassifierMixin | BaseEstimator</code> <p>A trained classifier that supports predict_proba or similar.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple (confidence, aleatoric_uncertainty) containing: - confidence: Model confidence scores. - aleatoric_uncertainty: Aleatoric uncertainty scores.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef calculate_confidence_uncertainty(\n    X: pd.DataFrame,\n    y: pd.Series | np.ndarray,\n    clf: ClassifierMixin | BaseEstimator,\n) -&gt; tuple:\n    \"\"\"Calculate model confidence and aleatoric uncertainty using DataIQ.\n\n    Args:\n        X: Feature matrix.\n        y: Target labels or values.\n        clf: A trained classifier that supports predict_proba or similar.\n\n    Returns:\n        A tuple (confidence, aleatoric_uncertainty) containing:\n            - confidence: Model confidence scores.\n            - aleatoric_uncertainty: Aleatoric uncertainty scores.\n    \"\"\"\n    data_iq = DataIQSKLearn(X=X, y=y)\n    data_iq.on_epoch_end(clf=clf, iteration=10)\n    confidence = data_iq.confidence\n    aleatoric_uncertainty = data_iq.aleatoric\n    return confidence, aleatoric_uncertainty\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.estimate_causal_effects","title":"<code>estimate_causal_effects(D_c)</code>  <code>staticmethod</code>","text":"<p>Estimate the causal effect of each concept on a binary outcome.</p> <p>Parameters:</p> Name Type Description Default <code>D_c</code> <code>DataFrame</code> <p>DataFrame where columns are concepts plus the outcome 'L_f' (binary).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of concept names to their estimated coefficients (logistic regression).</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef estimate_causal_effects(D_c: pd.DataFrame) -&gt; dict:\n    \"\"\"Estimate the causal effect of each concept on a binary outcome.\n\n    Args:\n        D_c: DataFrame where columns are concepts plus the outcome 'L_f' (binary).\n\n    Returns:\n        Dictionary of concept names to their estimated coefficients (logistic regression).\n    \"\"\"\n    effects = {}\n    outcome = \"L_f\"\n\n    for concept in D_c.columns:\n        if concept != outcome:\n            # Prepare data\n            X = D_c[[concept]].copy()\n            # Control for other concepts\n            other_concepts = [\n                col for col in D_c.columns if col not in [concept, outcome]\n            ]\n            if other_concepts:\n                X[other_concepts] = D_c[other_concepts]\n            X = sm.add_constant(X)\n            y = D_c[outcome]\n            # Fit logistic regression\n            model = sm.Logit(y, X).fit(disp=0)\n            # Extract the coefficient for the concept\n            coef = model.params[concept]\n            effects[concept] = coef\n            logger.info(\n                f\"{concept}: Estimated Coefficient (Causal Effect) = {coef:.4f}\"\n            )\n    return effects\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.estimate_causal_effects_on_continuous_outcomes","title":"<code>estimate_causal_effects_on_continuous_outcomes(D_c, outcome_name)</code>  <code>staticmethod</code>","text":"<p>Estimate causal effects on continuous outcomes using econML's LinearDML or CausalForestDML.</p> <p>Parameters:</p> Name Type Description Default <code>D_c</code> <code>DataFrame</code> <p>DataFrame where columns include concepts and a continuous outcome.</p> required <code>outcome_name</code> <code>str</code> <p>Name of the continuous outcome column.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of concept names to their estimated causal effect on the outcome.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef estimate_causal_effects_on_continuous_outcomes(\n    D_c: pd.DataFrame, outcome_name: str\n) -&gt; dict:\n    \"\"\"Estimate causal effects on continuous outcomes using econML's LinearDML or CausalForestDML.\n\n    Args:\n        D_c: DataFrame where columns include concepts and a continuous outcome.\n        outcome_name: Name of the continuous outcome column.\n\n    Returns:\n        Dictionary of concept names to their estimated causal effect on the outcome.\n    \"\"\"\n    from sklearn.ensemble import RandomForestRegressor\n\n    effects = {}\n    for concept in D_c.columns:\n        if concept != outcome_name:\n            # Define treatment and outcome\n            T = D_c[[concept]].values.ravel()\n            Y = D_c[outcome_name].values\n            # Control for other concepts\n            X_controls = D_c.drop(columns=[concept, outcome_name])\n\n            # Simple heuristic for selecting estimator\n            if X_controls.shape[0] &gt; X_controls.shape[1] * 2:\n                est = LinearDML(\n                    model_y=RandomForestRegressor(),\n                    model_t=RandomForestRegressor(),\n                    linear_first_stages=False,\n                )\n            else:\n                logger.info(\n                    f\"Using CausalForestDML for {concept} due to high dimensionality in controls.\"\n                )\n                est = CausalForestDML()\n\n            # Fit model and calculate treatment effect\n            est.fit(Y, T, X=X_controls)\n            treatment_effect = est.effect(X=X_controls)\n\n            # Store the mean effect for the current concept\n            effects[concept] = treatment_effect.mean()\n            logger.info(\n                f\"{concept}: Estimated Causal Effect = {treatment_effect.mean():.4f}\"\n            )\n\n    return effects\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.evaluate_discriminability","title":"<code>evaluate_discriminability(D, N, clusters, auc_threshold, k_min_cluster_size, random_state=42)</code>  <code>staticmethod</code>","text":"<p>Evaluate discriminability of clusters using an SVM and AUC.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>DataFrame</code> <p>The discovery dataset, expected to include an 'index' column.</p> required <code>N</code> <code>DataFrame</code> <p>The negative (natural) dataset, expected to include an 'index' column.</p> required <code>clusters</code> <code>ndarray</code> <p>Cluster labels from perform_clustering.</p> required <code>auc_threshold</code> <code>float</code> <p>A threshold for AUC to consider a cluster discriminative.</p> required <code>k_min_cluster_size</code> <code>int</code> <p>Minimum cluster size for evaluation.</p> required <code>random_state</code> <p>Seed for splitting and SVC</p> <code>42</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing information about discriminative clusters.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef evaluate_discriminability(\n    D: pd.DataFrame,\n    N: pd.DataFrame,\n    clusters: np.ndarray,\n    auc_threshold: float,\n    k_min_cluster_size: int,\n    random_state=42,\n) -&gt; list:\n    \"\"\"Evaluate discriminability of clusters using an SVM and AUC.\n\n    Args:\n        D: The discovery dataset, expected to include an 'index' column.\n        N: The negative (natural) dataset, expected to include an 'index' column.\n        clusters: Cluster labels from perform_clustering.\n        auc_threshold: A threshold for AUC to consider a cluster discriminative.\n        k_min_cluster_size: Minimum cluster size for evaluation.\n        random_state: Seed for splitting and SVC\n\n    Returns:\n        A list of dictionaries containing information about discriminative clusters.\n    \"\"\"\n    discriminative_clusters = []\n    cluster_labels = np.unique(clusters)\n\n    for cluster_label in cluster_labels:\n        cluster_indices = np.where(clusters == cluster_label)[0]\n        if len(cluster_indices) &gt;= k_min_cluster_size:\n            # Prepare data for SVM classifier\n            X_cluster = D.iloc[cluster_indices]\n            y_cluster = np.ones(len(cluster_indices))\n            X_N = N.drop(columns=\"index\")  # Negative class is the natural dataset N\n            y_N = np.zeros(len(X_N))\n            # Combine datasets\n            X_train = pd.concat([X_cluster.drop(columns=\"index\"), X_N], axis=0)\n            y_train = np.concatenate([y_cluster, y_N])\n            # Split into training and validation sets\n            X_train_svm, X_val_svm, y_train_svm, y_val_svm = train_test_split(\n                X_train, y_train, test_size=0.3, random_state=random_state\n            )\n            # Train SVM\n            S_i = SVC(kernel=\"linear\", probability=True, random_state=random_state)\n            S_i.fit(X_train_svm, y_train_svm)\n            # Evaluate discriminability using AUC\n            y_scores = S_i.decision_function(X_val_svm)\n            auc_score = roc_auc_score(y_val_svm, y_scores)\n            logger.info(f\"Cluster {cluster_label}: AUC = {auc_score:.4f}\")\n            if auc_score &gt; auc_threshold:\n                # Discriminative cluster found\n                discriminative_clusters.append(\n                    {\n                        \"classifier\": S_i,\n                        \"cluster_label\": cluster_label,\n                        \"cluster_indices\": D.iloc[cluster_indices][\"index\"].values,\n                        \"auc_score\": auc_score,\n                    }\n                )\n    return discriminative_clusters\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.extract_concept_meanings","title":"<code>extract_concept_meanings(D, cluster_concepts, original_data)</code>","text":"<p>Extract the meanings (dominant features) of each concept.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>DataFrame</code> <p>Preprocessed discovery dataset with an 'index' column.</p> required <code>cluster_concepts</code> <code>list</code> <p>List of discriminative cluster dictionaries.</p> required <code>original_data</code> <code>DataFrame</code> <p>Original dataset (before one-hot encoding).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping concept names to their selected features and values.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>def extract_concept_meanings(\n    self, D: pd.DataFrame, cluster_concepts: list, original_data: pd.DataFrame\n) -&gt; dict:\n    \"\"\"Extract the meanings (dominant features) of each concept.\n\n    Args:\n        D: Preprocessed discovery dataset with an 'index' column.\n        cluster_concepts: List of discriminative cluster dictionaries.\n        original_data: Original dataset (before one-hot encoding).\n\n    Returns:\n        A dictionary mapping concept names to their selected features and values.\n    \"\"\"\n    selected_features_per_concept = {}\n    features = D.drop(columns=\"index\").columns.tolist()\n\n    for idx, concept in enumerate(cluster_concepts):\n        cluster_indices = concept[\"cluster_indices\"]\n        concept_data = D.set_index(\"index\").loc[cluster_indices]\n        other_indices = D.index.difference(concept_data.index)\n        other_data = D.loc[other_indices]\n        selected_features = self.select_features_for_concept(\n            concept_data, other_data, features, original_data\n        )\n        concept_key = f\"Concept_{idx}\"\n        selected_features_per_concept[concept_key] = selected_features\n        logger.info(f\"\\n{concept_key} selected features and values:\")\n        for feature, details in selected_features.items():\n            if details[\"type\"] == \"numeric\":\n                logger.info(f\"  {feature}: range {details['range']}\")\n            else:\n                logger.info(f\"  {feature}: categories {details['categories']}\")\n    return selected_features_per_concept\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.extract_concepts","title":"<code>extract_concepts(D, N, auc_threshold=0.7, k_min_cluster_size=100, max_clusters=10, max_iterations=10)</code>","text":"<p>Extract concepts from a discovery dataset.</p> <p>Clusters the dataset incrementally and looks for discriminative clusters.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>DataFrame</code> <p>Discovery dataset with an 'index' column.</p> required <code>N</code> <code>DataFrame</code> <p>Negative (natural) dataset with an 'index' column.</p> required <code>auc_threshold</code> <code>float</code> <p>Threshold for AUC to declare a cluster discriminative.</p> <code>0.7</code> <code>k_min_cluster_size</code> <code>int</code> <p>Minimum cluster size for evaluation.</p> <code>100</code> <code>max_clusters</code> <code>int</code> <p>Maximum number of clusters to attempt.</p> <code>10</code> <code>max_iterations</code> <code>int</code> <p>Maximum iterations for incremental clustering.</p> <code>10</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of discriminative cluster dictionaries.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>def extract_concepts(\n    self,\n    D: pd.DataFrame,\n    N: pd.DataFrame,\n    auc_threshold: float = 0.7,\n    k_min_cluster_size: int = 100,\n    max_clusters: int = 10,\n    max_iterations: int = 10,\n) -&gt; list:\n    \"\"\"Extract concepts from a discovery dataset.\n\n    Clusters the dataset incrementally and looks for discriminative clusters.\n\n    Args:\n        D: Discovery dataset with an 'index' column.\n        N: Negative (natural) dataset with an 'index' column.\n        auc_threshold: Threshold for AUC to declare a cluster discriminative.\n        k_min_cluster_size: Minimum cluster size for evaluation.\n        max_clusters: Maximum number of clusters to attempt.\n        max_iterations: Maximum iterations for incremental clustering.\n\n    Returns:\n        A list of discriminative cluster dictionaries.\n    \"\"\"\n    svm_classifiers = []\n    cluster_concepts = []\n    no_improvement_counter = 0\n    num_clusters = 9\n    iteration = 0\n\n    while num_clusters &lt;= max_clusters and iteration &lt; max_iterations:\n        iteration += 1\n        logger.info(\n            f\"\\nIteration {iteration}: Clustering with {num_clusters} clusters\"\n        )\n        clusters = self.perform_clustering(D.drop(columns=\"index\"), num_clusters)\n        discriminative_clusters = self.evaluate_discriminability(\n            D, N, clusters, auc_threshold, k_min_cluster_size\n        )\n\n        if discriminative_clusters:\n            for concept in discriminative_clusters:\n                svm_classifiers.append(concept[\"classifier\"])\n                cluster_concepts.append(concept)\n            no_improvement_counter = 0\n        else:\n            no_improvement_counter += 1\n\n        if no_improvement_counter &gt;= 3:\n            logger.info(\"No significant improvement in discriminability.\")\n            break\n\n        num_clusters += 1\n\n    return cluster_concepts\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.generate_concept_space","title":"<code>generate_concept_space(X, cluster_concepts)</code>  <code>staticmethod</code>","text":"<p>Generate a binary concept space from the given cluster concepts.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The entire preprocessed dataset.</p> required <code>cluster_concepts</code> <code>list</code> <p>A list of discriminative cluster dictionaries.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with binary columns indicating concept membership.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef generate_concept_space(X: pd.DataFrame, cluster_concepts: list) -&gt; pd.DataFrame:\n    \"\"\"Generate a binary concept space from the given cluster concepts.\n\n    Args:\n        X: The entire preprocessed dataset.\n        cluster_concepts: A list of discriminative cluster dictionaries.\n\n    Returns:\n        A DataFrame with binary columns indicating concept membership.\n    \"\"\"\n    A = pd.DataFrame(index=X.index)\n    for idx, concept in enumerate(cluster_concepts):\n        classifier = concept[\"classifier\"]\n        A_i_scores = classifier.decision_function(X)\n        A[f\"Concept_{idx}\"] = (A_i_scores &gt; 0).astype(int)\n    return A\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.perform_clustering","title":"<code>perform_clustering(D, num_clusters, random_state=42)</code>  <code>staticmethod</code>","text":"<p>Perform KMeans clustering on the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>D</code> <code>DataFrame</code> <p>The dataset for clustering (without index column).</p> required <code>num_clusters</code> <code>int</code> <p>The number of clusters to form.</p> required <code>random_state</code> <p>Seed for KMeans.</p> <code>42</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of cluster labels.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef perform_clustering(\n    D: pd.DataFrame, num_clusters: int, random_state=42\n) -&gt; np.ndarray:\n    \"\"\"Perform KMeans clustering on the dataset.\n\n    Args:\n        D: The dataset for clustering (without index column).\n        num_clusters: The number of clusters to form.\n        random_state: Seed for KMeans.\n\n    Returns:\n        Array of cluster labels.\n    \"\"\"\n    kmeans = KMeans(n_clusters=num_clusters, random_state=random_state)\n    clusters = kmeans.fit_predict(D)\n    return clusters\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.plot_tornado","title":"<code>plot_tornado(effects_dict, title='Tornado Plot', figsize=(10, 6))</code>  <code>staticmethod</code>","text":"<p>Visualize causal effects using a tornado plot.</p> <p>Parameters:</p> Name Type Description Default <code>effects_dict</code> <code>dict</code> <p>Dictionary of {concept: effect_size}</p> required <code>title</code> <code>str</code> <p>Title for the plot</p> <code>'Tornado Plot'</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure dimensions</p> <code>(10, 6)</code> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef plot_tornado(\n    effects_dict: dict,\n    title: str = \"Tornado Plot\",\n    figsize: tuple[int, int] = (10, 6),\n):\n    \"\"\"Visualize causal effects using a tornado plot.\n\n    Args:\n        effects_dict: Dictionary of {concept: effect_size}\n        title: Title for the plot\n        figsize: Figure dimensions\n    \"\"\"\n    # Sort effects by absolute value\n    sorted_effects = sorted(\n        effects_dict.items(), key=lambda x: abs(x[1]), reverse=True\n    )\n\n    # Prepare data for plotting\n    concepts = [k for k, v in sorted_effects]\n    values = [v for k, v in sorted_effects]\n    colors = [\n        \"#4C72B0\" if v &gt; 0 else \"#DD8452\" for v in values\n    ]  # Blue for positive, orange for negative\n\n    # Create plot\n    plt.figure(figsize=figsize)\n    y_pos = np.arange(len(concepts))\n\n    # Create horizontal bars\n    bars = plt.barh(y_pos, values, color=colors)\n\n    # Add reference line and styling\n    plt.axvline(0, color=\"black\", linewidth=0.8)\n    plt.yticks(y_pos, concepts)\n    plt.xlabel(\"Causal Effect Size\")\n    plt.title(title)\n    plt.gca().invert_yaxis()  # Largest effect at top\n\n    # Add value labels\n    for bar, value in zip(bars, values):\n        if value &gt; 0:\n            ha = \"left\"\n            xpos = min(value + 0.01, max(values) * 0.95)\n        else:\n            ha = \"right\"\n            xpos = max(value - 0.01, min(values) * 0.95)\n        plt.text(\n            xpos,\n            bar.get_y() + bar.get_height() / 2,\n            f\"{value:.3f}\",\n            ha=ha,\n            va=\"center\",\n            color=\"black\",\n        )\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/explainable/concepts_causal/#applybn.explainable.causal_analysis.ConceptCausalExplainer.select_features_for_concept","title":"<code>select_features_for_concept(concept_data, other_data, features, original_data, lambda_reg=0.1)</code>  <code>staticmethod</code>","text":"<p>Select features for a concept and extract value ranges or categories.</p> <p>Parameters:</p> Name Type Description Default <code>concept_data</code> <code>DataFrame</code> <p>Data points belonging to the concept.</p> required <code>other_data</code> <code>DataFrame</code> <p>Remaining data points not in the concept.</p> required <code>features</code> <code>list</code> <p>List of feature names in the preprocessed dataset.</p> required <code>original_data</code> <code>DataFrame</code> <p>Original dataset (before one-hot encoding).</p> required <code>lambda_reg</code> <code>float</code> <p>Regularization parameter to penalize variance or overlap.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping features to their type and range/categories.</p> Source code in <code>applybn/explainable/causal_analysis/concept_causal_effect.py</code> <pre><code>@staticmethod\ndef select_features_for_concept(\n    concept_data: pd.DataFrame,\n    other_data: pd.DataFrame,\n    features: list,\n    original_data: pd.DataFrame,\n    lambda_reg: float = 0.1,\n) -&gt; dict:\n    \"\"\"Select features for a concept and extract value ranges or categories.\n\n    Args:\n        concept_data: Data points belonging to the concept.\n        other_data: Remaining data points not in the concept.\n        features: List of feature names in the preprocessed dataset.\n        original_data: Original dataset (before one-hot encoding).\n        lambda_reg: Regularization parameter to penalize variance or overlap.\n\n    Returns:\n        A dictionary mapping features to their type and range/categories.\n    \"\"\"\n    selected_features = {}\n    for feature in features:\n        X_i_feature = concept_data[feature]\n        X_minus_i_feature = other_data[feature]\n        # For numerical features, calculate the mean difference\n        if (\n            feature in original_data.columns\n            and original_data[feature].dtype != object\n        ):\n            mean_diff = abs(X_i_feature.mean() - X_minus_i_feature.mean())\n            var_within = X_i_feature.var()\n            score = mean_diff - lambda_reg * var_within\n            if score &gt; 0:\n                # Get value range from original data\n                original_feature = feature\n                indices = concept_data.index\n                X_i_orig_feature = original_data.loc[indices, original_feature]\n                value_range = (X_i_orig_feature.min(), X_i_orig_feature.max())\n                selected_features[original_feature] = {\n                    \"type\": \"numeric\",\n                    \"range\": value_range,\n                }\n        else:\n            # For one-hot encoded categorical features\n            proportion_in_concept = X_i_feature.mean()\n            proportion_in_others = X_minus_i_feature.mean()\n            proportion_diff = proportion_in_concept - proportion_in_others\n            score = abs(proportion_diff) - lambda_reg\n            if score &gt; 0:\n                # Map back to original feature and category\n                if \"_\" in feature:\n                    original_feature = \"_\".join(feature.split(\"_\")[:-1])\n                    category = feature.split(\"_\")[-1]\n                    selected_features.setdefault(\n                        original_feature, {\"type\": \"categorical\", \"categories\": []}\n                    )\n                    selected_features[original_feature][\"categories\"].append(\n                        category\n                    )\n    return selected_features\n</code></pre>"},{"location":"api/explainable/concepts_causal/#_2","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":""},{"location":"api/explainable/interventions_causal/","title":"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0432\u043c\u0435\u0448\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432","text":"<p>\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>class InterventionCausalExplainer:\n    def __init__(self, n_estimators=10):\n        \"\"\"Initialize the ModelInterpreter.\n\n        Attributes:\n            n_estimators: Number of estimators for Data-IQ.\n        \"\"\"\n        logger.info(\n            \"Initializing InterventionCausalExplainer with %d estimators\", n_estimators\n        )\n        self.n_estimators = n_estimators\n        self.clf = None\n        self.dataiq_train = None\n        self.dataiq_test = None\n        self.confidence_train = None\n        self.confidence_test = None\n        self.aleatoric_uncertainty_train = None\n        self.aleatoric_uncertainty_test = None\n        self.feature_effects = None\n        self.confidence_test_before_intervention = None\n        self.aleatoric_uncertainty_test_before_intervention = None\n\n    def train_model(self, model: BaseEstimator | ClassifierMixin, X, y):\n        \"\"\"Train the model on the training data.\n\n        Args:\n            model: The model to train\n            X: Training data\n            y: Training labels\n        \"\"\"\n        logger.info(\"Training the model with %d samples\", X.shape[0])\n        self.clf = model\n        self.clf.fit(X, y)\n        logger.info(\"Model training complete.\")\n\n    def _compute_confidence_uncertainty(self, X, y, suffix: str):\n        \"\"\"Helper to compute confidence and uncertainty for train/test data using Data-IQ.\"\"\"\n        data_type = \"training\" if suffix == \"train\" else \"test\"\n        logger.info(\n            f\"Computing confidence and uncertainty on {data_type} data using Data-IQ.\"\n        )\n\n        dataiq = DataIQSKLearn(X=X, y=y)\n        dataiq.on_epoch_end(clf=self.clf, iteration=self.n_estimators)\n\n        setattr(self, f\"dataiq_{suffix}\", dataiq)\n        setattr(self, f\"confidence_{suffix}\", dataiq.confidence)\n        setattr(self, f\"aleatoric_uncertainty_{suffix}\", dataiq.aleatoric)\n\n    def compute_confidence_uncertainty_train(self, X, y):\n        \"\"\"Compute model confidence and aleatoric uncertainty on training data using Data-IQ.\"\"\"\n        self._compute_confidence_uncertainty(X, y, \"train\")\n\n    def compute_confidence_uncertainty_test(self, X, y):\n        \"\"\"Compute model confidence and aleatoric uncertainty on test data using Data-IQ.\"\"\"\n        self._compute_confidence_uncertainty(X, y, \"test\")\n\n    def estimate_feature_impact(self, X, random_state=42):\n        \"\"\"Estimate the causal effect of each feature on the model's confidence using training data.\"\"\"\n        logger.info(\n            \"Estimating feature impact using causal inference on training data.\"\n        )\n        self.feature_effects = {}\n        for feature in track(X.columns, description=\"Estimating feature impacts\"):\n            logger.debug(f\"Estimating effect of feature '{feature}'.\")\n            treatment = X[feature].values\n            outcome = self.confidence_train\n            covariates = X.drop(columns=[feature])\n\n            est = CausalForestDML(\n                model_y=RandomForestRegressor(),\n                model_t=RandomForestRegressor(),\n                discrete_treatment=False,\n                random_state=random_state,\n            )\n            est.fit(Y=outcome, T=treatment, X=covariates)\n            te = est.const_marginal_effect(covariates).mean()\n            self.feature_effects[feature] = te\n\n        # Convert to Series and sort\n        self.feature_effects = (\n            pd.Series(self.feature_effects).abs().sort_values(ascending=False)\n        )\n        logger.info(\"Feature effects estimated.\")\n\n    def plot_aleatoric_uncertainty(self, before_intervention: bool = True):\n        \"\"\"Plot aleatoric uncertainty for test data before and after intervention.\"\"\"\n        if before_intervention:\n            plt.figure(figsize=(10, 5))\n            plt.hist(\n                self.aleatoric_uncertainty_test_before_intervention,\n                bins=30,\n                alpha=0.5,\n                label=\"Uncertainty Before Intervention\",\n            )\n            plt.hist(\n                self.aleatoric_uncertainty_test,\n                bins=30,\n                alpha=0.5,\n                color=\"red\",\n                label=\"Uncertainty After Intervention\",\n            )\n            plt.title(\"Test Data: Aleatoric Uncertainty Before and After Intervention\")\n            plt.xlabel(\"Aleatoric Uncertainty\")\n            plt.ylabel(\"Frequency\")\n            plt.legend()\n            plt.show()\n\n    def plot_top_feature_effects(self, top_n: int = 10):\n        \"\"\"Plot a bin plot of the top N most impactful features with their causal effects.\n\n        Args:\n            top_n: Number of top features to plot.\n        \"\"\"\n        top_features = self.feature_effects.head(top_n)\n        plt.figure(figsize=(10, 8))\n        top_features.plot(kind=\"bar\", color=\"skyblue\")\n        plt.title(f\"Top {top_n} Most Impactful Features by Causal Effect\")\n        plt.xlabel(\"Features\")\n        plt.ylabel(\"Causal Effect\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.show()\n\n    def perform_intervention(self, X_test, y_test):\n        \"\"\"Perform an intervention on the top 5 most impactful features in the test data and observe changes.\"\"\"\n        if self.feature_effects is None:\n            raise ValueError(\"Feature effects have not been estimated yet.\")\n\n        top_features = self.feature_effects.head(5).index.tolist()\n        logger.info(f\"Top {len(top_features)} most impactful features: {top_features}\")\n\n        # Compute confidence on test data before intervention\n        self.compute_confidence_uncertainty_test(X=X_test, y=y_test)\n        self.confidence_test_before_intervention = self.confidence_test.copy()\n        self.aleatoric_uncertainty_test_before_intervention = (\n            self.aleatoric_uncertainty_test.copy()\n        )\n\n        original_feature_values_test = X_test[top_features].copy()\n\n        for feature in track(top_features, description=\"Performing interventions\"):\n            plt.figure(figsize=(10, 5))\n            plt.hist(\n                original_feature_values_test[feature],\n                bins=30,\n                alpha=0.5,\n                label=\"Before Intervention\",\n            )\n\n            logger.debug(f\"Performing intervention on '{feature}' in test data.\")\n            min_val = original_feature_values_test[feature].min()\n            max_val = original_feature_values_test[feature].max()\n            np.random.seed(42)\n            new_values = np.random.uniform(\n                low=min_val, high=max_val, size=X_test.shape[0]\n            )\n            X_test[feature] = new_values\n\n            plt.hist(\n                X_test[feature],\n                bins=30,\n                alpha=0.5,\n                color=\"orange\",\n                label=\"After Intervention\",\n            )\n            plt.title(\n                f\"Test Data: Distribution of '{feature}' Before and After Intervention\"\n            )\n            plt.xlabel(feature)\n            plt.ylabel(\"Frequency\")\n            plt.legend()\n            plt.show()\n\n        self.compute_confidence_uncertainty_test(X=X_test, y=y_test)\n\n        plt.figure(figsize=(10, 5))\n        plt.hist(\n            self.confidence_test_before_intervention,\n            bins=30,\n            alpha=0.5,\n            label=\"Confidence Before Intervention\",\n        )\n        plt.hist(\n            self.confidence_test,\n            bins=30,\n            alpha=0.5,\n            color=\"green\",\n            label=\"Confidence After Intervention\",\n        )\n        plt.title(\n            f\"Test Data: Model Confidence Before and After Intervention on {len(top_features)} features\"\n        )\n        plt.xlabel(\"Confidence\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.show()\n\n        self.plot_aleatoric_uncertainty()\n\n        logger.info(\n            \"Intervention complete. Observed changes in model confidence on test data.\"\n        )\n\n    def interpret(\n        self,\n        model,\n        X_train: pd.DataFrame,\n        y_train: pd.Series,\n        X_test: pd.DataFrame,\n        y_test: pd.Series,\n    ):\n        \"\"\"Run the full interpretation process.\"\"\"\n        self.train_model(model=model, X=X_train, y=y_train)\n        self.compute_confidence_uncertainty_train(X=X_train, y=y_train)\n        self.estimate_feature_impact(X=X_train)\n        self.plot_top_feature_effects()\n        self.perform_intervention(X_test=X_test, y_test=y_test)\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.__init__","title":"<code>__init__(n_estimators=10)</code>","text":"<p>Initialize the ModelInterpreter.</p> <p>Attributes:</p> Name Type Description <code>n_estimators</code> <p>Number of estimators for Data-IQ.</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def __init__(self, n_estimators=10):\n    \"\"\"Initialize the ModelInterpreter.\n\n    Attributes:\n        n_estimators: Number of estimators for Data-IQ.\n    \"\"\"\n    logger.info(\n        \"Initializing InterventionCausalExplainer with %d estimators\", n_estimators\n    )\n    self.n_estimators = n_estimators\n    self.clf = None\n    self.dataiq_train = None\n    self.dataiq_test = None\n    self.confidence_train = None\n    self.confidence_test = None\n    self.aleatoric_uncertainty_train = None\n    self.aleatoric_uncertainty_test = None\n    self.feature_effects = None\n    self.confidence_test_before_intervention = None\n    self.aleatoric_uncertainty_test_before_intervention = None\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.compute_confidence_uncertainty_test","title":"<code>compute_confidence_uncertainty_test(X, y)</code>","text":"<p>Compute model confidence and aleatoric uncertainty on test data using Data-IQ.</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def compute_confidence_uncertainty_test(self, X, y):\n    \"\"\"Compute model confidence and aleatoric uncertainty on test data using Data-IQ.\"\"\"\n    self._compute_confidence_uncertainty(X, y, \"test\")\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.compute_confidence_uncertainty_train","title":"<code>compute_confidence_uncertainty_train(X, y)</code>","text":"<p>Compute model confidence and aleatoric uncertainty on training data using Data-IQ.</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def compute_confidence_uncertainty_train(self, X, y):\n    \"\"\"Compute model confidence and aleatoric uncertainty on training data using Data-IQ.\"\"\"\n    self._compute_confidence_uncertainty(X, y, \"train\")\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.estimate_feature_impact","title":"<code>estimate_feature_impact(X, random_state=42)</code>","text":"<p>Estimate the causal effect of each feature on the model's confidence using training data.</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def estimate_feature_impact(self, X, random_state=42):\n    \"\"\"Estimate the causal effect of each feature on the model's confidence using training data.\"\"\"\n    logger.info(\n        \"Estimating feature impact using causal inference on training data.\"\n    )\n    self.feature_effects = {}\n    for feature in track(X.columns, description=\"Estimating feature impacts\"):\n        logger.debug(f\"Estimating effect of feature '{feature}'.\")\n        treatment = X[feature].values\n        outcome = self.confidence_train\n        covariates = X.drop(columns=[feature])\n\n        est = CausalForestDML(\n            model_y=RandomForestRegressor(),\n            model_t=RandomForestRegressor(),\n            discrete_treatment=False,\n            random_state=random_state,\n        )\n        est.fit(Y=outcome, T=treatment, X=covariates)\n        te = est.const_marginal_effect(covariates).mean()\n        self.feature_effects[feature] = te\n\n    # Convert to Series and sort\n    self.feature_effects = (\n        pd.Series(self.feature_effects).abs().sort_values(ascending=False)\n    )\n    logger.info(\"Feature effects estimated.\")\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.interpret","title":"<code>interpret(model, X_train, y_train, X_test, y_test)</code>","text":"<p>Run the full interpretation process.</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def interpret(\n    self,\n    model,\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_test: pd.DataFrame,\n    y_test: pd.Series,\n):\n    \"\"\"Run the full interpretation process.\"\"\"\n    self.train_model(model=model, X=X_train, y=y_train)\n    self.compute_confidence_uncertainty_train(X=X_train, y=y_train)\n    self.estimate_feature_impact(X=X_train)\n    self.plot_top_feature_effects()\n    self.perform_intervention(X_test=X_test, y_test=y_test)\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.perform_intervention","title":"<code>perform_intervention(X_test, y_test)</code>","text":"<p>Perform an intervention on the top 5 most impactful features in the test data and observe changes.</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def perform_intervention(self, X_test, y_test):\n    \"\"\"Perform an intervention on the top 5 most impactful features in the test data and observe changes.\"\"\"\n    if self.feature_effects is None:\n        raise ValueError(\"Feature effects have not been estimated yet.\")\n\n    top_features = self.feature_effects.head(5).index.tolist()\n    logger.info(f\"Top {len(top_features)} most impactful features: {top_features}\")\n\n    # Compute confidence on test data before intervention\n    self.compute_confidence_uncertainty_test(X=X_test, y=y_test)\n    self.confidence_test_before_intervention = self.confidence_test.copy()\n    self.aleatoric_uncertainty_test_before_intervention = (\n        self.aleatoric_uncertainty_test.copy()\n    )\n\n    original_feature_values_test = X_test[top_features].copy()\n\n    for feature in track(top_features, description=\"Performing interventions\"):\n        plt.figure(figsize=(10, 5))\n        plt.hist(\n            original_feature_values_test[feature],\n            bins=30,\n            alpha=0.5,\n            label=\"Before Intervention\",\n        )\n\n        logger.debug(f\"Performing intervention on '{feature}' in test data.\")\n        min_val = original_feature_values_test[feature].min()\n        max_val = original_feature_values_test[feature].max()\n        np.random.seed(42)\n        new_values = np.random.uniform(\n            low=min_val, high=max_val, size=X_test.shape[0]\n        )\n        X_test[feature] = new_values\n\n        plt.hist(\n            X_test[feature],\n            bins=30,\n            alpha=0.5,\n            color=\"orange\",\n            label=\"After Intervention\",\n        )\n        plt.title(\n            f\"Test Data: Distribution of '{feature}' Before and After Intervention\"\n        )\n        plt.xlabel(feature)\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.show()\n\n    self.compute_confidence_uncertainty_test(X=X_test, y=y_test)\n\n    plt.figure(figsize=(10, 5))\n    plt.hist(\n        self.confidence_test_before_intervention,\n        bins=30,\n        alpha=0.5,\n        label=\"Confidence Before Intervention\",\n    )\n    plt.hist(\n        self.confidence_test,\n        bins=30,\n        alpha=0.5,\n        color=\"green\",\n        label=\"Confidence After Intervention\",\n    )\n    plt.title(\n        f\"Test Data: Model Confidence Before and After Intervention on {len(top_features)} features\"\n    )\n    plt.xlabel(\"Confidence\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n    plt.show()\n\n    self.plot_aleatoric_uncertainty()\n\n    logger.info(\n        \"Intervention complete. Observed changes in model confidence on test data.\"\n    )\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.plot_aleatoric_uncertainty","title":"<code>plot_aleatoric_uncertainty(before_intervention=True)</code>","text":"<p>Plot aleatoric uncertainty for test data before and after intervention.</p> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def plot_aleatoric_uncertainty(self, before_intervention: bool = True):\n    \"\"\"Plot aleatoric uncertainty for test data before and after intervention.\"\"\"\n    if before_intervention:\n        plt.figure(figsize=(10, 5))\n        plt.hist(\n            self.aleatoric_uncertainty_test_before_intervention,\n            bins=30,\n            alpha=0.5,\n            label=\"Uncertainty Before Intervention\",\n        )\n        plt.hist(\n            self.aleatoric_uncertainty_test,\n            bins=30,\n            alpha=0.5,\n            color=\"red\",\n            label=\"Uncertainty After Intervention\",\n        )\n        plt.title(\"Test Data: Aleatoric Uncertainty Before and After Intervention\")\n        plt.xlabel(\"Aleatoric Uncertainty\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.show()\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.plot_top_feature_effects","title":"<code>plot_top_feature_effects(top_n=10)</code>","text":"<p>Plot a bin plot of the top N most impactful features with their causal effects.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>Number of top features to plot.</p> <code>10</code> Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def plot_top_feature_effects(self, top_n: int = 10):\n    \"\"\"Plot a bin plot of the top N most impactful features with their causal effects.\n\n    Args:\n        top_n: Number of top features to plot.\n    \"\"\"\n    top_features = self.feature_effects.head(top_n)\n    plt.figure(figsize=(10, 8))\n    top_features.plot(kind=\"bar\", color=\"skyblue\")\n    plt.title(f\"Top {top_n} Most Impactful Features by Causal Effect\")\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Causal Effect\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/explainable/interventions_causal/#applybn.explainable.causal_analysis.InterventionCausalExplainer.train_model","title":"<code>train_model(model, X, y)</code>","text":"<p>Train the model on the training data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator | ClassifierMixin</code> <p>The model to train</p> required <code>X</code> <p>Training data</p> required <code>y</code> <p>Training labels</p> required Source code in <code>applybn/explainable/causal_analysis/intervention_causal_effect.py</code> <pre><code>def train_model(self, model: BaseEstimator | ClassifierMixin, X, y):\n    \"\"\"Train the model on the training data.\n\n    Args:\n        model: The model to train\n        X: Training data\n        y: Training labels\n    \"\"\"\n    logger.info(\"Training the model with %d samples\", X.shape[0])\n    self.clf = model\n    self.clf.fit(X, y)\n    logger.info(\"Model training complete.\")\n</code></pre>"},{"location":"api/explainable/interventions_causal/#_2","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<pre><code>from applybn.explainable.causal_analysis import InterventionCausalExplainer\nfrom sklearn.ensemble import RandomForestClassifier\n\ninterpreter = InterventionCausalExplainer()\ninterpreter.interpret(RandomForestClassifier(), X_train, y_train, X_test, y_test)\n</code></pre>"},{"location":"api/feature_extraction/feature_extraction/","title":"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439","text":"<p>\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Generates features based on a Bayesian Network (BN).</p> Source code in <code>applybn/feature_extraction/bn_feature_extractor.py</code> <pre><code>class BNFeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Generates features based on a Bayesian Network (BN).\n    \"\"\"\n\n    def __init__(self):\n        self.bn = None\n\n    def fit(self, X: pd.DataFrame, y: pd.Series | None = None):\n        \"\"\"\n        Fits the BNFeatureGenerator to the data.\n\n        This involves:\n        1.  Adding the target variable (if provided) to the input data.\n        2.  Encoding categorical columns.\n        3.  Discretizing continuous columns.\n        4.  Creating a Bayesian Network based on the data types.\n        5.  Learning the structure of the Bayesian Network (if known_structure is not provided).\n        6.  Fitting the parameters of the Bayesian Network.\n\n        Args:\n            X: The input data.\n            y: The target variable. If provided, it will be added to the input data and\n                treated as a node in the Bayesian Network.\n\n        Returns:\n            self: The fitted BNFeatureGenerator object.\n        \"\"\"\n\n        encoder = preprocessing.LabelEncoder()\n        discretizer = preprocessing.KBinsDiscretizer(\n            n_bins=5, encode=\"ordinal\", strategy=\"kmeans\", subsample=None\n        )\n        preprocessor = pp.Preprocessor(\n            [(\"encoder\", encoder), (\"discretizer\", discretizer)]\n        )\n        target_name = None\n        if y is not None:\n            target_name = y.name\n            X = pd.concat([X, y], axis=1).reset_index(drop=True)\n        clean_data = X\n\n        discretized_data, est = preprocessor.apply(X)\n        discretized_data = pd.DataFrame(discretized_data, columns=X.columns)\n\n        # Initializing BNEstimator for BN selection &amp; fitting\n        info = preprocessor.info\n        params = {}\n        if target_name:\n            bl = self.create_black_list(X, target_name)  # Edges to avoid\n            params = {\"bl_add\": bl}\n        learning_params = {\"params\": params}\n        bn_estimator = BNEstimator(\n            use_mixture=False, has_logit=True, learning_params=learning_params\n        )\n        L = (discretized_data, info, clean_data)\n        self.bn = bn_estimator.fit(L)\n        self.bn = bn_estimator.bn_\n        logging.info(self.bn.get_info())\n\n        return self\n\n    def _process_target(self, target_name, X: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"\n        Processes the target variable by making predictions using the Bayesian Network.\n\n        Args:\n            X: The input data.\n\n        Returns:\n            Predictions for the target variable.\n        \"\"\"\n        if not target_name:\n            return None\n\n        predictions = self.bn.predict(test=X, parall_count=-1, progress_bar=False)\n        return pd.Series(predictions[target_name])\n\n    def transform(self, X: pd.DataFrame, fill_na: bool = True) -&gt; pd.DataFrame:\n        \"\"\"\n        Transforms the input DataFrame `X` into a new DataFrame where each column\n        represents the calculated feature based on the fitted BN.\n\n        Args:\n            X (pd.DataFrame) is the input DataFrame to transform.\n\n        Returns:\n            A new DataFrame with lambda-features.\n        \"\"\"\n        if not self.bn:\n            raise AttributeError(\"Parameter learning wasn't done. Call fit method\")\n\n        results = []\n        X_nodes = X.columns\n        target_name = {node.name for node in self.bn.nodes} - set(X_nodes)\n        if target_name:\n            target_name = target_name.pop()\n\n        # Process each feature (column) in the row (excluding target) using the BN\n        for _, row in X.iterrows():\n            row_probs = [\n                self.process_feature(feat, row, X, fill_na) for feat in X_nodes\n            ]\n            results.append(row_probs)\n\n        result = pd.DataFrame(results, columns=[\"lambda_\" + c for c in X_nodes])\n\n        # Process target\n        target_predictions = self._process_target(target_name, X)\n        if target_predictions is not None:\n            result[\"lambda_\" + target_name] = target_predictions\n\n        return pd.concat([X, result], axis=1)\n\n    def create_black_list(self, X: pd.DataFrame, y: Optional[str]):\n        \"\"\"\n        Creates a black list of edges to prevent the target variable from being a parent of any features.\n\n        Args:\n            X: The input data containing feature columns.\n            y: The name of the target variable. If provided, edges from the target to other features are blacklisted.\n\n        Returns:\n            A list of tuples representing edges to be excluded from the Bayesian Network structure.\n        \"\"\"\n        if not y:\n            return []\n        target_node = y\n        black_list = [\n            (target_node, (col)) for col in X.columns.to_list() if col != target_node\n        ]\n\n        return black_list\n\n    def process_feature(\n        self, feature: str, row: pd.Series, X: pd.DataFrame, fill_na: bool = True\n    ):\n        \"\"\"\n        Processes a single feature (node) in the Bayesian network for a given row of data.\n\n        Args:\n            feature: The name of the feature (node) being processed.\n            row: A row from X.\n            X: DataFrame that we transform.\n\n        Returns:\n            The probability or observed value depending on the node type.\n        \"\"\"\n\n        node = next((n for n in self.bn.nodes if n.name == feature), None)\n        pvals = {}\n        pvals_disc = []\n\n        # Iterate through the continuous parents\n        for p in node.cont_parents:\n            pvals[p] = row[p]\n\n        # Iterate through the discrete parents\n        for p in node.disc_parents:\n            norm_val = str(row[p])\n            pvals[p] = norm_val\n            pvals_disc.append(norm_val)\n        # Process discrete nodes\n        if node.type == \"Discrete\" or \"logit\" in str(node.type).lower():\n            vals = X[node.name].value_counts(normalize=True).sort_index()\n            vals = [str(i) for i in vals.index.to_list()]\n            return self._process_discrete_node(feature, row, pvals, vals, fill_na)\n        # Process non-discrete nodes\n        else:\n            vals = X[node.name].value_counts(normalize=True).sort_index()\n            vals = [(i) for i in vals.index.to_list()]\n            return self._process_non_discrete_node(feature, row, pvals, vals, fill_na)\n\n    def _process_discrete_node(self, feature, row, pvals, vals, fill_na):\n        \"\"\"\n        Processes a discrete node.\n\n        Args:\n            node - the discrete node object.\n            feature (str): the name of the feature (node).\n            row (pd.Series): a row of data from the DataFrame.\n            pvals (dict): list of parent values.\n            vals: possible values of the 'feature'.\n\n        Returns:\n            float: value of a new feature.\n        \"\"\"\n\n        obs_value = str(row[feature])\n        if fill_na:\n            imputed_value = (\n                pd.Series(vals).value_counts(normalize=True).get(obs_value, 0.0)\n            )\n        else:\n            imputed_value = np.nan\n        try:\n            dist = self.bn.get_dist(feature, pvals=pvals).get()\n            idx = dist[1].index(obs_value)\n            return dist[0][idx]\n        except:\n            logging.exception(\n                \"Distribution not found for node %s and value %s; setting to %s\",\n                feature,\n                str(obs_value),\n                str(imputed_value),\n            )\n            return imputed_value\n\n    def _process_non_discrete_node(self, feature, row, pvals, vals, fill_na):\n        \"\"\"\n        Processes a non-discrete node.\n\n        Args:\n            feature (str): the name of the feature (node).\n            row (pd.Series): a row of data from the DataFrame.\n            pvals (dict): list of parent values.\n            vals: possible values of the 'feature'.\n\n        Returns:\n            float: value of a new feature.\n        \"\"\"\n        obs_value = row[feature]\n        if fill_na:\n            imputed_value = (\n                (pd.Series(vals) &lt;= obs_value).mean()\n                if isinstance(vals, list)\n                else (vals &lt;= obs_value).mean()\n            )\n        else:\n            imputed_value = np.nan\n        try:\n            dist = self.bn.get_dist(feature, pvals=pvals).get()\n            mean, variance = dist\n            if np.isnan(mean) or np.isnan(variance):\n                return imputed_value\n            sigma = variance\n            prob = norm.cdf(obs_value, loc=mean, scale=sigma)\n            if np.isnan(prob):  # if std is 0\n                return imputed_value\n            return prob\n        except:\n            logging.exception(\n                \"Distribution not found for node %s and value %s; setting to %s\",\n                feature,\n                str(obs_value),\n                str(imputed_value),\n            )\n            return imputed_value\n</code></pre>"},{"location":"api/feature_extraction/feature_extraction/#applybn.feature_extraction.bn_feature_extractor.BNFeatureGenerator.create_black_list","title":"<code>create_black_list(X, y)</code>","text":"<p>Creates a black list of edges to prevent the target variable from being a parent of any features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data containing feature columns.</p> required <code>y</code> <code>Optional[str]</code> <p>The name of the target variable. If provided, edges from the target to other features are blacklisted.</p> required <p>Returns:</p> Type Description <p>A list of tuples representing edges to be excluded from the Bayesian Network structure.</p> Source code in <code>applybn/feature_extraction/bn_feature_extractor.py</code> <pre><code>def create_black_list(self, X: pd.DataFrame, y: Optional[str]):\n    \"\"\"\n    Creates a black list of edges to prevent the target variable from being a parent of any features.\n\n    Args:\n        X: The input data containing feature columns.\n        y: The name of the target variable. If provided, edges from the target to other features are blacklisted.\n\n    Returns:\n        A list of tuples representing edges to be excluded from the Bayesian Network structure.\n    \"\"\"\n    if not y:\n        return []\n    target_node = y\n    black_list = [\n        (target_node, (col)) for col in X.columns.to_list() if col != target_node\n    ]\n\n    return black_list\n</code></pre>"},{"location":"api/feature_extraction/feature_extraction/#applybn.feature_extraction.bn_feature_extractor.BNFeatureGenerator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the BNFeatureGenerator to the data.</p> <p>This involves: 1.  Adding the target variable (if provided) to the input data. 2.  Encoding categorical columns. 3.  Discretizing continuous columns. 4.  Creating a Bayesian Network based on the data types. 5.  Learning the structure of the Bayesian Network (if known_structure is not provided). 6.  Fitting the parameters of the Bayesian Network.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input data.</p> required <code>y</code> <code>Series | None</code> <p>The target variable. If provided, it will be added to the input data and treated as a node in the Bayesian Network.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <p>The fitted BNFeatureGenerator object.</p> Source code in <code>applybn/feature_extraction/bn_feature_extractor.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: pd.Series | None = None):\n    \"\"\"\n    Fits the BNFeatureGenerator to the data.\n\n    This involves:\n    1.  Adding the target variable (if provided) to the input data.\n    2.  Encoding categorical columns.\n    3.  Discretizing continuous columns.\n    4.  Creating a Bayesian Network based on the data types.\n    5.  Learning the structure of the Bayesian Network (if known_structure is not provided).\n    6.  Fitting the parameters of the Bayesian Network.\n\n    Args:\n        X: The input data.\n        y: The target variable. If provided, it will be added to the input data and\n            treated as a node in the Bayesian Network.\n\n    Returns:\n        self: The fitted BNFeatureGenerator object.\n    \"\"\"\n\n    encoder = preprocessing.LabelEncoder()\n    discretizer = preprocessing.KBinsDiscretizer(\n        n_bins=5, encode=\"ordinal\", strategy=\"kmeans\", subsample=None\n    )\n    preprocessor = pp.Preprocessor(\n        [(\"encoder\", encoder), (\"discretizer\", discretizer)]\n    )\n    target_name = None\n    if y is not None:\n        target_name = y.name\n        X = pd.concat([X, y], axis=1).reset_index(drop=True)\n    clean_data = X\n\n    discretized_data, est = preprocessor.apply(X)\n    discretized_data = pd.DataFrame(discretized_data, columns=X.columns)\n\n    # Initializing BNEstimator for BN selection &amp; fitting\n    info = preprocessor.info\n    params = {}\n    if target_name:\n        bl = self.create_black_list(X, target_name)  # Edges to avoid\n        params = {\"bl_add\": bl}\n    learning_params = {\"params\": params}\n    bn_estimator = BNEstimator(\n        use_mixture=False, has_logit=True, learning_params=learning_params\n    )\n    L = (discretized_data, info, clean_data)\n    self.bn = bn_estimator.fit(L)\n    self.bn = bn_estimator.bn_\n    logging.info(self.bn.get_info())\n\n    return self\n</code></pre>"},{"location":"api/feature_extraction/feature_extraction/#applybn.feature_extraction.bn_feature_extractor.BNFeatureGenerator.process_feature","title":"<code>process_feature(feature, row, X, fill_na=True)</code>","text":"<p>Processes a single feature (node) in the Bayesian network for a given row of data.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>str</code> <p>The name of the feature (node) being processed.</p> required <code>row</code> <code>Series</code> <p>A row from X.</p> required <code>X</code> <code>DataFrame</code> <p>DataFrame that we transform.</p> required <p>Returns:</p> Type Description <p>The probability or observed value depending on the node type.</p> Source code in <code>applybn/feature_extraction/bn_feature_extractor.py</code> <pre><code>def process_feature(\n    self, feature: str, row: pd.Series, X: pd.DataFrame, fill_na: bool = True\n):\n    \"\"\"\n    Processes a single feature (node) in the Bayesian network for a given row of data.\n\n    Args:\n        feature: The name of the feature (node) being processed.\n        row: A row from X.\n        X: DataFrame that we transform.\n\n    Returns:\n        The probability or observed value depending on the node type.\n    \"\"\"\n\n    node = next((n for n in self.bn.nodes if n.name == feature), None)\n    pvals = {}\n    pvals_disc = []\n\n    # Iterate through the continuous parents\n    for p in node.cont_parents:\n        pvals[p] = row[p]\n\n    # Iterate through the discrete parents\n    for p in node.disc_parents:\n        norm_val = str(row[p])\n        pvals[p] = norm_val\n        pvals_disc.append(norm_val)\n    # Process discrete nodes\n    if node.type == \"Discrete\" or \"logit\" in str(node.type).lower():\n        vals = X[node.name].value_counts(normalize=True).sort_index()\n        vals = [str(i) for i in vals.index.to_list()]\n        return self._process_discrete_node(feature, row, pvals, vals, fill_na)\n    # Process non-discrete nodes\n    else:\n        vals = X[node.name].value_counts(normalize=True).sort_index()\n        vals = [(i) for i in vals.index.to_list()]\n        return self._process_non_discrete_node(feature, row, pvals, vals, fill_na)\n</code></pre>"},{"location":"api/feature_extraction/feature_extraction/#applybn.feature_extraction.bn_feature_extractor.BNFeatureGenerator.transform","title":"<code>transform(X, fill_na=True)</code>","text":"<p>Transforms the input DataFrame <code>X</code> into a new DataFrame where each column represents the calculated feature based on the fitted BN.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with lambda-features.</p> Source code in <code>applybn/feature_extraction/bn_feature_extractor.py</code> <pre><code>def transform(self, X: pd.DataFrame, fill_na: bool = True) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the input DataFrame `X` into a new DataFrame where each column\n    represents the calculated feature based on the fitted BN.\n\n    Args:\n        X (pd.DataFrame) is the input DataFrame to transform.\n\n    Returns:\n        A new DataFrame with lambda-features.\n    \"\"\"\n    if not self.bn:\n        raise AttributeError(\"Parameter learning wasn't done. Call fit method\")\n\n    results = []\n    X_nodes = X.columns\n    target_name = {node.name for node in self.bn.nodes} - set(X_nodes)\n    if target_name:\n        target_name = target_name.pop()\n\n    # Process each feature (column) in the row (excluding target) using the BN\n    for _, row in X.iterrows():\n        row_probs = [\n            self.process_feature(feat, row, X, fill_na) for feat in X_nodes\n        ]\n        results.append(row_probs)\n\n    result = pd.DataFrame(results, columns=[\"lambda_\" + c for c in X_nodes])\n\n    # Process target\n    target_predictions = self._process_target(target_name, X)\n    if target_predictions is not None:\n        result[\"lambda_\" + target_name] = target_predictions\n\n    return pd.concat([X, result], axis=1)\n</code></pre>"},{"location":"api/feature_extraction/feature_extraction/#_2","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<pre><code>from applybn.feature_extraction.bn_feature_extractor import BNFeatureGenerator\nimport pandas as pd\n\ndata = pd.read_csv('your_data.csv')\n\ngenerator = BNFeatureGenerator()\ngenerator.fit(data)\n\ndata_with_features = generator.transform(data)\n</code></pre>"},{"location":"api/feature_selection/causal_feature_selection/","title":"CausalFeatureSelector: \u043e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430","text":"<p>\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> <p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Causal Feature Selector based on information-theoretic causal influence.</p> <p>This class implements feature selection by evaluating the causal effect of features on the target variable using entropy-based metrics. Features are selected if they significantly reduce the uncertainty (conditional entropy) of the target when controlled for other selected features.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>Union[int, str]</code> <p>Number of bins for data discretization. Use \"auto\" to determine bins via IQR-based heuristic. Defaults to \"auto\".</p> <code>'auto'</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>Union[int, str]</code> <p>Number of bins used for discretization (set during initialization).</p> <code>support_</code> <code>ndarray</code> <p>Boolean mask array indicating selected features. Shape (n_features,).</p> <code>X_</code> <code>ndarray</code> <p>Feature matrix from the training data. Shape (n_samples, n_features).</p> <code>y_</code> <code>ndarray</code> <p>Target variable from the training data. Shape (n_samples,).</p> Source code in <code>applybn/feature_selection/ce_feature_selector.py</code> <pre><code>class CausalFeatureSelector(BaseEstimator, SelectorMixin):\n    \"\"\"Causal Feature Selector based on information-theoretic causal influence.\n\n    This class implements feature selection by evaluating the causal effect of features\n    on the target variable using entropy-based metrics. Features are selected if they\n    significantly reduce the uncertainty (conditional entropy) of the target when\n    controlled for other selected features.\n\n    Parameters:\n        n_bins (Union[int, str]): Number of bins for data discretization.\n            Use \"auto\" to determine bins via IQR-based heuristic. Defaults to \"auto\".\n\n    Attributes:\n        n_bins (Union[int, str]): Number of bins used for discretization (set during initialization).\n        support_ (np.ndarray): Boolean mask array indicating selected features. Shape (n_features,).\n        X_ (np.ndarray): Feature matrix from the training data. Shape (n_samples, n_features).\n        y_ (np.ndarray): Target variable from the training data. Shape (n_samples,).\n    \"\"\"\n\n    def __init__(self, n_bins: Union[int, str] = \"auto\"):\n        \"\"\"\n        Initialize the causal feature selector.\n\n        Args:\n            n_bins: Number of bins for discretization or 'auto' for automatic selection.\n        \"\"\"\n        self.n_bins = n_bins\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"CausalFeatureSelector\":\n        \"\"\"\n        Fit the causal feature selector to the data.\n\n        Args:\n            X: Feature matrix (2D array).\n            y: Target variable (1D array).\n\n        Returns:\n            self\n        \"\"\"\n        # Validate inputs\n        X, y = check_X_y(X, y)\n        self.X_ = X\n        self.y_ = y\n\n        # Compute the support mask for selected features\n        self.support_ = self._select_features(X, y)\n        return self\n\n    def _select_features(self, data: np.ndarray, target: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Core logic for selecting features based on causal influence.\n\n        Args:\n            data: Feature matrix.\n            target: Target variable.\n\n        Returns:\n            Boolean mask of selected features.\n        \"\"\"\n        # Discretize target and features\n        target_discretized = self._discretize_data_iqr(target)[0]\n        data_discretized = np.array(\n            [self._discretize_data_iqr(data[:, i])[0] for i in range(data.shape[1])]\n        ).T\n\n        selected_mask = np.zeros(data.shape[1], dtype=bool)\n        other_features = np.array([])\n\n        for i in range(data.shape[1]):\n            feature = data[:, i]\n            # Compute causal effect of the current feature\n            ce = self._causal_effect(feature, target, other_features)\n\n            if ce &gt; 0:  # If causal effect is significant\n                selected_mask[i] = True\n                if other_features.size &gt; 0:\n                    other_features = np.c_[other_features, feature]\n                else:\n                    other_features = feature.reshape(-1, 1)\n\n        return selected_mask\n\n    def _get_support_mask(self) -&gt; np.ndarray:\n        \"\"\"\n        Required by SelectorMixin to return the mask of selected features.\n\n        Returns:\n            Boolean mask of selected features.\n        \"\"\"\n        return self.support_\n\n    def _discretize_data_iqr(self, data: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Discretize data using the interquartile range (IQR) rule.\n\n        Args:\n            data: Data to discretize.\n\n        Returns:\n            Discretized data and bin edges.\n        \"\"\"\n        # Compute range and IQR\n        R = np.ptp(data)\n        iqr = np.subtract(*np.percentile(data, [75, 25]))\n        iqr = max(iqr, 1e-8)  # Avoid division by zero\n\n        n = len(data)\n        # Determine the number of bins\n        n_bins = (\n            self.n_bins\n            if self.n_bins != \"auto\"\n            else max(2, int(np.ceil((R / (2 * iqr * n ** (1 / 3))) * np.log2(n + 1))))\n        )\n\n        bins = np.linspace(np.min(data), np.max(data), n_bins + 1)\n        discretized_data = np.digitize(data, bins) - 1\n        discretized_data = np.clip(discretized_data, 0, len(bins) - 2)\n\n        return discretized_data, bins\n\n    def _causal_effect(\n        self, Xi: np.ndarray, Y: np.ndarray, other_features: np.ndarray\n    ) -&gt; float:\n        \"\"\"\n        Compute the causal effect of Xi on Y, controlling for other features.\n\n        Args:\n            Xi: Feature for which the causal effect is calculated.\n            Y: Target variable.\n            other_features: Matrix of other features to control for.\n\n        Returns:\n            Causal effect of Xi on Y.\n        \"\"\"\n        Y_discretized = self._discretize_data_iqr(Y)[0]\n        Xi_discretized = self._discretize_data_iqr(Xi)[0]\n\n        if other_features.size &gt; 0:\n            # Discretize other features if they exist\n            other_features_discretized = np.array(\n                [\n                    self._discretize_data_iqr(other_features[:, i])[0]\n                    for i in range(other_features.shape[1])\n                ]\n            ).T\n            combined_features = np.c_[other_features_discretized, Xi_discretized]\n            H_Y_given_other = self._conditional_entropy(\n                other_features_discretized, Y_discretized\n            )\n            H_Y_given_Xi_other = self._conditional_entropy(\n                combined_features, Y_discretized\n            )\n            return H_Y_given_other - H_Y_given_Xi_other\n        else:\n            return self._entropy(Y_discretized) - self._conditional_entropy(\n                Xi_discretized, Y_discretized\n            )\n\n    def _entropy(self, discretized_data: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute the entropy of discretized data.\n\n        Args:\n            discretized_data: Discretized data.\n\n        Returns:\n            Entropy of the data.\n        \"\"\"\n        value_counts = np.unique(discretized_data, return_counts=True)[1]\n        probabilities = value_counts / len(discretized_data)\n        return -np.sum(\n            probabilities[probabilities &gt; 0] * np.log2(probabilities[probabilities &gt; 0])\n        )\n\n    def _conditional_entropy(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute the conditional entropy H(Y|X).\n\n        Args:\n            X: Discretized features.\n            Y: Discretized target variable.\n\n        Returns:\n            Conditional entropy H(Y|X).\n        \"\"\"\n        unique_x = np.unique(X, axis=0)\n        cond_entropy = 0\n\n        for x in unique_x:\n            # Mask for rows where X equals the current unique value\n            if X.ndim == 1:\n                mask = X == x  # Simple comparison for 1D array\n            else:\n                mask = np.all(X == x, axis=1)\n            subset_entropy = self._entropy(Y[mask])\n            cond_entropy += np.sum(mask) / len(X) * subset_entropy\n\n        return cond_entropy\n</code></pre>"},{"location":"api/feature_selection/causal_feature_selection/#applybn.feature_selection.ce_feature_selector.CausalFeatureSelector.__init__","title":"<code>__init__(n_bins='auto')</code>","text":"<p>Initialize the causal feature selector.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>Union[int, str]</code> <p>Number of bins for discretization or 'auto' for automatic selection.</p> <code>'auto'</code> Source code in <code>applybn/feature_selection/ce_feature_selector.py</code> <pre><code>def __init__(self, n_bins: Union[int, str] = \"auto\"):\n    \"\"\"\n    Initialize the causal feature selector.\n\n    Args:\n        n_bins: Number of bins for discretization or 'auto' for automatic selection.\n    \"\"\"\n    self.n_bins = n_bins\n</code></pre>"},{"location":"api/feature_selection/causal_feature_selection/#applybn.feature_selection.ce_feature_selector.CausalFeatureSelector.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the causal feature selector to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix (2D array).</p> required <code>y</code> <code>ndarray</code> <p>Target variable (1D array).</p> required <p>Returns:</p> Type Description <code>CausalFeatureSelector</code> <p>self</p> Source code in <code>applybn/feature_selection/ce_feature_selector.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"CausalFeatureSelector\":\n    \"\"\"\n    Fit the causal feature selector to the data.\n\n    Args:\n        X: Feature matrix (2D array).\n        y: Target variable (1D array).\n\n    Returns:\n        self\n    \"\"\"\n    # Validate inputs\n    X, y = check_X_y(X, y)\n    self.X_ = X\n    self.y_ = y\n\n    # Compute the support mask for selected features\n    self.support_ = self._select_features(X, y)\n    return self\n</code></pre>"},{"location":"api/feature_selection/causal_feature_selection/#_1","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<pre><code>from applybn.feature_selection.ce_feature_selector import CausalFeatureSelector\n\ncausal_selector = CausalFeatureSelector(n_bins=10)\ncausal_selector.fit(X_train, y_train)\nX_train_selected = causal_selector.transform(X_train)\n</code></pre>"},{"location":"api/feature_selection/nmi_feature_selection/","title":"NMIFeatureSelector: \u043e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0432\u0437\u0430\u0438\u043c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438","text":"<p>\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> <p>               Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Feature selection based on Normalized Mutual Information (NMI).</p> <p>This selector performs a two-stage feature selection process:</p> <ol> <li> <p>Select features with NMI to the target above a threshold.</p> </li> <li> <p>Remove redundant features based on pairwise NMI between features.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold value for the first stage selection. Features with NMI greater than this value are retained after the first stage.</p> <code>0.0</code> <code>n_bins</code> <code>int</code> <p>The number of bins to use for discretizing continuous features.</p> <code>10</code> <p>Attributes:</p> Name Type Description <code>nmi_features_target_</code> <code>ndarray</code> <p>The NMI between each feature and the target.</p> <code>selected_features_</code> <code>ndarray</code> <p>The indices of the selected features after both stages.</p> <code>selected_mask_</code> <code>ndarray</code> <p>Boolean mask indicating selected features.</p> <code>feature_names_in_</code> <code>ndarray</code> <p>Names of features seen during fit.</p> Source code in <code>applybn/feature_selection/bn_nmi_feature_selector.py</code> <pre><code>class NMIFeatureSelector(BaseEstimator, SelectorMixin):\n    \"\"\"Feature selection based on Normalized Mutual Information (NMI).\n\n    This selector performs a two-stage feature selection process:\n\n    1. Select features with NMI to the target above a threshold.\n\n    2. Remove redundant features based on pairwise NMI between features.\n\n\n    Args:\n        threshold (float): The threshold value for the first stage selection. Features with NMI\n            greater than this value are retained after the first stage.\n        n_bins (int): The number of bins to use for discretizing continuous features.\n\n    Attributes:\n        nmi_features_target_ (ndarray): The NMI between each feature and the target.\n        selected_features_ (ndarray): The indices of the selected features after both stages.\n        selected_mask_ (ndarray): Boolean mask indicating selected features.\n        feature_names_in_ (ndarray): Names of features seen during fit.\n    \"\"\"\n\n    nmi_features_target_: np.ndarray\n    selected_features_: np.ndarray\n    selected_mask_: np.ndarray\n    feature_names_in_: np.ndarray\n\n    def __init__(self, threshold: float = 0.0, n_bins: int = 10) -&gt; None:\n        \"\"\"Initialize the NMIFeatureSelector.\"\"\"\n        self.threshold = threshold\n        self.n_bins = n_bins\n\n    def fit(\n        self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]\n    ) -&gt; \"NMIFeatureSelector\":\n        \"\"\"Fit the feature selector to the data.\n\n        Args:\n            X (array-like of shape (n_samples, n_features)): The training input samples.\n            y (array-like of shape (n_samples,)): The target values.\n\n        Returns:\n            self (NMIFeatureSelector): The fitted feature selector instance.\n        \"\"\"\n\n        # Capture feature names BEFORE data conversion\n        if hasattr(X, \"columns\"):\n            self.feature_names_in_ = X.columns.to_numpy()\n        else:\n            self.feature_names_in_ = np.arange(X.shape[1])\n\n        # Convert to numpy arrays after capturing names\n        X, y = check_X_y(X, y, dtype=None, force_all_finite=True)\n\n        # Discretize continuous features and target\n        X_disc = self._discretize_features(X)\n        y_disc = self._discretize_target(y.reshape(-1, 1)).flatten()\n\n        # Compute NMI between each feature and target\n        mi = np.array(\n            [information_mutual(X_disc[:, i], y_disc) for i in range(X_disc.shape[1])]\n        )\n        h_y = entropy(y_disc)\n        h_features = np.array([entropy(X_disc[:, i]) for i in range(X_disc.shape[1])])\n        self.nmi_features_target_ = np.zeros_like(mi)\n        for i in range(len(mi)):\n            min_h = min(h_features[i], h_y)\n            self.nmi_features_target_[i] = mi[i] / min_h if min_h &gt; 0 else 0.0\n\n        # First stage: select features above threshold\n        first_stage_mask = self.nmi_features_target_ &gt; self.threshold\n        selected_indices = np.where(first_stage_mask)[0]\n\n        # Second stage: remove redundant features\n        keep = np.ones(len(selected_indices), dtype=bool)\n        nmi_selected = self.nmi_features_target_[selected_indices]\n\n        for j in range(len(selected_indices)):\n            fj_idx = selected_indices[j]\n            fj = X_disc[:, fj_idx]\n            nmi_j = nmi_selected[j]\n            h_fj = entropy(fj)\n\n            for i in range(len(selected_indices)):\n                if i == j or not keep[i]:\n                    continue\n                fi_idx = selected_indices[i]\n                fi = X_disc[:, fi_idx]\n                nmi_i = nmi_selected[i]\n\n                if nmi_i &gt; nmi_j:\n                    mi_pair = information_mutual(fi, fj)\n                    h_fi = entropy(fi)\n                    min_h_pair = min(h_fi, h_fj)\n                    nmi_pair = mi_pair / min_h_pair if min_h_pair &gt; 0 else 0.0\n\n                    if nmi_pair &gt; nmi_j:\n                        keep[j] = False\n                        break  # No need to check other i's\n\n        self.selected_features_ = selected_indices[keep]\n        self.selected_mask_ = np.zeros(X.shape[1], dtype=bool)\n        self.selected_mask_[self.selected_features_] = True\n\n        return self\n\n    def _get_support_mask(self) -&gt; np.ndarray:\n        \"\"\"Get the boolean mask indicating which features are selected.\n\n        Returns:\n            ndarray: Boolean array indicating selected features.\n        \"\"\"\n        return self.selected_mask_\n\n    def _discretize_features(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Discretize continuous features using KBinsDiscretizer.\n\n        Args:\n            X (ndarray): Input features.\n\n        Returns:\n            ndarray: Discretized features.\n        \"\"\"\n        X_disc = np.empty_like(X, dtype=np.int32)\n        for i in range(X.shape[1]):\n            col = X[:, i]\n            unique_vals = np.unique(col)\n            if len(unique_vals) &gt; self.n_bins:\n                # Discretize continuous feature\n                discretizer = KBinsDiscretizer(\n                    n_bins=self.n_bins, encode=\"ordinal\", strategy=\"uniform\"\n                )\n                discretized_col = (\n                    discretizer.fit_transform(col.reshape(-1, 1))\n                    .flatten()\n                    .astype(np.int32)\n                )\n                X_disc[:, i] = discretized_col\n            else:\n                # Treat as discrete (convert to integers for pyitlib)\n                X_disc[:, i] = col.astype(np.int32)\n        return X_disc\n\n    def _discretize_target(self, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Discretize target variable if continuous.\n\n        Args:\n            y (ndarray): Target variable.\n\n        Returns:\n            ndarray: Discretized target variable.\n        \"\"\"\n        unique_vals = np.unique(y)\n        if len(unique_vals) &gt; self.n_bins:\n            discretizer = KBinsDiscretizer(\n                n_bins=self.n_bins, encode=\"ordinal\", strategy=\"uniform\"\n            )\n            y_disc = discretizer.fit_transform(y).flatten().astype(np.int32)\n        else:\n            y_disc = y.astype(np.int32).flatten()\n        return y_disc\n</code></pre>"},{"location":"api/feature_selection/nmi_feature_selection/#applybn.feature_selection.bn_nmi_feature_selector.NMIFeatureSelector.__init__","title":"<code>__init__(threshold=0.0, n_bins=10)</code>","text":"<p>Initialize the NMIFeatureSelector.</p> Source code in <code>applybn/feature_selection/bn_nmi_feature_selector.py</code> <pre><code>def __init__(self, threshold: float = 0.0, n_bins: int = 10) -&gt; None:\n    \"\"\"Initialize the NMIFeatureSelector.\"\"\"\n    self.threshold = threshold\n    self.n_bins = n_bins\n</code></pre>"},{"location":"api/feature_selection/nmi_feature_selection/#applybn.feature_selection.bn_nmi_feature_selector.NMIFeatureSelector.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the feature selector to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features</code> <p>The training input samples.</p> required <code>y</code> <code>array-like of shape (n_samples,</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>NMIFeatureSelector</code> <p>The fitted feature selector instance.</p> Source code in <code>applybn/feature_selection/bn_nmi_feature_selector.py</code> <pre><code>def fit(\n    self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]\n) -&gt; \"NMIFeatureSelector\":\n    \"\"\"Fit the feature selector to the data.\n\n    Args:\n        X (array-like of shape (n_samples, n_features)): The training input samples.\n        y (array-like of shape (n_samples,)): The target values.\n\n    Returns:\n        self (NMIFeatureSelector): The fitted feature selector instance.\n    \"\"\"\n\n    # Capture feature names BEFORE data conversion\n    if hasattr(X, \"columns\"):\n        self.feature_names_in_ = X.columns.to_numpy()\n    else:\n        self.feature_names_in_ = np.arange(X.shape[1])\n\n    # Convert to numpy arrays after capturing names\n    X, y = check_X_y(X, y, dtype=None, force_all_finite=True)\n\n    # Discretize continuous features and target\n    X_disc = self._discretize_features(X)\n    y_disc = self._discretize_target(y.reshape(-1, 1)).flatten()\n\n    # Compute NMI between each feature and target\n    mi = np.array(\n        [information_mutual(X_disc[:, i], y_disc) for i in range(X_disc.shape[1])]\n    )\n    h_y = entropy(y_disc)\n    h_features = np.array([entropy(X_disc[:, i]) for i in range(X_disc.shape[1])])\n    self.nmi_features_target_ = np.zeros_like(mi)\n    for i in range(len(mi)):\n        min_h = min(h_features[i], h_y)\n        self.nmi_features_target_[i] = mi[i] / min_h if min_h &gt; 0 else 0.0\n\n    # First stage: select features above threshold\n    first_stage_mask = self.nmi_features_target_ &gt; self.threshold\n    selected_indices = np.where(first_stage_mask)[0]\n\n    # Second stage: remove redundant features\n    keep = np.ones(len(selected_indices), dtype=bool)\n    nmi_selected = self.nmi_features_target_[selected_indices]\n\n    for j in range(len(selected_indices)):\n        fj_idx = selected_indices[j]\n        fj = X_disc[:, fj_idx]\n        nmi_j = nmi_selected[j]\n        h_fj = entropy(fj)\n\n        for i in range(len(selected_indices)):\n            if i == j or not keep[i]:\n                continue\n            fi_idx = selected_indices[i]\n            fi = X_disc[:, fi_idx]\n            nmi_i = nmi_selected[i]\n\n            if nmi_i &gt; nmi_j:\n                mi_pair = information_mutual(fi, fj)\n                h_fi = entropy(fi)\n                min_h_pair = min(h_fi, h_fj)\n                nmi_pair = mi_pair / min_h_pair if min_h_pair &gt; 0 else 0.0\n\n                if nmi_pair &gt; nmi_j:\n                    keep[j] = False\n                    break  # No need to check other i's\n\n    self.selected_features_ = selected_indices[keep]\n    self.selected_mask_ = np.zeros(X.shape[1], dtype=bool)\n    self.selected_mask_[self.selected_features_] = True\n\n    return self\n</code></pre>"},{"location":"api/feature_selection/nmi_feature_selection/#_1","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<pre><code>from applybn.feature_selection.bn_nmi_feature_selector import NMIFeatureSelector\n\nselector = NMIFeatureSelector(threshold=0.5, n_bins=20)\nselector.fit(X_train, y_train)\nX_train_selected = selector.transform(X_train)\n</code></pre>"},{"location":"api/oversampling/bn_oversampling/","title":"\u0420\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439","text":"<p>\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> <p>               Bases: <code>BaseOverSampler</code></p> <p>A Bayesian Network-based oversampler for handling imbalanced datasets.</p> <p>This class uses Bayesian Networks to learn the joint probability distribution of features and generates synthetic samples for minority classes to balance class distribution. Inherits from BaseOverSampler to ensure compatibility with scikit-learn pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>class_column</code> <p>Name of the target class column. If None, will attempt to infer from y's name attribute.</p> <code>None</code> <code>strategy</code> <p>Oversampling strategy. Either 'max_class' to match the largest class size or an integer specifying target sample count per class.</p> <code>'max_class'</code> <code>shuffle</code> <p>Whether to shuffle the dataset after resampling.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data_generator_</code> <p>Fitted Bayesian Network synthetic data generator instance.</p> Source code in <code>applybn/imbalanced/over_sampling/bn_over_sampler.py</code> <pre><code>class BNOverSampler(BaseOverSampler):\n    \"\"\"A Bayesian Network-based oversampler for handling imbalanced datasets.\n\n    This class uses Bayesian Networks to learn the joint probability distribution of features\n    and generates synthetic samples for minority classes to balance class distribution.\n    Inherits from BaseOverSampler to ensure compatibility with scikit-learn pipelines.\n\n    Args:\n        class_column: Name of the target class column. If None, will attempt to infer from y's\n            name attribute.\n        strategy: Oversampling strategy. Either 'max_class' to match the largest class size or\n            an integer specifying target sample count per class.\n        shuffle: Whether to shuffle the dataset after resampling.\n\n    Attributes:\n        data_generator_: Fitted Bayesian Network synthetic data generator instance.\n    \"\"\"\n\n    def __init__(self, class_column=None, strategy=\"max_class\", shuffle=True):\n        \"\"\"Initialize the BNOverSampler.\"\"\"\n        super().__init__()\n        self.class_column = class_column\n        self.strategy = strategy\n        self.shuffle = shuffle\n        self.data_generator_ = BNEstimator()\n\n    def _generate_samples_for_class(\n        self, cls: str | int, needed: int, data_columns: list, types_dict: dict\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate synthetic samples for a specific minority class.\n\n        Args:\n            cls: Target class value to generate samples for.\n            needed: Number of synthetic samples needed for this class.\n            data_columns: List of column names in the original dataset.\n            types_dict: Dictionary mapping columns to their data types\n                (e.g., 'disc_num' for discrete numeric).\n\n        Returns:\n            samples: Generated samples with proper data types.\n        \"\"\"\n        samples = self.data_generator_.sample(\n            needed, evidence={self.class_column: cls}, filter_neg=False\n        )[data_columns]\n        if samples.shape[0] &lt; needed:\n            additional = self.data_generator_.sample(\n                needed, evidence={self.class_column: cls}, filter_neg=False\n            )[data_columns]\n            samples = pd.concat([samples, additional.sample(needed - samples.shape[0])])\n        return self._adjust_sample_types(samples, types_dict)\n\n    def _adjust_sample_types(\n        self, samples: pd.DataFrame, types_dict: dict\n    ) -&gt; pd.DataFrame:\n        \"\"\"Adjust data types of generated samples to match original data.\n\n        Args:\n            samples: Generated synthetic samples.\n            types_dict: Dictionary mapping columns to their data types.\n\n        Returns:\n            samples: Samples with corrected data types.\n        \"\"\"\n        disc_num_cols = {\n            col for col, dtype in types_dict.items() if dtype == \"disc_num\"\n        }\n        samples = samples.apply(\n            lambda col: col.astype(int) if col.name in disc_num_cols else col\n        )\n        return samples\n\n    def _balance_classes(\n        self, data: pd.DataFrame, class_counts: pd.Series, target_size: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate synthetic samples to balance class distribution.\n\n        Args:\n            data: Original dataset with target class column.\n            class_counts: Count of samples per class.\n            target_size: Target number of samples per class.\n\n        Returns:\n            balanced_data: Balanced dataset containing original and synthetic samples.\n        \"\"\"\n        samples = []\n        types_dict = self.data_generator_.bn_.descriptor[\"types\"]\n\n        # Calculate needed samples for each class\n        needed_samples = (target_size - class_counts).clip(lower=0)\n\n        # Generate samples for classes requiring augmentation\n        for cls, needed in needed_samples.items():\n            if needed &gt; 0:\n                samples.append(\n                    self._generate_samples_for_class(\n                        cls, needed, data.columns, types_dict\n                    )\n                )\n\n        # Combine original data with all generated samples at once\n        return (\n            pd.concat([data] + samples, ignore_index=True) if samples else data.copy()\n        )\n\n    def _fit_resample(\n        self, X: pd.DataFrame | np.ndarray, y: pd.Series | np.ndarray, **params: Any\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Resample the dataset using Bayesian Network synthetic generation.\n        Args:\n            X: Feature matrix.\n            y: Target vector.\n\n        Returns:\n            X_res: Resampled feature matrix.\n            y_res: Corresponding resampled target vector.\n\n        Raises:\n            NotFittedError: If synthetic generator fails to fit Bayesian Network.\n\n        Note:\n            1. Combines X and y into single DataFrame for Bayesian Network learning\n            2. Determines target sample sizes based on strategy\n            3. Generates synthetic samples for minority classes using conditional sampling\n            4. Preserves original data types and column names\n        \"\"\"\n\n        # Combine X and y into a DataFrame with class column\n        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X.copy()\n        y_series = pd.Series(y) if not isinstance(y, pd.Series) else y.copy()\n        if self.class_column is None:\n            self.class_column = y.name if hasattr(y, \"name\") else \"class\"\n        data = X_df.assign(**{self.class_column: y_series})\n\n        # Preprocess data\n        n_bins = 5\n        feature_types = pp = Preprocessor([]).get_nodes_types(data)\n        for k in feature_types:\n            if (feature_types[k] != \"cont\") &amp; (data[k].nunique() &gt; n_bins):\n                n_bins = data[k].nunique()\n        encoder = LabelEncoder()\n\n        discretizer = KBinsDiscretizer(\n            n_bins=n_bins, encode=\"ordinal\", strategy=\"quantile\"\n        )\n\n        pp = Preprocessor([(\"encoder\", encoder), (\"discretizer\", discretizer)])\n        preprocessed_data, _ = pp.apply(data)\n\n        # Fit Bayesian Network\n        self.data_generator_.use_mixture = True\n        fit_package = (preprocessed_data, pp.info, data)\n        self.data_generator_.fit(X=fit_package)\n\n        if self.data_generator_.bn_ is None:\n            raise NotFittedError(\"Generator model must be fitted first.\")\n\n        # Determine target class size\n        class_counts = (\n            data[self.class_column].value_counts().sort_values(ascending=False)\n        )\n        target_size = (\n            class_counts.iloc[0] if self.strategy == \"max_class\" else self.strategy\n        )\n\n        # Generate synthetic samples for minority classes\n        balanced_data = self._balance_classes(data, class_counts, target_size)\n        # shuffle data\n        if self.shuffle:\n            balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n        # Split back into features and target\n        X_res = balanced_data.drop(columns=[self.class_column]).to_numpy()\n        y_res = balanced_data[self.class_column].to_numpy()\n\n        return X_res, y_res\n</code></pre>"},{"location":"api/oversampling/bn_oversampling/#applybn.imbalanced.over_sampling.BNOverSampler.__init__","title":"<code>__init__(class_column=None, strategy='max_class', shuffle=True)</code>","text":"<p>Initialize the BNOverSampler.</p> Source code in <code>applybn/imbalanced/over_sampling/bn_over_sampler.py</code> <pre><code>def __init__(self, class_column=None, strategy=\"max_class\", shuffle=True):\n    \"\"\"Initialize the BNOverSampler.\"\"\"\n    super().__init__()\n    self.class_column = class_column\n    self.strategy = strategy\n    self.shuffle = shuffle\n    self.data_generator_ = BNEstimator()\n</code></pre>"},{"location":"api/oversampling/bn_oversampling/#_2","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<pre><code>from applybn.imbalanced.over_sampling import BNOverSampler\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0441 BN \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 GMM (\u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0447\u0435\u0440\u0435\u0437 use_mixture=True)\noversampler = BNOverSampler(\n    class_column='target', \n    strategy='max_class'  # \u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u0443 \u0441\u0430\u043c\u043e\u0433\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430\n)\n\n# \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c P(X|class) \u0438\u0437 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 BN\nX_res, y_res = oversampler.fit_resample(X, y)\n</code></pre>"},{"location":"development/contributing/","title":"\u041a\u0430\u043a \u0432\u043d\u0435\u0441\u0442\u0438 \u0441\u0432\u043e\u0439 \u0432\u043a\u043b\u0430\u0434","text":""},{"location":"development/contributing/#-","title":"\u0411\u044b\u0441\u0442\u0440\u044b\u0439 \u0447\u0435\u043a-\u043b\u0438\u0441\u0442","text":"<ul> <li> \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u0444\u043e\u0440\u043a \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f</li> <li> \u041a\u043b\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439</li> <li> \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u043d\u043e\u0432\u0443\u044e \u0432\u0435\u0442\u043a\u0443</li> <li> \u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u043e\u0441\u0442\u044c \u0441\u043e sklearn</li> <li> \u041d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u0438\u043b\u0438 \u043e\u0431\u043d\u043e\u0432\u0438\u0442\u044c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u0441 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\u043c\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f</li> <li> \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e serve</li> <li> \u041d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u044e\u043d\u0438\u0442-\u0442\u0435\u0441\u0442\u044b</li> <li> \u0417\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0442\u0435\u0441\u0442\u044b, \u0447\u0442\u043e\u0431\u044b \u0443\u0431\u0435\u0434\u0438\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u0432\u0441\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442</li> <li> \u0417\u0430\u043a\u043e\u043c\u043c\u0438\u0442\u0438\u0442\u044c \u0441\u0432\u043e\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f</li> <li> \u041e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u0441\u0432\u043e\u044e \u0432\u0435\u0442\u043a\u0443</li> <li> \u041e\u0442\u043a\u0440\u044b\u0442\u044c pull request</li> </ul>"},{"location":"development/contributing/#_2","title":"\u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e \u0443\u0447\u0430\u0441\u0442\u0438\u044e","text":"<p>\u041c\u044b \u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u043c \u0432\u043a\u043b\u0430\u0434 \u0432 \u043f\u0440\u043e\u0435\u043a\u0442. \u0427\u0442\u043e\u0431\u044b \u0432\u043d\u0435\u0441\u0442\u0438 \u0441\u0432\u043e\u0439 \u0432\u043a\u043b\u0430\u0434, \u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0441\u043b\u0435\u0434\u0443\u0439\u0442\u0435 \u044d\u0442\u0438\u043c \u0448\u0430\u0433\u0430\u043c:</p> <ol> <li> <p>\u041a\u043b\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f</p> <ul> <li>\u0421\u0434\u0435\u043b\u0430\u0439\u0442\u0435 \u0444\u043e\u0440\u043a \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f \u0432 \u0441\u0432\u043e\u0439 \u0430\u043a\u043a\u0430\u0443\u043d\u0442 \u043d\u0430 GitHub.</li> <li>\u041a\u043b\u043e\u043d\u0438\u0440\u0443\u0439\u0442\u0435 \u0441\u0432\u043e\u0439 \u0444\u043e\u0440\u043a\u043d\u0443\u0442\u044b\u0439 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439 \u043d\u0430 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u0443\u044e \u043c\u0430\u0448\u0438\u043d\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u044b:   <pre><code>git clone https://github.com/Anaxagor/applyBN.git\n</code></pre></li> <li>\u041f\u0435\u0440\u0435\u0439\u0434\u0438\u0442\u0435 \u0432 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044e \u043f\u0440\u043e\u0435\u043a\u0442\u0430:   <pre><code>cd applyBN\n</code></pre></li> </ul> </li> <li> <p>\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0432\u0435\u0442\u043a\u0438</p> <ul> <li>\u0421\u043e\u0437\u0434\u0430\u0439\u0442\u0435 \u043d\u043e\u0432\u0443\u044e \u0432\u0435\u0442\u043a\u0443 \u0434\u043b\u044f \u0432\u0430\u0448\u0435\u0439 \u0444\u0438\u0447\u0438 \u0438\u043b\u0438 \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0431\u0430\u0433\u0430:   <pre><code>git checkout -b \u0432\u0430\u0448\u0435-\u0438\u043c\u044f-\u0432\u0435\u0442\u043a\u0438\n</code></pre></li> </ul> </li> <li> <p>\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u043e\u0441\u0442\u0438 \u0441\u043e sklearn</p> <ul> <li>\u0423\u0431\u0435\u0434\u0438\u0442\u0435\u0441\u044c, \u0447\u0442\u043e \u0432\u0430\u0448\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u044b \u0441 <code>scikit-learn</code>, \u0433\u0434\u0435 \u044d\u0442\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u043c\u043e. \u042d\u0442\u043e \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u0432 \u0441\u0435\u0431\u044f \u0441\u043e\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435 \u0438\u0445 \u043a\u043e\u043d\u0432\u0435\u043d\u0446\u0438\u0439, \u043d\u0430\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0438 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u0435 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u043e\u0441\u0442\u0438.</li> </ul> </li> <li> <p>\u041d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0441 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f</p> <ul> <li>\u041c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c <code>mkdocs</code> \u0434\u043b\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438. \u0414\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u0438\u043b\u0438 \u043e\u0431\u043d\u043e\u0432\u0438\u0442\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u0432 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 <code>docs</code>.</li> <li>\u0412\u043a\u043b\u044e\u0447\u0438\u0442\u0435 \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u043a\u0430\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u0430\u0448\u0443 \u0444\u0438\u0447\u0443 \u0438\u043b\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f.</li> <li>\u0414\u043b\u044f \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u0435:   <pre><code>mkdocs serve\n</code></pre></li> </ul> </li> <li> <p>\u041d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u044e\u043d\u0438\u0442-\u0442\u0435\u0441\u0442\u043e\u0432</p> <ul> <li>\u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u044e\u043d\u0438\u0442-\u0442\u0435\u0441\u0442\u044b \u0434\u043b\u044f \u0432\u0430\u0448\u0435\u0433\u043e \u043a\u043e\u0434\u0430, \u0447\u0442\u043e\u0431\u044b \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0435\u0433\u043e \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c.</li> <li>\u0420\u0430\u0437\u043c\u0435\u0441\u0442\u0438\u0442\u0435 \u0432\u0430\u0448\u0438 \u0442\u0435\u0441\u0442\u044b \u0432 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 <code>tests</code>.</li> <li>\u0417\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u0435 \u0442\u0435\u0441\u0442\u044b, \u0447\u0442\u043e\u0431\u044b \u0443\u0431\u0435\u0434\u0438\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u0432\u0441\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442:   <pre><code>pytest -s tests\n</code></pre></li> </ul> </li> <li> <p>\u041e\u0442\u043f\u0440\u0430\u0432\u043a\u0430 \u0432\u0430\u0448\u0438\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439</p> <ul> <li>\u0417\u0430\u043a\u043e\u043c\u043c\u0438\u0442\u044c\u0442\u0435 \u0441\u0432\u043e\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0441 \u043e\u043f\u0438\u0441\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435\u043c \u043a\u043e\u043c\u043c\u0438\u0442\u0430:   <pre><code>git commit -m \"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0432\u0430\u0448\u0438\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439\"\n</code></pre></li> <li>\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u0441\u0432\u043e\u044e \u0432\u0435\u0442\u043a\u0443 \u0432 \u0432\u0430\u0448 \u0444\u043e\u0440\u043a\u043d\u0443\u0442\u044b\u0439 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439:   <pre><code>git push origin \u0432\u0430\u0448\u0435-\u0438\u043c\u044f-\u0432\u0435\u0442\u043a\u0438\n</code></pre></li> <li>\u041e\u0442\u043a\u0440\u043e\u0439\u0442\u0435 pull request \u0432 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u043c \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0438 \u0438 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u044c\u0442\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0432\u0430\u0448\u0438\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439.</li> </ul> </li> </ol> <p>\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u0430\u0448 \u0432\u043a\u043b\u0430\u0434!</p>"},{"location":"examples/bn_oversampling/","title":"\u041f\u0440\u0438\u043c\u0435\u0440: \u0440\u0430\u0431\u043e\u0442\u0430 \u0441 \u0441\u0438\u043b\u044c\u043d\u044b\u043c \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u043e\u043c \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e BNOverSampler \u0438 SMOTE","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043e, \u043a\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u0441\u0438\u043b\u044c\u043d\u044b\u043c \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u043e\u043c \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0434\u0432\u0443\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0440\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430: BNOverSampler (\u043c\u0435\u0442\u043e\u0434 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439) \u0438 SMOTE. \u041c\u044b \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c \u0438\u0445 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u043e\u0432\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a F1-score \u0438 AUC-ROC.</p>"},{"location":"examples/bn_oversampling/#_1","title":"\u0417\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438","text":"<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom applybn.imbalanced.over_sampling import BNOverSampler  # \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439 \u043f\u0430\u043a\u0435\u0442\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score\n</code></pre>"},{"location":"examples/bn_oversampling/#1","title":"\u0428\u0430\u0433 1: \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0430","text":"<p>\u041c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u043c \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u0435 \u0438 \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 30:1 \u0434\u043b\u044f \u0438\u043c\u0438\u0442\u0430\u0446\u0438\u0438 \u0441\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0430: <pre><code>data = pd.read_csv('data/bn_oversampling/bank-marketing.csv', index_col=[0])\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441 5:1\n_, X_min, _, y_min = train_test_split(\n    X[y == 1], y[y == 1], \n    test_size=0.2, \n    random_state=42\n)\nX_imb = pd.concat([X[y == 0], X_min])\ny_imb = pd.concat([y[y == 0], y_min])\n</code></pre></p>"},{"location":"examples/bn_oversampling/#2","title":"\u0428\u0430\u0433 2: \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0438","text":"<p>\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 70% \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 30% \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044f \u0441\u0442\u0440\u0430\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044e: <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X_imb, y_imb, \n    test_size=0.3, \n    stratify=y_imb,\n    random_state=42\n)\n</code></pre></p>"},{"location":"examples/bn_oversampling/#3","title":"\u0428\u0430\u0433 3: \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0440\u0435\u0441\u0435\u043c\u043f\u043b\u0435\u0440\u043e\u0432","text":"<p>\u041d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u043c \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430: <pre><code>bn_sampler = BNOverSampler(class_column='class', strategy='max_class')  # \u0411\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u0443\u0435\u0442 \u0434\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043c\u0430\u0436\u043e\u0440\u0438\u0442\u0430\u0440\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430\nsmote = SMOTE(k_neighbors=5, random_state=42)\n</code></pre></p>"},{"location":"examples/bn_oversampling/#4","title":"\u0428\u0430\u0433 4: \u0420\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<p>\u041f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043e\u0431\u0430 \u043c\u0435\u0442\u043e\u0434\u0430 \u0438 \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u043c \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f: <pre><code># BNOverSampler\nstart_bn = time.time()\nX_bn, y_bn = bn_sampler.fit_resample(X_train, y_train)\ntime_bn = time.time() - start_bn\n\n# SMOTE\nstart_smote = time.time()\nX_smote, y_smote = smote.fit_resample(X_train, y_train)\ntime_smote = time.time() - start_smote\n</code></pre></p>"},{"location":"examples/bn_oversampling/#5","title":"\u0428\u0430\u0433 5: \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u043e\u0446\u0435\u043d\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c RandomForestClassifier \u0434\u043b\u044f \u043e\u0431\u043e\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0441\u043b\u0435 \u0440\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430: <pre><code>clf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_bn, y_bn)\nbn_pred = clf.predict(X_test)\nbn_proba = clf.predict_proba(X_test)[:, 1]\n\nclf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_smote.fit(X_smote, y_smote)\nsmote_pred = clf_smote.predict(X_test)\nsmote_proba = clf_smote.predict_proba(X_test)[:, 1]\n</code></pre></p>"},{"location":"examples/bn_oversampling/#6","title":"\u0428\u0430\u0433 6: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b","text":"\u041c\u0435\u0442\u043e\u0434 F1-Score AUC-ROC \u0412\u0440\u0435\u043c\u044f (\u0441) BNOverSampler 0.575 0.858 35.1 SMOTE 0.556 0.844 0.0 <p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 BNOverSampler, \u043a\u043e\u0433\u0434\u0430 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0432\u0430\u0436\u043d\u044b \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f, \u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u0441\u0443\u0440\u0441\u044b \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u044b.</p>"},{"location":"examples/causal_feature_selection/","title":"Robustness Analysis for Causal Feature Selection","text":""},{"location":"examples/causal_feature_selection/#_1","title":"\u0410\u043d\u0430\u043b\u0438\u0437 \u0440\u043e\u0431\u0430\u0441\u0442\u043d\u043e\u0441\u0442\u0438: \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u043c \u043d\u0435\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c <code>CausalFeatureSelector</code> \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u0430\u0436\u0435 \u043f\u0440\u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0438 \u043d\u0435\u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u0432\u0438\u043d\u0435.</p>"},{"location":"examples/causal_feature_selection/#1","title":"\u0428\u0430\u0433 1: \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<pre><code>data = pd.read_csv('data/feature_selection/WineQT.csv')\ndata = data.drop(columns=['Id'])\nX = data.drop(\"quality\", axis=1)\ny = data[\"quality\"].values\nfeature_names = list(data.columns)[:-1]\n</code></pre>"},{"location":"examples/causal_feature_selection/#2","title":"\u0428\u0430\u0433 2: \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0432\u0442\u043e\u0440\u043e\u0433\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0430) \u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u0448\u0438\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043d\u0435\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","text":"<pre><code>poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_poly = poly.fit_transform(X)\nnp.random.seed(42)\nX_poly = np.random.permutation(X_poly)  # \u0423\u043d\u0438\u0447\u0442\u043e\u0436\u0435\u043d\u0438\u0435 \u0441\u0432\u044f\u0437\u0438 \u0441 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\n\n# \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0438 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nX_combined = np.hstack([X, X_poly])\nprint(f\"\u0412\u0441\u0435\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: {X_combined.shape[1]} (30 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 + {X_poly.shape[1]} \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445)\")\n</code></pre>"},{"location":"examples/causal_feature_selection/#3","title":"\u0428\u0430\u0433 3: \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440\u043e\u0432 \u0434\u043b\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","text":"<pre><code>n_uninformative_steps = np.arange(0, 200, 20)\naccuracies_full = []\naccuracies_selected = []\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439\nselector = CausalFeatureSelector(n_bins=5)\nmodel = LogisticRegression(max_iter=1000, random_state=42)\n</code></pre>"},{"location":"examples/causal_feature_selection/#4","title":"\u0428\u0430\u0433 4: \u041f\u043e\u0441\u0442\u0435\u043f\u0435\u043d\u043d\u043e\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","text":"<pre><code>for n in n_uninformative_steps:\n    # \u0412\u044b\u0431\u043e\u0440 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430 \u043d\u0435\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    X_current = np.hstack([X, X_poly[:, :n]])\n\n    # \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_current, y, test_size=0.3, random_state=42\n    )\n\n    # \u0421 \u043e\u0442\u0431\u043e\u0440\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    selector.fit(X_train, y_train)\n    X_train_sel = selector.transform(X_train)\n    X_test_sel = selector.transform(X_test)\n\n    model.fit(X_train_sel, y_train)\n    acc_selected = accuracy_score(y_test, model.predict(X_test_sel))\n\n    # \u0411\u0435\u0437 \u043e\u0442\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    model.fit(X_train, y_train)\n    acc_full = accuracy_score(y_test, model.predict(X_test))\n\n    # \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n    accuracies_full.append(acc_full)\n    accuracies_selected.append(acc_selected)\n\n    print(f\"\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043e {n} \u0448\u0443\u043c\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 | \"\n          f\"\u041e\u0442\u043e\u0431\u0440\u0430\u043d\u043e: {X_train_sel.shape[1]} \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 | \"\n          f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c: {acc_selected:.3f} (\u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435) vs {acc_full:.3f} (\u043f\u043e\u043b\u043d\u044b\u0435)\")\n</code></pre>"},{"location":"examples/causal_feature_selection/#5","title":"\u0428\u0430\u0433 5: \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438","text":"<p><pre><code>plt.figure(figsize=(10, 6))\nplt.plot(n_uninformative_steps, accuracies_full, 'r--', label='\u041f\u043e\u043b\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432')\nplt.plot(n_uninformative_steps, accuracies_selected, 'g-', label='\u041e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438')\nplt.xlabel('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043d\u0435\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432')\nplt.ylabel('\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u0442\u0435\u0441\u0442\u0435')\nplt.title('\u0420\u043e\u0431\u0430\u0441\u0442\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043a \u043d\u0435\u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c')\nplt.legend()\nplt.grid(True)\nplt.show()\n\ntrue_features = set(range(30))\nselected_features = set(np.where(selector.support_)[0])\n\nprint(f\"\\n\u0410\u043d\u0430\u043b\u0438\u0437 \u043e\u043a\u043e\u043d\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0442\u0431\u043e\u0440\u0430:\")\nprint(f\"- \u0412\u0441\u0435\u0433\u043e \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043e: {len(selected_features)} \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\")\nprint(f\"- \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043e \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: {len(true_features &amp; selected_features)}/{30}\")\nprint(f\"- \u041e\u0442\u043e\u0431\u0440\u0430\u043d\u043e \u0448\u0443\u043c\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: {len(selected_features - true_features)}\")\n</code></pre> </p>"},{"location":"examples/causal_feature_selection/#_2","title":"\u0417\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0440\u043e\u0431\u0430\u0441\u0442\u043d\u043e\u0441\u0442\u044c \u043c\u0435\u0442\u043e\u0434\u0430 \u0432 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u0446\u0435\u043d\u0430\u0440\u0438\u044f\u0445, \u0433\u0434\u0435 \u043d\u0430\u0431\u043e\u0440\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0447\u0430\u0441\u0442\u043e \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u043c\u043d\u043e\u0433\u043e \u043d\u0435\u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0445 \u0438\u043b\u0438 \u0441\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \u041c\u0435\u0445\u0430\u043d\u0438\u0437\u043c \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0442\u0431\u043e\u0440\u0430 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e \u0432\u044b\u044f\u0432\u043b\u044f\u0435\u0442 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438, \u043d\u0435\u0441\u043c\u043e\u0442\u0440\u044f \u043d\u0430 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u044e\u0449\u0435\u0435 \u0448\u0443\u043c\u043e\u0432\u043e\u0435 \u0437\u0430\u0433\u0440\u044f\u0437\u043d\u0435\u043d\u0438\u0435.</p>"},{"location":"examples/concept_causal_effect/","title":"\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u0438\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043e \u043f\u043e\u0448\u0430\u0433\u043e\u0432\u043e\u0435 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e \u0441\u043a\u0440\u0438\u043f\u0442\u0443 <code>concept_explainer.py</code>, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0430 <code>ConceptCausalExplainer</code> \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>applybn</code>. \u0421\u043a\u0440\u0438\u043f\u0442 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043f\u043e\u043b\u043d\u044b\u0439 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440 \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445, \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432 \u044d\u0442\u0438\u0445 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043c\u043e\u0434\u0435\u043b\u0438, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c.</p>"},{"location":"examples/concept_causal_effect/#_2","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u0421\u043a\u0440\u0438\u043f\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u0448\u0430\u0433\u0438:</p> <ol> <li>\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445: \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u0442 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 UCI Adult, \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043e\u0447\u0438\u0441\u0442\u043a\u0443, one-hot \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</li> <li>\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430 \u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u0435\u043b\u044f: \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440 <code>ConceptCausalExplainer</code>.</li> <li>\u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432: \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u0435\u043b\u044c \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u0432 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432: \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432.</li> <li>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438: \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 Random Forest \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445.</li> <li>\u0420\u0430\u0441\u0447\u0435\u0442 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438: \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u044e\u0442\u0441\u044f \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438.</li> <li>\u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432: \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0445 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438.</li> <li>\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432: \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u044b \"\u0442\u043e\u0440\u043d\u0430\u0434\u043e\" \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u044d\u0442\u0438\u0445 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432.</li> <li>\u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432: \u0441\u043a\u0440\u0438\u043f\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0442\u0438\u0432\u043d\u044b \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043d\u043e\u0433\u043e \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430.</li> </ol>"},{"location":"examples/concept_causal_effect/#1","title":"1. \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u0438 \u0438\u043c\u043f\u043e\u0440\u0442","text":"<p>\u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u043c\u044b \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438. <code>pandas</code> \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043c\u0430\u043d\u0438\u043f\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u043c\u0438, <code>sklearn</code> \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445), \u0430 <code>ConceptCausalExplainer</code> \u2014 \u044d\u0442\u043e \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441 \u0438\u0437 \u043d\u0430\u0448\u0435\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438.</p> <pre><code>import pandas as pd\nfrom rich import print as rprint # \u0434\u043b\u044f \u043a\u0440\u0430\u0441\u0438\u0432\u043e\u0433\u043e \u0432\u044b\u0432\u043e\u0434\u0430\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# \u041e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u0438\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432\nfrom applybn.explainable.causal_analysis import ConceptCausalExplainer\n</code></pre>"},{"location":"examples/concept_causal_effect/#2","title":"2. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f <code>load_and_preprocess_data</code> \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 UCI Adult, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u043c \u044d\u0442\u0430\u043b\u043e\u043d\u043d\u044b\u043c \u043d\u0430\u0431\u043e\u0440\u043e\u043c \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438.</p> <pre><code>def load_and_preprocess_data():\n    \"\"\"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 UCI Adult.\n\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        tuple: (X_processed, y, X_original), \u0433\u0434\u0435:\n            X_processed (pd.DataFrame): \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0433\u043e\u0442\u043e\u0432\u044b\u0435 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n            y (pd.Series): \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438 (\u0434\u043e\u0445\u043e\u0434 &gt;50K \u0438\u043b\u0438 &lt;=50K).\n            X_original (pd.DataFrame): \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043e \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f/\u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n    \"\"\"\n    # URL \u0434\u043b\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 UCI Adult\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n    # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0438\u043c\u0435\u043d \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\n    column_names = [\n        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n        \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n        \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\",\n    ]\n    # \u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u044f ' ?' \u043a\u0430\u043a \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f NaN\n    data = pd.read_csv(url, names=column_names, header=None, na_values=\" ?\")\n\n    # \u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u043e\u043a \u0441 \u043b\u044e\u0431\u044b\u043c\u0438 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438\n    data.dropna(inplace=True)\n    # \u0421\u0431\u0440\u043e\u0441 \u0438\u043d\u0434\u0435\u043a\u0441\u0430 DataFrame \u043f\u043e\u0441\u043b\u0435 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u044f \u0441\u0442\u0440\u043e\u043a\n    data.reset_index(drop=True, inplace=True)\n\n    # \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (X) \u0438 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 (y)\n    # X_original \u0445\u0440\u0430\u043d\u0438\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043e \u043b\u044e\u0431\u043e\u0433\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f, \u0447\u0442\u043e \u043f\u043e\u043b\u0435\u0437\u043d\u043e \u0434\u043b\u044f \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u043f\u043e\u0437\u0436\u0435\n    X_original = data.drop(\"income\", axis=1).reset_index(drop=True)\n    # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0434\u043e\u0445\u043e\u0434\u0430 \u0432 \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438 (1, \u0435\u0441\u043b\u0438 &gt;50K, \u0438\u043d\u0430\u0447\u0435 0)\n    y = (\n        data[\"income\"]\n        .apply(lambda x: 1 if x.strip() == \"&gt;50K\" else 0)\n        .reset_index(drop=True)\n    )\n\n    # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u0434\u043b\u044f one-hot \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n    categorical_cols = X_original.select_dtypes(include=[\"object\"]).columns\n    # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f OneHotEncoder:\n    # - sparse_output=False \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u043e\u043d \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043f\u043b\u043e\u0442\u043d\u044b\u0439 \u043c\u0430\u0441\u0441\u0438\u0432\n    # - handle_unknown='ignore' \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0435\u0441\u043b\u0438 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f, \u043e\u043d\u0430 \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u0443\u0435\u0442\u0441\u044f\n    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n    # \u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 one-hot \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n    X_encoded = pd.DataFrame(\n        encoder.fit_transform(X_original[categorical_cols]),\n        columns=encoder.get_feature_names_out(categorical_cols),\n    )\n\n    # \u041e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432\n    X_numeric = X_original.select_dtypes(exclude=[\"object\"]).reset_index(drop=True)\n    # \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043d\u043e\u0432\u044b\u043c\u0438 one-hot \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438\n    X_processed = pd.concat(\n        [X_numeric.reset_index(drop=True), X_encoded.reset_index(drop=True)], axis=1\n    )\n\n    # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u0434\u043b\u044f \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n    numeric_cols_to_scale = X_numeric.columns # \u041f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435: \u043e\u043d\u0438 \u0438\u0437 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e X_numeric, \u0430 \u043d\u0435 X_original\n    # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f StandardScaler \u0434\u043b\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u0443\u0442\u0435\u043c \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u043e \u0435\u0434\u0438\u043d\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438\n    scaler = StandardScaler()\n    # \u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043a \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    X_processed[numeric_cols_to_scale] = scaler.fit_transform(X_processed[numeric_cols_to_scale])\n    # \u0421\u0431\u0440\u043e\u0441 \u0438\u043d\u0434\u0435\u043a\u0441\u0430 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\n    X_processed.reset_index(drop=True, inplace=True)\n\n    return X_processed, y, X_original\n</code></pre>"},{"location":"examples/concept_causal_effect/#3","title":"3. \u041e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0431\u043b\u043e\u043a \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f","text":"<p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f <code>main</code> \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0443\u0435\u0442 \u0432\u0435\u0441\u044c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u0438\u044f.</p> <pre><code>def main():\n    \"\"\"\u0414\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f CausalModelExplainer \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445.\"\"\"\n    # \u0428\u0430\u0433 1: \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\n    X, y, original_X = load_and_preprocess_data()\n\n    # \u0428\u0430\u0433 2: \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f (D) \u0438 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e (N)\n    # \u041d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043d\u0430 \u043d\u0430\u0431\u043e\u0440 \u0434\u043b\u044f '\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f' (D), \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0439 \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432,\n    # \u0438 '\u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439' \u043d\u0430\u0431\u043e\u0440 (N), \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0434\u0440\u0443\u0433\u0438\u0445 \u0446\u0435\u043b\u0435\u0439 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438, \u0445\u043e\u0442\u044f \u0437\u0434\u0435\u0441\u044c \u044d\u0442\u043e \u044f\u0432\u043d\u043e \u043d\u0435 \u0434\u0435\u043b\u0430\u0435\u0442\u0441\u044f).\n    # shuffle=False \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0430 \u043f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438, \u0445\u043e\u0442\u044f random_state \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c.\n    D, N = train_test_split(X, test_size=0.3, random_state=42, shuffle=False)\n    # \u0421\u0431\u0440\u043e\u0441 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432 \u0434\u043b\u044f D \u0438 N, \u0447\u0442\u043e\u0431\u044b \u0443\u0431\u0435\u0434\u0438\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u043e\u043d\u0438 \u0447\u0438\u0441\u0442\u044b\u0435 \u0438 \u043d\u0430\u0447\u0438\u043d\u0430\u044e\u0442\u0441\u044f \u0441 0.\n    # drop=False \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u0441\u0442\u0430\u0440\u044b\u0439 \u0438\u043d\u0434\u0435\u043a\u0441 \u043a\u0430\u043a \u043d\u043e\u0432\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u043d\u043e \u0434\u043b\u044f \u043e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u044f.\n    D.reset_index(drop=False, inplace=True)\n    N.reset_index(drop=False, inplace=True)\n\n    # \u0428\u0430\u0433 3: \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430 \u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u0435\u043b\u044f\n    # \u042d\u0442\u043e \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u043a\u0442 \u043d\u0430\u0448\u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 ConceptCausalExplainer.\n    explainer = ConceptCausalExplainer()\n\n    # \u0428\u0430\u0433 4: \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432\n    # \u041c\u0435\u0442\u043e\u0434 extract_concepts \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f (D) \u0438 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 (N)\n    # \u0434\u043b\u044f \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0437\u043d\u0430\u0447\u0438\u043c\u044b\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0442\u043e\u0447\u0435\u043a \u0434\u0430\u043d\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0430\u0442\u0435\u043c \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u044b.\n    cluster_concepts = explainer.extract_concepts(D, N)\n    rprint(f\"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u044b (\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432): {cluster_concepts}\")\n\n    # \u0428\u0430\u0433 5: \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432\n    # \u041c\u0435\u0442\u043e\u0434 generate_concept_space \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (X)\n    # \u0432 \u043d\u043e\u0432\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e (A), \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u043e\u0435 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0435 \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0439 \u043a\u043e\u043d\u0446\u0435\u043f\u0442.\n    # \u041e\u0431\u044b\u0447\u043d\u043e \u044d\u0442\u043e \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u043f\u0440\u0438\u0441\u0432\u043e\u0435\u043d\u0438\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 \u0442\u043e\u0447\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 X \u043e\u0434\u043d\u043e\u043c\u0443 \u0438\u043b\u0438 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c.\n    A = explainer.generate_concept_space(X, cluster_concepts)\n    rprint(f\"\u041f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 A:\\n{A.head()}\")\n\n    # \u0428\u0430\u0433 6: \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\n    # \u0414\u043b\u044f \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f RandomForestClassifier \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 (X) \u0438 \u043c\u0435\u0442\u043a\u0430\u0445 (y).\n    # \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u044d\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u043e\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c.\n    predictive_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    predictive_model.fit(X, y)\n\n    # \u0428\u0430\u0433 7: \u0420\u0430\u0441\u0447\u0435\u0442 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438\n    # \u041c\u0435\u0442\u043e\u0434 calculate_confidence_uncertainty \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\n    # \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0435\u0435 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 X.\n    confidence, uncertainty = explainer.calculate_confidence_uncertainty(\n        X, y, predictive_model\n    )\n    rprint(f\"\u0420\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u0430\u044f \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c (\u043f\u0435\u0440\u0432\u044b\u0435 5): {confidence[:5]}\")\n    rprint(f\"\u0420\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u0430\u044f \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c (\u043f\u0435\u0440\u0432\u044b\u0435 5): {uncertainty[:5]}\")\n\n    # \u0428\u0430\u0433 8: \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0430\n    # \u0421\u043e\u0437\u0434\u0430\u044e\u0442\u0441\u044f \u043a\u043e\u043f\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 (A) \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0435 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043d\u043e\u0432\u044b\u0445 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432.\n    # \u041e\u043d\u0438 \u0431\u0443\u0434\u0443\u0442 \u0441\u043b\u0443\u0436\u0438\u0442\u044c \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u043c\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0430.\n    D_c_confidence = A.copy()\n    D_c_confidence[\"confidence\"] = confidence # \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\n\n    D_c_uncertainty = A.copy()\n    D_c_uncertainty[\"uncertainty\"] = uncertainty # \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\n\n    # \u0428\u0430\u0433 9: \u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432\n    # \u041c\u0435\u0442\u043e\u0434 estimate_causal_effects_on_continuous_outcomes \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0434\u0432\u0430\u0436\u0434\u044b:\n    # \u043e\u0434\u0438\u043d \u0440\u0430\u0437 \u0434\u043b\u044f \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u043e\u0434\u0438\u043d \u0440\u0430\u0437 \u0434\u043b\u044f \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438. \u041e\u043d \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442, \u043a\u0430\u043a \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432\n    # \u043d\u0430\u043b\u0438\u0447\u0438\u0438 \u0438\u043b\u0438 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u044d\u0442\u0438 \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b.\n    effects_confidence = explainer.estimate_causal_effects_on_continuous_outcomes(\n        D_c_confidence, outcome_name=\"confidence\"\n    )\n    rprint(f\"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u044b \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c:\\n{effects_confidence}\")\n\n    effects_uncertainty = explainer.estimate_causal_effects_on_continuous_outcomes(\n        D_c_uncertainty, outcome_name=\"uncertainty\"\n    )\n    rprint(f\"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u044b \u043d\u0430 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c:\\n{effects_uncertainty}\")\n\n    # \u0428\u0430\u0433 10: \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0439\n    # \u0414\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u044b \"\u0442\u043e\u0440\u043d\u0430\u0434\u043e\" \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043e\u0446\u0435\u043d\u0435\u043d\u043d\u044b\u0445 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432.\n    # \u042d\u0442\u0438 \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u044b \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442.\n    explainer.plot_tornado(\n        effects_confidence, title=\"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u044b \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438\", figsize=(10, 8)\n    )\n\n    explainer.plot_tornado(\n        effects_uncertainty,\n        title=\"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u044b \u043d\u0430 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438\",\n        figsize=(10, 8),\n    )\n\n    # \u0428\u0430\u0433 11: \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438 \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432\n    # \u041c\u0435\u0442\u043e\u0434 extract_concept_meanings \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043a\u0430\u0436\u0434\u044b\u0439 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0439 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\n    # \u043f\u0443\u0442\u0435\u043c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0438\u0437 original_X) \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u0438\u0437 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 D.\n    selected_features_per_concept = explainer.extract_concept_meanings(\n        D, cluster_concepts, original_X\n    )\n    # \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0432\u044b\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u0432 \u043a\u043e\u043d\u0441\u043e\u043b\u044c.\n    rprint(f\"\\n\u0414\u0435\u0442\u0430\u043b\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432: {selected_features_per_concept}\")\n\n# \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0430\u044f \u0438\u0434\u0438\u043e\u043c\u0430 Python \u0434\u043b\u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u0440\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0438 \u0441\u043a\u0440\u0438\u043f\u0442\u0430.\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p> </p>"},{"location":"examples/concept_causal_effect/#_3","title":"\u0417\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440 \u0441\u043a\u0440\u0438\u043f\u0442\u0430 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441\u043d\u044b\u0439 \u0440\u0430\u0431\u043e\u0447\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u043e\u0431\u044a\u044f\u0441\u043d\u0435\u043d\u0438\u0439 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432. \u0421\u043b\u0435\u0434\u0443\u044f \u044d\u0442\u0438\u043c \u0448\u0430\u0433\u0430\u043c, \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u043c\u043e\u0433\u0443\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e \u0442\u043e\u043c, \u043a\u0430\u043a \u0430\u0431\u0441\u0442\u0440\u0430\u043a\u0442\u043d\u044b\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u044b, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0438\u0437 \u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432 \u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044e\u0442 \u0434\u0435\u0439\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0432\u044b\u0445\u043e\u0434\u044f\u0449\u0435\u0435 \u0437\u0430 \u0440\u0430\u043c\u043a\u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</p> <p>\u041d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u043f\u0443\u0442\u0438 \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c-\u0437\u0430\u0433\u043b\u0443\u0448\u043a\u0430\u043c \u043d\u0430 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043f\u0443\u0442\u0438 \u043a \u0432\u0430\u0448\u0438\u043c \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0430\u043c.</p>"},{"location":"examples/custom_estimators/","title":"\u041e\u0446\u0435\u043d\u0449\u0438\u043a\u0438, \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u044b \u0438 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 (\u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c)","text":""},{"location":"examples/custom_estimators/#_2","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u041e\u0446\u0435\u043d\u0449\u0438\u043a\u0438 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u0443\u044e \u043e\u0431\u0435\u0440\u0442\u043a\u0443 \u0434\u043b\u044f <code>bamt</code> \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435, \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u043e\u043c \u0441 <code>sklearn</code>, \u0447\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u0430\u043c \u0431\u0435\u0441\u043f\u0440\u0435\u043f\u044f\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u043c\u0438 \u0441\u0435\u0442\u044f\u043c\u0438. \u0426\u0435\u043b\u044c \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0432 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u0431\u043a\u0438\u0445 \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u043e\u0432 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u043e\u0432.</p> <p>\u0424\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a <code>applybn</code> \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u044d\u0442\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435: <pre><code>\u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 (\u042f\u0434\u0440\u043e: bamt)\n         |\n         \u2193\n  \u041e\u0446\u0435\u043d\u0449\u0438\u043a\u0438 BN (\u041e\u0431\u0435\u0440\u0442\u043a\u0430 \u0434\u043b\u044f \u0441\u0435\u0442\u0435\u0439 bamt) [\u0423\u0440\u043e\u0432\u0435\u043d\u044c \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u0430]\n         |\n         \u2193\n     \u041a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u044b (\u0418\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441 applybn \u0434\u043b\u044f bamt) [\u0423\u0440\u043e\u0432\u0435\u043d\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f]\n</code></pre></p>"},{"location":"examples/custom_estimators/#_3","title":"\u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438","text":"<p>\u0412 <code>bamt</code> \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0442\u0440\u0438 \u0442\u0438\u043f\u0430 \u0441\u0435\u0442\u0435\u0439:</p> <ul> <li><code>HybridBN</code></li> <li><code>ContinuousBN</code></li> <li><code>DiscreteBN</code></li> </ul> <p>\u041a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e, <code>bamt</code> \u043c\u043e\u0436\u0435\u0442 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0442\u044c \u0442\u0438\u043f\u044b \u0434\u0430\u043d\u043d\u044b\u0445.</p>"},{"location":"examples/custom_estimators/#_4","title":"\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u0442\u0438\u043f\u044b \u0434\u0430\u043d\u043d\u044b\u0445:","text":"<pre><code># \u0414\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u0435 \u0442\u0438\u043f\u044b\ncategorical_types = [\"str\", \"O\", \"b\", \"categorical\", \"object\", \"bool\"]\n# \u0414\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0442\u0438\u043f\u044b\ninteger_types = [\"int32\", \"int64\"]\n# \u041d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0435 \u0442\u0438\u043f\u044b\nfloat_types = [\"float32\", \"float64\"]\n</code></pre> <p>Danger</p> <p>\u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u044d\u0442\u043e\u0442 \u0443\u0440\u043e\u0432\u0435\u043d\u044c, \u0435\u0441\u043b\u0438 \u0432 \u044d\u0442\u043e\u043c \u043d\u0435\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438.</p>"},{"location":"examples/custom_estimators/#_5","title":"\u041e\u0446\u0435\u043d\u0449\u0438\u043a\u0438","text":"<p>\u041e\u0446\u0435\u043d\u0449\u0438\u043a\u0438 \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u044b \u0434\u043b\u044f \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u043e\u0432 \u0438 \u043d\u0430\u0441\u043b\u0435\u0434\u0443\u044e\u0442\u0441\u044f \u043e\u0442 <code>sklearn.BaseEstimator</code>.</p>"},{"location":"examples/custom_estimators/#bn","title":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0438\u043f\u0430 BN","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u044d\u0442\u043e\u0442 \u0441\u0442\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043c\u0435\u0442\u043e\u0434 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f <code>bn_type</code> \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445: <pre><code>estimator = BNEstimator()\ndata = pd.read_csv(DATA_PATH)\nestimator.detect_bn(data)\n</code></pre></p>"},{"location":"examples/custom_estimators/#fit","title":"\u041c\u0435\u0442\u043e\u0434 Fit","text":"<pre><code>estimator = BNEstimator()\ndata = pd.read_csv(DATA_PATH)\ndescriptor = {\"types\": {...}, \"signs\": {...}}\npreprocessed_data = preprocessor([...]).apply(data)\n\nfit_package = (preprocessed_data, descriptor, data)\nestimator.fit(fit_package)\n</code></pre>"},{"location":"examples/custom_estimators/#partialfalse","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 (<code>partial=False</code>, \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e)","text":""},{"location":"examples/custom_estimators/#_6","title":"\u041d\u0438\u0437\u043a\u043e\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434:","text":"<pre><code>bn = HybridBN(use_mixture=False, has_logit=True)  # \u041c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043b\u044e\u0431\u043e\u0439 \u0442\u0438\u043f \u0441\u0435\u0442\u0438\nbn.add_nodes(descriptor)\nbn.add_edges(X, scoring_function=(\"MI\", ))\nbn.fit_parameters(clean_data)\n</code></pre>"},{"location":"examples/custom_estimators/#applybn","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 <code>applybn</code>:","text":"<pre><code>estimator = BNEstimator(use_mixture=False, has_logit=True,\n                        learning_params={\"scoring_function\": (\"MI\", )})\n&lt;...&gt;\nestimator.fit(fit_package)\n</code></pre>"},{"location":"examples/custom_estimators/#partialstructure","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b (<code>partial=\"structure\"</code>)","text":""},{"location":"examples/custom_estimators/#_7","title":"\u041d\u0438\u0437\u043a\u043e\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434:","text":"<pre><code>bn.add_nodes(descriptor)\nbn.add_edges(X)\n</code></pre>"},{"location":"examples/custom_estimators/#applybn_1","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 <code>applybn</code>:","text":"<pre><code>estimator = BNEstimator(partial=\"structure\")\n&lt;...&gt;\nestimator.fit(fit_package)\n</code></pre>"},{"location":"examples/custom_estimators/#partialparameters","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 (<code>partial=\"parameters\"</code>)","text":"<p>Note</p> <p>\u042d\u0442\u043e\u0442 \u043c\u0435\u0442\u043e\u0434 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b. \u0415\u0441\u043b\u0438 <code>estimator.bn_</code> \u043d\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d \u0438\u043b\u0438 <code>estimator.bn_.edges</code> \u043f\u0443\u0441\u0442, \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0437\u0432\u0430\u043d\u043e \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 <code>NotFittedError</code>.</p>"},{"location":"examples/custom_estimators/#_8","title":"\u041d\u0438\u0437\u043a\u043e\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434:","text":"<pre><code>bn.fit_parameters(clean_data)\n</code></pre>"},{"location":"examples/custom_estimators/#applybn_2","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 <code>applybn</code>:","text":"<pre><code>estimator = BNEstimator(partial=\"parameters\")\n&lt;...&gt;\nestimator.fit(fit_package)\n</code></pre>"},{"location":"examples/custom_estimators/#_9","title":"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u043e\u0432","text":"<p>\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u0430\u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u043d\u0430\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f: <pre><code>from applybn.core.estimators.estimator_factory import EstimatorPipelineFactory\nimport inspect\n\nfactory = EstimatorPipelineFactory(task_type=\"classification\")\nestimator_with_default_interface = factory.estimator.__class__\n\nclass StaticEstimator(estimator_with_default_interface):\n    def __init__(self):\n        pass\n\nmy_estimator = StaticEstimator()\nprint(*inspect.getmembers(my_estimator), sep=\"\\n\")  # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432\n</code></pre></p>"},{"location":"examples/custom_estimators/#_10","title":"\u0421\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f \u0434\u0435\u043b\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f","text":"<p>\u0415\u0441\u043b\u0438 \u043c\u0435\u0442\u043e\u0434 \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u0435\u043d <code>BNEstimator</code>, \u043e\u043d \u0431\u0443\u0434\u0435\u0442 \u0434\u0435\u043b\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d <code>self.bn_</code>. \u0415\u0441\u043b\u0438 <code>bn_</code> \u043d\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d, \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0437\u0432\u0430\u043d\u043e \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 <code>NotFittedError</code> \u0438\u043b\u0438 <code>AttributeError</code>.</p>"},{"location":"examples/custom_estimators/#_11","title":"\u041a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u044b","text":"<p>\u041a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u044b \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u044e\u0442 <code>bamt_preprocessor</code> \u0438 <code>BNEstimator</code>. \u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440 \u043b\u044e\u0431\u044b\u043c \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u043e\u043c, \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u044b\u043c \u0441 <code>scikit-learn</code>.</p> <p>\u041a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u044b \u0441\u043b\u0435\u0434\u0443\u044e\u0442 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u0443 \u0444\u0430\u0431\u0440\u0438\u043a\u0438, \u0447\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u043b\u044e\u0431\u043e\u0439 \u0432\u044b\u0437\u043e\u0432 <code>getattr</code> \u0434\u0435\u043b\u0435\u0433\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u043c\u0443 \u0448\u0430\u0433\u0443 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430 (\u043e\u0431\u044b\u0447\u043d\u043e <code>BNEstimator</code>).</p>"},{"location":"examples/custom_estimators/#_12","title":"\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0444\u0430\u0431\u0440\u0438\u043a\u0438","text":"<p>\u0423\u043a\u0430\u0436\u0438\u0442\u0435 \u0442\u0438\u043f \u0437\u0430\u0434\u0430\u0447\u0438 (\u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0438\u043b\u0438 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f) \u043f\u0440\u0438 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438: <pre><code>interfaces = {\"classification\": ClassifierMixin,\n              \"regression\": RegressorMixin}\n</code></pre> <pre><code>import pandas as pd\nfrom applybn.core.estimators.estimator_factory import EstimatorPipelineFactory\n\nX = pd.read_csv(DATA_PATH)\n# y = X.pop(\"anomaly\")  # \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439, \u0435\u0441\u043b\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\n\nfactory = EstimatorPipelineFactory(task_type=\"classification\")\n</code></pre></p>"},{"location":"examples/custom_estimators/#_13","title":"\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430","text":"<pre><code># \u041a\u043e\u043d\u0432\u0435\u0439\u0435\u0440 \u0441 \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u043c \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\npipeline = factory()\n# \u041a\u043e\u043d\u0432\u0435\u0439\u0435\u0440 \u0441 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u043c \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u043c\n# pipeline = factory(preprocessor)\n\npipeline.fit(X)\n</code></pre>"},{"location":"examples/custom_estimators/#_14","title":"\u0423\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u0430\u043c\u0438 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430","text":"<p>\u0427\u0442\u043e\u0431\u044b \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443, \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043e\u0447\u043d\u044b\u0435 \u0448\u0430\u0433\u0438, \u0430 \u0437\u0430\u0442\u0435\u043c \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: <pre><code># \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 MI\npipeline = factory(partial=\"structure\", learning_params={\"scoring_function\": \"MI\"})\npipeline.fit(X)\n\n# \u041f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043e\u0447\u043d\u044b\u0435 \u0448\u0430\u0433\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438\n&lt;...&gt;\n\n# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\npipeline.set_params(bn_estimator__partial=\"parameters\")\npipeline.fit(X)\n\nprint(pipeline.bn_.edges)\n</code></pre></p>"},{"location":"examples/custom_estimators/#_15","title":"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","text":"<p>\u041a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b \u0444\u0430\u0431\u0440\u0438\u043a\u0438 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c: <pre><code>CorePipeline([(\"preprocessor\", wrapped_preprocessor),\n              (\"bn_estimator\", estimator)])\n</code></pre> \u0427\u0442\u043e\u0431\u044b \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u044b \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430: <pre><code>pipeline.set_params(preprocessor__attrName=value)\n</code></pre></p> <p>Tip</p> <p>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u044b \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u043b\u0438 \u043f\u043e\u0441\u043b\u0435 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430.</p>"},{"location":"examples/custom_estimators/#_16","title":"\u0421\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f \u0434\u0435\u043b\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f","text":"<p>\u041c\u0435\u0442\u043e\u0434\u044b \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430 \u0434\u0435\u043b\u0435\u0433\u0438\u0440\u0443\u044e\u0442 \u0432\u044b\u0437\u043e\u0432\u044b \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u043c\u0443 \u0448\u0430\u0433\u0443 (<code>BNEstimator</code>). <pre><code>factory = EstimatorPipelineFactory(task_type=\"classification\")\npipeline = factory()\npipeline.fit(X)\n\npipeline.get_info(as_df=False)\npipeline.save(\"mybn\")\n</code></pre></p>"},{"location":"examples/custom_estimators/#_17","title":"\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432","text":"<p>\u0427\u0442\u043e\u0431\u044b \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439 \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440, \u0443\u0431\u0435\u0434\u0438\u0442\u0435\u0441\u044c, \u0447\u0442\u043e \u043e\u043d \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c \u0441 <code>scikit-learn</code>, \u0443\u043d\u0430\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0432 \u043e\u0442 <code>BaseEstimator</code> \u0438 <code>TransformerMixin</code>. <pre><code>from sklearn.base import BaseEstimator, TransformerMixin\n\nclass PreprocessorWrapper(BaseEstimator, TransformerMixin):\n       &lt;...&gt;\n       def transform(self, X):\n            df = do_smth(X)\n            return df\n</code></pre></p>"},{"location":"examples/feature_extraction/","title":"BN-based vs Polynomial & Interaction Features","text":""},{"location":"examples/feature_extraction/#_1","title":"\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437: \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439 \u0438 \u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c <code>BNFeatureGenerator</code> \u0443\u043b\u0443\u0447\u0448\u0430\u0442\u044c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 \u043c\u0435\u0442\u043e\u0434\u0430\u043c\u0438 \u0438\u043d\u0436\u0438\u043d\u0438\u0440\u0438\u043d\u0433\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0442\u0430\u043a\u0438\u043c\u0438 \u043a\u0430\u043a \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</p>"},{"location":"examples/feature_extraction/#1","title":"\u0428\u0430\u0433 1: \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<pre><code># \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 Wilt \u0438\u0437 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0430\u0439\u043b\u043e\u0432\ntraining_data = pd.read_csv('data/feature_extraction/wilt/training.csv')\ntesting_data = pd.read_csv('data/feature_extraction/wilt/testing.csv')\n\n# \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\ncombined_data = pd.concat([training_data, testing_data], ignore_index=True)\n\n# \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0446\u0435\u043b\u0438\nX = combined_data.drop(\"class\", axis=1) \ny = combined_data[\"class\"]\n\n# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438\npreprocessor = ColumnTransformer(\n    transformers=[('scale', StandardScaler(), list(X.columns))],\n    remainder='passthrough'\n)\npreprocessor.fit(X)\n</code></pre>"},{"location":"examples/feature_extraction/#2","title":"\u0428\u0430\u0433 2: \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","text":""},{"location":"examples/feature_extraction/#21","title":"2.1: \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438","text":"<pre><code>def generate_original_features(X_train, X_test, preprocessor):\n    \"\"\"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c.\"\"\"\n    X_train_scaled = pd.DataFrame(\n        preprocessor.transform(X_train),\n        columns=X_train.columns\n    )\n    X_test_scaled = pd.DataFrame(\n        preprocessor.transform(X_test),\n        columns=X_test.columns\n    )\n    return X_train_scaled, X_test_scaled\n</code></pre>"},{"location":"examples/feature_extraction/#22","title":"2.2: \u041f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438","text":"<pre><code>def generate_polynomial_features(X_train, X_test, preprocessor):\n    \"\"\"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0441\u0442\u0435\u043f\u0435\u043d\u044c 2).\"\"\"\n    # \u041f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\n    X_train_scaled, X_test_scaled = generate_original_features(X_train, X_test, preprocessor)\n\n    # \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    poly = PolynomialFeatures(degree=2, include_bias=False)\n    poly.fit(X_train_scaled)\n\n    X_train_poly = poly.transform(X_train_scaled)\n    X_test_poly = poly.transform(X_test_scaled)\n\n    feature_names = poly.get_feature_names_out(X_train_scaled.columns)\n    X_train_features = pd.DataFrame(X_train_poly, columns=feature_names)\n    X_test_features = pd.DataFrame(X_test_poly, columns=feature_names)\n\n    return X_train_features, X_test_features\n</code></pre>"},{"location":"examples/feature_extraction/#23","title":"2.3: \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f","text":"<pre><code>def generate_interaction_features(X_train, X_test, preprocessor):\n    \"\"\"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f (\u043f\u043e\u043f\u0430\u0440\u043d\u043e\u0435 \u0443\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u0435).\"\"\"\n    # \u041f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\n    X_train_scaled, X_test_scaled = generate_original_features(X_train, X_test, preprocessor)\n\n    # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043a\u043e\u043f\u0438\u0439 \u0434\u043b\u044f \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\n    X_train_interact = X_train_scaled.copy()\n    X_test_interact = X_test_scaled.copy()\n\n    # \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043f\u0430\u0440\u043d\u044b\u0445 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439\n    features = X_train_scaled.columns\n    for i, feat1 in enumerate(features):\n        for j, feat2 in enumerate(features):\n            if i &lt; j:  # \u0418\u0437\u0431\u0435\u0433\u0430\u043d\u0438\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0438 \u0441\u0430\u043c\u043e\u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439\n                X_train_interact[f'interaction_{feat1}_{feat2}'] = X_train_scaled[feat1] * X_train_scaled[feat2]\n                X_test_interact[f'interaction_{feat1}_{feat2}'] = X_test_scaled[feat1] * X_test_scaled[feat2]\n\n    return X_train_interact, X_test_interact\n</code></pre>"},{"location":"examples/feature_extraction/#24","title":"2.4: \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0437 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438","text":"<pre><code>def generate_bayesian_network_features(X_train, X_test, y_train):\n    \"\"\"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438.\"\"\"\n    # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0430 BN\n    bn_generator = BNFeatureGenerator()\n    bn_generator.fit(X=X_train, y=y_train)\n\n    # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n    X_train_bn = bn_generator.transform(X_train).reset_index(drop=True)\n    X_test_bn = bn_generator.transform(X_test).reset_index(drop=True)\n\n    # \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    encoders = {}\n    for col in X_train_bn.columns:\n        if X_train_bn[col].dtype == 'object':\n            encoders[col] = LabelEncoder()\n            encoders[col].fit(X_train_bn[col].unique()) \n            X_train_bn[col] = encoders[col].transform(X_train_bn[col])\n\n    for col in X_test_bn.columns:\n        if col in encoders and X_test_bn[col].dtype == 'object':\n            X_test_bn[col] = encoders[col].transform(X_test_bn[col])\n\n    return X_train_bn, X_test_bn\n</code></pre>"},{"location":"examples/feature_extraction/#3","title":"\u0428\u0430\u0433 3: \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043e\u0446\u0435\u043d\u043a\u0438","text":"<pre><code># \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f\nmodels = {\n    'Decision Tree': (DecisionTreeClassifier, {'random_state': 42}),\n    'Logistic Regression': (LogisticRegression, {'random_state': 42, 'max_iter': 1000}),\n    'SVC': (SVC, {'random_state': 42})\n}\n\n# \u041c\u0435\u0442\u043e\u0434\u044b \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nfeature_generators = {\n    'Original': generate_original_features,\n    'Polynomial': generate_polynomial_features,\n    'Interaction': generate_interaction_features,\n    'Bayesian Network': generate_bayesian_network_features\n}\n\n# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043e\u0446\u0435\u043d\u043a\u0438\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# \u041a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440 \u0434\u043b\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\nall_results = {}\n</code></pre>"},{"location":"examples/feature_extraction/#4-","title":"\u0428\u0430\u0433 4: \u041a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0438 \u043e\u0446\u0435\u043d\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439","text":"<pre><code># \u041e\u0446\u0435\u043d\u043a\u0430 \u0432\u0441\u0435\u0445 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0442\u0438\u043f\u043e\u0432 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nfor model_name, (model_class, model_params) in models.items():\n    all_results[model_name] = {}\n\n    for feature_type, feature_generator in feature_generators.items():\n        accuracy_scores = []\n        f1_scores = []\n        feature_importance = None\n\n        # \u041a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n        for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\n            # \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n            if feature_type == 'Bayesian Network':\n                X_train_features, X_test_features = feature_generator(X_train, X_test, y_train)\n            else:\n                X_train_features, X_test_features = feature_generator(X_train, X_test, preprocessor)\n\n            # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\n            model = model_class(**model_params)\n            model.fit(X_train_features, y_train)\n\n            # \u041e\u0446\u0435\u043d\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\n            y_pred = model.predict(X_test_features)\n            accuracy_scores.append(accuracy_score(y_test, y_pred))\n            f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n\n            # \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0444\u043e\u043b\u0434\u0430\n            if fold_idx == 0 and hasattr(model, 'feature_importances_'):\n                feature_importance = pd.Series(\n                    model.feature_importances_,\n                    index=X_train_features.columns\n                )\n\n        # \u0420\u0430\u0441\u0447\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u043c\u0435\u0442\u0440\u0438\u043a\n        all_results[model_name][feature_type] = {\n            'accuracy': np.mean(accuracy_scores),\n            'accuracy_std': np.std(accuracy_scores),\n            'f1': np.mean(f1_scores),\n            'f1_std': np.std(f1_scores)\n        }\n\n        if feature_importance is not None:\n            all_results[model_name][feature_type]['importance'] = feature_importance\n\n# \u0412\u044b\u0432\u043e\u0434 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0433\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\nprint(\"\\n\" + \"=\"*80)\nprint(\"\u041f\u041e\u0414\u0420\u041e\u0411\u041d\u041e\u0415 \u0421\u0420\u0410\u0412\u041d\u0415\u041d\u0418\u0415 \u041f\u0420\u041e\u0418\u0417\u0412\u041e\u0414\u0418\u0422\u0415\u041b\u042c\u041d\u041e\u0421\u0422\u0418\")\nprint(\"=\"*80)\n\nfor feature_name in feature_generators.keys():\n    print(f\"\\n\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 {feature_name}:\")\n    print(\"-\" * 60)\n\n    for model_name in models:\n        result = all_results[model_name][feature_name]\n        print(f\"\\n{model_name}:\")\n        print(f\"  \u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c: {result['accuracy']:.4f} (\u00b1{result['accuracy_std']:.4f})\")\n        print(f\"  F1-\u043e\u0446\u0435\u043d\u043a\u0430: {result['f1']:.4f} (\u00b1{result['f1_std']:.4f})\")\n</code></pre>"},{"location":"examples/feature_extraction/#5","title":"\u0428\u0430\u0433 5: \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0430\u043d\u0430\u043b\u0438\u0437","text":"<pre><code># \u041f\u043e\u0438\u0441\u043a \u043b\u0443\u0447\u0448\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u0438\u043f\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nprint(\"\\n\\n\" + \"=\"*80)\nprint(\"\u0421\u0412\u041e\u0414\u041a\u0410 \u041b\u0423\u0427\u0428\u0418\u0425 \u041c\u041e\u0414\u0415\u041b\u0415\u0419\")\nprint(\"=\"*80)\n\nfor feature_name in feature_generators.keys():\n    best_model = max(models.keys(), \n                    key=lambda model: all_results[model][feature_name]['accuracy'])\n\n    print(f\"\\n\u041b\u0443\u0447\u0448\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 {feature_name}: {best_model}\")\n    print(f\"  \u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c: {all_results[best_model][feature_name]['accuracy']:.4f} (\u00b1{all_results[best_model][feature_name]['accuracy_std']:.4f})\")\n    print(f\"  F1-\u043e\u0446\u0435\u043d\u043a\u0430: {all_results[best_model][feature_name]['f1']:.4f} (\u00b1{all_results[best_model][feature_name]['f1_std']:.4f})\")\n\n# \u041f\u043e\u0438\u0441\u043a \u043b\u0443\u0447\u0448\u0435\u0439 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nbest_feature = None\nbest_model = None\nbest_accuracy = 0\n\nfor model_name in models:\n    for feature_name in feature_generators.keys():\n        acc = all_results[model_name][feature_name]['accuracy']\n        if acc &gt; best_accuracy:\n            best_accuracy = acc\n            best_model = model_name\n            best_feature = feature_name\n\n# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u044b \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\naccuracies = []\nf1_scores = []\nmodel_names = []\nfeature_types = []\n\nfor model_name in models:\n    for feature_type in feature_generators.keys():\n        result = all_results[model_name][feature_type]\n        accuracies.append(result['accuracy'])\n        f1_scores.append(result['f1'])\n        model_names.append(model_name)\n        feature_types.append(feature_type)\n\nresults_df = pd.DataFrame({\n    'Model': model_names,\n    'Feature Type': feature_types,\n    'Accuracy': accuracies,\n    'F1 Score': f1_scores\n})\n\nplt.figure(figsize=(12, 8))\ng = sns.barplot(x='Feature Type', y='Accuracy', hue='Model', data=results_df)\nplt.title('Model Accuracy by Feature Generation Method')\nplt.xlabel('Feature Generation Method')\nplt.ylabel('Accuracy (5-fold CV)')\nplt.ylim(0.9, 1.0)  \nplt.xticks(rotation=45)\nplt.legend(title='Model')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/feature_extraction/#example-results","title":"Example Results","text":"<p>When running this code on the Wilt dataset, the Bayesian Network features consistently outperform traditional feature engineering methods, particularly when combined with SVC:</p> <p><pre><code>================================================================================\nBEST MODELS SUMMARY\n================================================================================\n\nBest model with Original features: SVC\n  Accuracy: 0.9806 (\u00b10.0047)\n  F1 Score: 0.8834 (\u00b10.0390)\n\nBest model with Polynomial features: Logistic Regression\n  Accuracy: 0.9795 (\u00b10.0052)\n  F1 Score: 0.8798 (\u00b10.0349)\n\nBest model with Interaction features: Decision Tree\n  Accuracy: 0.9760 (\u00b10.0024)\n  F1 Score: 0.8822 (\u00b10.0170)\n\nBest model with Bayesian Network features: SVC\n  Accuracy: 0.9833 (\u00b10.0016)\n  F1 Score: 0.9168 (\u00b10.0124)\n</code></pre> </p> <p><pre><code># Visualize feature importance for the best model\nprint(\"\\n\\n\" + \"=\"*80)\nprint(\"FEATURE IMPORTANCE VISUALIZATION\")\nprint(\"=\"*80)\n\nprint(f\"\\nBest overall model: {best_model} with {best_feature} features\")\nprint(f\"Accuracy: {all_results[best_model][best_feature]['accuracy']:.4f} (\u00b1{all_results[best_model][best_feature]['accuracy_std']:.4f})\")\n\nif 'importance' in all_results[best_model][best_feature]:\n    importance = all_results[best_model][best_feature]['importance']\n    top_n = min(20, len(importance))\n\n    plt.figure(figsize=(12, 8))\n    top_importance = importance.sort_values(ascending=False).head(top_n)\n    sns.barplot(x=top_importance.values, y=top_importance.index)\n    plt.title(f'Top {top_n} Feature Importance - {best_model} with {best_feature} features')\n    plt.xlabel('Importance')\n    plt.ylabel('Feature')\n    plt.tight_layout()\n    plt.show()\n</code></pre> </p>"},{"location":"examples/feature_extraction/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the effectiveness of Bayesian Network feature generation compared to traditional feature engineering methods. The BNFeatureGenerator captures complex dependencies in the data that are not easily represented by polynomial or interaction features. </p>"},{"location":"examples/nmi_feature_selection/","title":"Feature Selection with NMIFeatureSelector","text":""},{"location":"examples/nmi_feature_selection/#nmifeatureselector","title":"\u041e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e NMIFeatureSelector \u043d\u0430 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0438\u043d\u0430","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c <code>NMIFeatureSelector</code> \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u043e\u0442\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u0442\u044c \u0448\u0443\u043c, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0438\u043d\u0430 \u0438\u0437 UCI. \u041c\u044b: - \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0435\u0433\u043e. - \u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0448\u0443\u043c\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0438\u043c\u0438\u0442\u0430\u0446\u0438\u0438 \u043d\u0435\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u043e\u0432. - \u0421\u0440\u0430\u0432\u043d\u0438\u043c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u043e\u0442\u0431\u043e\u0440\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0431\u0435\u0437 \u043d\u0435\u0433\u043e \u043f\u043e \u043c\u0435\u0440\u0435 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0448\u0443\u043c\u0430. - \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b.</p>"},{"location":"examples/nmi_feature_selection/#1","title":"\u0428\u0430\u0433 1: \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom bn_nmi_feature_selector import NMIFeatureSelector\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0438\u043d\u0430 (\u043a\u0440\u0430\u0441\u043d\u043e\u0435 \u0432\u0438\u043d\u043e)\ndata = pd.read_csv('data/feature_selection/WineQT.csv')\ndata = data.drop(columns=['Id'])\nX = data.drop(\"quality\", axis=1)\ny = data[\"quality\"].values\n</code></pre>"},{"location":"examples/nmi_feature_selection/#2","title":"\u0428\u0430\u0433 2: \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0448\u0443\u043c\u0430 \u0438 \u043e\u0446\u0435\u043d\u043a\u0438","text":"<pre><code>def evaluate_performance(X, y, noise_features=0, threshold=0.05):\n    # \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u0448\u0443\u043c\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    np.random.seed(42)\n    noise = np.random.randn(X.shape[0], noise_features)\n    X_combined = np.hstack([X.values, noise])\n\n    # \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_combined, y, test_size=0.3, random_state=42\n    )\n\n    # \u0421 \u043e\u0442\u0431\u043e\u0440\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    selector = NMIFeatureSelector(threshold=threshold, n_bins=10)\n    X_train_selected = selector.fit_transform(X_train, y_train)\n    X_test_selected = selector.transform(X_test)\n\n    # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_train_selected, y_train)\n    y_pred = model.predict(X_test_selected)\n    acc_selected = accuracy_score(y_test, y_pred)\n\n    # \u0411\u0435\u0437 \u043e\u0442\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc_full = accuracy_score(y_test, y_pred)\n\n    return acc_full, acc_selected, selector.selected_features_.shape[0]\n</code></pre>"},{"location":"examples/nmi_feature_selection/#3","title":"\u0428\u0430\u0433 3: \u0417\u0430\u043f\u0443\u0441\u043a \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430 \u0441 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435\u043c \u0448\u0443\u043c\u0430","text":"<pre><code>noise_levels = np.arange(0, 50, 10)  # \u0422\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u043e\u0442 0 \u0434\u043e 40 \u0448\u0443\u043c\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nresults = []\n\nfor noise in noise_levels:\n    acc_full, acc_selected, n_selected = evaluate_performance(X, y, noise_features=noise)\n    results.append({\n        \"Noise Features\": noise,\n        \"Accuracy (Full)\": acc_full,\n        \"Accuracy (Selected)\": acc_selected,\n        \"Selected Features\": n_selected\n    })\n    print(f\"Noise: {noise:2d} | Full Acc: {acc_full:.3f} | Selected Acc: {acc_selected:.3f} | Features Kept: {n_selected}\")\n\nresults_df = pd.DataFrame(results)\n</code></pre>"},{"location":"examples/nmi_feature_selection/#4","title":"\u0428\u0430\u0433 4: \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","text":"<p><pre><code>plt.figure(figsize=(10, 6))\nplt.plot(results_df[\"Noise Features\"], results_df[\"Accuracy (Full)\"], \n         marker=\"o\", label=\"Without Selection\", color=\"red\")\nplt.plot(results_df[\"Noise Features\"], results_df[\"Accuracy (Selected)\"], \n         marker=\"o\", label=\"With Selection\", color=\"green\")\nplt.xlabel(\"Number of Noise Features Added\")\nplt.ylabel(\"Model Accuracy\")\nplt.title(\"Impact of Noise Features on Model Performance\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> </p>"},{"location":"examples/nmi_feature_selection/#_1","title":"\u0417\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435","text":"<p><code>NMIFeatureSelector</code> \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e: - \u0418\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u0435\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445. - \u0424\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u0442 \u0448\u0443\u043c, \u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0449\u0430\u044f \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438. - \u0420\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043d\u0430\u0434\u0435\u0436\u043d\u043e, \u0434\u0430\u0436\u0435 \u043a\u043e\u0433\u0434\u0430 80% \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043d\u0435\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u043c\u0438 (40 \u0448\u0443\u043c\u043e\u0432\u044b\u0445 + 11 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445).</p> <p>\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044f: \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u0442\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>threshold</code> \u0434\u043b\u044f \u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435\u043c \u0448\u0443\u043c\u0430. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u043f\u043e\u0440\u043e\u0433 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, 0.1) \u043c\u043e\u0436\u0435\u0442 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c \u0448\u0443\u043c, \u043d\u043e \u0440\u0438\u0441\u043a\u0443\u0435\u0442 \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0441\u043b\u0430\u0431\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438.</p>"},{"location":"examples/tabular_anomaly_detection/","title":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442, \u043a\u0430\u043a \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432, \u0441 \u043e\u0446\u0435\u043d\u043a\u043e\u0439 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043a\u0430\u043a \u043e\u0442\u0447\u0435\u0442\u043e\u0432 \u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438, \u0442\u0430\u043a \u0438 \u043c\u0435\u0442\u0440\u0438\u043a ROC-AUC.</p> <p>\u0412 \u043e\u0431\u0449\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e\u043c\u0443 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u0443. \u041d\u043e \u0438\u043d\u043e\u0433\u0434\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043c\u043e\u0436\u0435\u0442 \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c\u0441\u044f \u0438\u0437-\u0437\u0430 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438.</p>"},{"location":"examples/tabular_anomaly_detection/#_2","title":"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430","text":"<pre><code>from tabulate import tabulate\n\ndef print_df(df):\n    print(tabulate(df, tablefmt=\"github\", headers=\"keys\", showindex=\"always\"))\n\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom applybn.anomaly_detection.static_anomaly_detector.tabular_detector import TabularDetector\n</code></pre>"},{"location":"examples/tabular_anomaly_detection/#1","title":"\u0428\u0430\u0433 1. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445","text":"<pre><code>data = pd.read_csv(\"../../applybn/anomaly_detection/data/tabular/ecoli.csv\")\nX = data.drop(columns=['y'])\ny = data['y']\n\n# \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438\npreprocessors = {\n    'No preprocessing': None,\n    'StandardScaler': StandardScaler(),\n    'RobustScaler': RobustScaler(),\n    'PowerTransformer': PowerTransformer(method='yeo-johnson')\n}\n\n# \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432\ndetectors = {\n    'IQR-based': TabularDetector(target_name='y', model_estimation_method='iqr'),\n    'Isolation Forest': TabularDetector(target_name='y', additional_score='IF'),\n    'Local Outlier Factor': TabularDetector(target_name='y', additional_score='LOF')\n}\n</code></pre>"},{"location":"examples/tabular_anomaly_detection/#2","title":"\u0428\u0430\u0433 2. \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0438","text":"<p>\u041d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u0442\u0440\u0430\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044e, \u0447\u0442\u043e\u0431\u044b \u0438\u0437\u0431\u0435\u0436\u0430\u0442\u044c \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u0432\u044b\u0431\u043e\u0440\u043e\u043a!</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"examples/tabular_anomaly_detection/#3","title":"\u0428\u0430\u0433 3. \u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435","text":"<pre><code>results = []\n\nfor preproc_name, preprocessor in preprocessors.items():\n    if preprocessor:\n        X_train_processed = preprocessor.fit_transform(X_train)\n        X_test_processed = preprocessor.transform(X_test)\n    else:\n        X_train_processed = X_train.values\n        X_test_processed = X_test.values\n\n    # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u043e\u0432 \u0434\u043b\u044f \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u0430\n    train_df = pd.DataFrame(X_train_processed, columns=X.columns)\n    train_df['y'] = y_train.values\n    test_df = pd.DataFrame(X_test_processed, columns=X.columns)\n    test_df['y'] = y_test.values\n\n    for det_name, detector in detectors.items():\n        print(f\"\\n=== {preproc_name} + {det_name} ===\")\n\n        # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n        detector.fit(train_df)\n        try:\n            preds = detector.predict(test_df)\n        except ValueError:\n            detector.y_ = test_df['y'].values\n            preds = detector.predict(test_df)\n\n        scores = detector.decision_function(test_df) if hasattr(detector, 'decision_function') else None\n\n        # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u0442\u0447\u0435\u0442\u0430 \u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\n        report = classification_report(y_test, preds, output_dict=True)\n        f1 = report['1']['f1-score']\n\n        # \u0420\u0430\u0441\u0447\u0435\u0442 ROC-AUC, \u0435\u0441\u043b\u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b \u043e\u0446\u0435\u043d\u043a\u0438\n\n        roc_auc = roc_auc_score(y_test, scores)\n\n        # \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n        results.append({\n            'Preprocessor': preproc_name,\n            'Detector': det_name,\n            'F1': f1,\n            'ROC-AUC': roc_auc\n        })\n</code></pre>"},{"location":"examples/tabular_anomaly_detection/#_3","title":"\u041e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","text":"<pre><code>results_df = pd.DataFrame(results)\n\nprint_df(results_df.pivot(index='Detector', columns='Preprocessor', values=['F1', 'ROC-AUC']))\n</code></pre> Detector ('F1', 'No preprocessing') ('F1', 'PowerTransformer') ('F1', 'RobustScaler') ('F1', 'StandardScaler') ('ROC-AUC', 'No preprocessing') ('ROC-AUC', 'PowerTransformer') ('ROC-AUC', 'RobustScaler') ('ROC-AUC', 'StandardScaler') IQR-based 0.5 0.0645161 0.0952381 0.8 1 0.969388 0.748299 0.727891 Isolation Forest 0 0 0 0.4 0.87415 0.945578 0.94898 0.972789 Local Outlier Factor 0.666667 0.0631579 0.08 0.666667 1 0.418367 0.64966 0.989796"},{"location":"examples/time_series_anomaly_detection/","title":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445","text":""},{"location":"examples/time_series_anomaly_detection/#_2","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u0442\u0449\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u043c\u0435\u0436\u0434\u0443 \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c\u044e \u0438 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u043d\u043e\u0441\u0442\u044c\u044e, \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u043f\u0440\u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439 \u0434\u043e\u0432\u0435\u0440\u0438\u044f (tDBN). \u042d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u044d\u0442\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0442\u043e\u0447\u043d\u043e\u0439 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432:</p> <p>\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b:</p> <ul> <li>num_parents: \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u0442 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u0435\u0442\u0438 (\u043e\u0431\u044b\u0447\u043d\u043e 1-5)</li> <li>markov_lag: \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u043e\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 (\u043e\u0431\u044b\u0447\u043d\u043e 1-3 \u0448\u0430\u0433\u0430) (\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442 \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439)</li> <li>Non_stationary: \u0437\u0430\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u044b <code>num_transition</code>. \u0415\u0441\u043b\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u044b\u0439, \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0430.</li> </ul> <p>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438:</p> <ul> <li>artificial_slicing: \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u043a\u043e\u043d</li> <li>artificial_slicing_params: \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043e\u043a\u043d\u0430 \u0438 \u0448\u0430\u0433\u0430</li> </ul> <p>\u041d\u0430\u0448 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442, \u0447\u0442\u043e \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0434\u043e\u0441\u0442\u0438\u0433\u0430\u0435\u0442\u0441\u044f \u043d\u0435 \u0437\u0430 \u0441\u0447\u0435\u0442 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438, \u0430 \u0437\u0430 \u0441\u0447\u0435\u0442 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432. \u041e\u0441\u043e\u0431\u043e\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0431\u0443\u0434\u0435\u0442 \u0443\u0434\u0435\u043b\u0435\u043d\u043e \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u043d\u0430\u0440\u0435\u0437\u043a\u0435.</p> <p>\u041d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u044b\u043b \u0432\u0437\u044f\u0442 \u043e\u0442\u0441\u044e\u0434\u0430 \u0437\u0434\u0435\u0441\u044c.</p>"},{"location":"examples/time_series_anomaly_detection/#_3","title":"\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430","text":"<pre><code>import numpy as np\n\nfrom applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector import FastTimeSeriesDetector\nfrom applybn.anomaly_detection.dynamic_anomaly_detector.data_formatter import TemporalDBNTransformer\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom pyts.approximation import SymbolicAggregateApproximation # \u043d\u0435 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\nfrom sklearn.metrics import f1_score\n\nsns.set_theme()\n\nnp.random.seed(51)\n</code></pre>"},{"location":"examples/time_series_anomaly_detection/#1","title":"\u0428\u0430\u0433 1. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<pre><code>df = pd.read_csv(\"data/anomaly_detection/ts/ECG200.csv\")\ndf = df.loc[:, df.columns[1::3].tolist() + [\"anomaly\"]] # \u0431\u0435\u0440\u0435\u043c \u043a\u0430\u0436\u0434\u044b\u0439 3-\u0439 \u0448\u0430\u0433 \u0434\u043b\u044f \u0443\u0441\u043a\u043e\u0440\u0435\u043d\u0438\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439\nprint(df.shape) # (200, 33)\n</code></pre>"},{"location":"examples/time_series_anomaly_detection/#2","title":"\u0428\u0430\u0433 2. \u0418\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u043d\u0430\u0440\u0435\u0437\u043a\u0430","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0440\u043e\u043b\u044c \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u0445 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u043b\u044f \u043d\u0430\u0440\u0435\u0437\u043a\u0438.</p> <p><pre><code>y = df.pop(\"anomaly\")\n\nfracs_normal = []\nfracs_anomaly = []\nx = np.linspace(1, 20, 20).astype(int)\npbar = tqdm(x)\n\nfor i in pbar:\n    pbar.set_description(f\"Processing {i}\")\n    transformer = TemporalDBNTransformer(window=i, stride=i)\n    df_ = transformer.transform(df, y)\n\n    try:\n        non_anom_frac = df_[\"anomaly\"].value_counts(normalize=True)[0]\n    except KeyError:\n        non_anom_frac = 0\n\n    fracs_normal.append(non_anom_frac)\n    fracs_anomaly.append(1 - non_anom_frac)\n\nfinal_df = pd.DataFrame({\"normal\": fracs_normal,\n                         \"anomaly\": fracs_anomaly},\n                        index=x)\n\nax = sns.lineplot(data=final_df)\nax.set(title=\"Stride=Window size\", xlabel='Window size', ylabel='Fraction')\nax.set_xticks(x)\nplt.show()\n</code></pre> </p> <p>\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0447\u0435\u043d\u044c \u0432\u0430\u0436\u043d\u043e \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0431\u0430\u043b\u0430\u043d\u0441 \u0432 \u0446\u0435\u043b\u0435\u0432\u043e\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0435.</p>"},{"location":"examples/time_series_anomaly_detection/#3","title":"\u0428\u0430\u0433 3. \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<pre><code>transformer = TemporalDBNTransformer(window=5, stride=1)\ndf_ = transformer.transform(df, y)\n\ny = df_.pop(\"anomaly\")\n</code></pre>"},{"location":"examples/time_series_anomaly_detection/#4-sax","title":"\u0428\u0430\u0433 4. SAX","text":"<pre><code>transformer = SymbolicAggregateApproximation()\nsax_vals = transformer.transform(df_.iloc[:, 1:])\ndf_ = df_.astype(str)\n\ndf_.iloc[:, 1:] = sax_vals\n</code></pre>"},{"location":"examples/time_series_anomaly_detection/#5","title":"\u0428\u0430\u0433 5. \u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435","text":"<pre><code>detector = FastTimeSeriesDetector(markov_lag=1, num_parents=1)\n\ndetector.fit(df_)\ndetector.calibrate(y)\npreds_cont = detector.predict(df_)\n\nprint(f1_score(y, preds_cont)) # 0.9171270718232044\n</code></pre>"},{"location":"examples/time_series_anomaly_detection/#_4","title":"\u0412\u0430\u0440\u0438\u0430\u0446\u0438\u0438","text":"<pre><code>for i in range(2, 6, 2):\n    df = pd.read_csv(\"data/anomaly_detection/ts/ECG200.csv\")\n    df = df.loc[:, df.columns[1::3].tolist() + [\"anomaly\"]]\n\n    y = df.pop(\"anomaly\")\n\n    transformer = TemporalDBNTransformer(window=i, stride=1)\n    df_ = transformer.transform(df, y)\n\n    y = df_.pop(\"anomaly\")\n\n    transformer = SymbolicAggregateApproximation()\n    sax_vals = transformer.transform(df_.iloc[:, 1:])\n    df_ = df_.astype(str)\n\n    df_.iloc[:, 1:] = sax_vals\n\n    detector = FastTimeSeriesDetector(markov_lag=1,\n                                      num_parents=1)\n    detector.fit(df_)\n    detector.calibrate(y, verbose=0)\n    preds_cont = detector.predict(df_)\n    print(i)\n    print(f1_score(y, preds_cont))\n    print(\"____\")\n</code></pre> <p>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <pre><code>Evaluating network with LL score.\n2\n0.6907894736842105\n____\nEvaluating network with LL score.\n4\n0.8774928774928775\n____\nEvaluating network with LL score.\n6\n0.9487870619946092\n____\n</code></pre></p>"},{"location":"getting-started/intro_bn/","title":"\u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 (\u0441\u0435\u0442\u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u044f)","text":"<p>\u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 (\u0442\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u043a\u0430\u043a \u0441\u0435\u0442\u0438 \u0411\u0430\u0439\u0435\u0441\u0430 \u0438\u043b\u0438 \u0441\u0435\u0442\u0438 \u0434\u043e\u0432\u0435\u0440\u0438\u044f) \u2014 \u044d\u0442\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0432\u043e\u043a\u0443\u043f\u043d\u043e\u0441\u0442\u044c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u0432\u0435\u043b\u0438\u0447\u0438\u043d \u0438 \u0438\u0445 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0445 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u0430\u0446\u0438\u043a\u043b\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0430 (DAG). \u0412 \u044d\u0442\u0438\u0445 \u0433\u0440\u0430\u0444\u0430\u0445 \u043a\u0430\u0436\u0434\u044b\u0439 \u0443\u0437\u0435\u043b \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0439 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u0435, \u0430 \u043a\u0430\u0436\u0434\u043e\u0435 \u0440\u0435\u0431\u0440\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043f\u0440\u044f\u043c\u0443\u044e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u0443\u044e \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c.</p> <p>\u042d\u0442\u0438 \u0441\u0435\u0442\u0438 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043a\u043e\u043c\u043f\u0430\u043a\u0442\u043d\u044b\u0439 \u0438 \u0438\u043d\u0442\u0443\u0438\u0442\u0438\u0432\u043d\u043e \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u0439 \u0441\u043f\u043e\u0441\u043e\u0431 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u044b\u0445 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439. \u0412\u043c\u0435\u0441\u0442\u043e \u044f\u0432\u043d\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f, \u0447\u0442\u043e \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u043c \u0441 \u0440\u043e\u0441\u0442\u043e\u043c \u0447\u0438\u0441\u043b\u0430 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445, \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u044e\u0442 \u0435\u0433\u043e, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0446\u0435\u043f\u043d\u043e\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438, \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0432 \u0433\u0440\u0430\u0444\u0435. \u042d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442 \u0438\u0445 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u043c\u0438 \u0434\u043b\u044f \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u0439 \u0432 \u0443\u0441\u043b\u043e\u0432\u0438\u044f\u0445 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438, \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0432\u044b\u0432\u043e\u0434\u0430 \u0438 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043d\u0435\u043f\u043e\u043b\u043d\u044b\u0445 \u0438\u043b\u0438 \u0437\u0430\u0448\u0443\u043c\u043b\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.</p> <p>\u0412 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438 \u043b\u0435\u0436\u0430\u0442 \u0434\u0432\u0430 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430:</p> <ol> <li>\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u2013 \u0441\u0430\u043c \u0433\u0440\u0430\u0444, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043c\u0435\u0436\u0434\u0443 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438.</li> <li>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u2013 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 (\u0423\u0420\u0412), \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 \u043a\u0430\u0436\u0434\u044b\u043c \u0443\u0437\u043b\u043e\u043c, \u043f\u0440\u0438 \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0433\u043e \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044f\u0445.</li> </ol> <p>\u0412 \u0442\u043e \u0432\u0440\u0435\u043c\u044f \u043a\u0430\u043a \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043d\u0430\u043c, \u043a\u0430\u043a\u0438\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0432\u043b\u0438\u044f\u044e\u0442 \u0434\u0440\u0443\u0433 \u043d\u0430 \u0434\u0440\u0443\u0433\u0430, \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u044e\u0442, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0438\u043b\u044c\u043d\u044b \u044d\u0442\u0438 \u0432\u043b\u0438\u044f\u043d\u0438\u044f. \u0412\u043c\u0435\u0441\u0442\u0435 \u043e\u043d\u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u044e\u0442 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c.</p>"},{"location":"getting-started/intro_bn/#_2","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u044f\u0445","text":"<p>\u041f\u0440\u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439 \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0434\u0432\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438:</p> <ul> <li> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435: \u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0433\u0440\u0430\u0444\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0442\u0440\u0430\u0436\u0430\u0435\u0442 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043c\u0435\u0436\u0434\u0443 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438. \u042d\u0442\u043e \u0447\u0430\u0441\u0442\u043e \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0442\u043e\u0440\u043d\u043e \u0441\u043b\u043e\u0436\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\u0439, \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0442\u0430\u043a \u043a\u0430\u043a \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u0432 \u0441\u0435\u0431\u044f \u043f\u043e\u0438\u0441\u043a \u043f\u043e \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0443 \u0432\u0441\u0435\u0445 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 DAG.</p> </li> <li> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c: \u041e\u0446\u0435\u043d\u043a\u0430 \u0423\u0420\u0412 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0443\u0437\u043b\u0430 \u043f\u0440\u0438 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435. \u0415\u0441\u043b\u0438 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u0430 \u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u043d\u044b, \u044d\u0442\u043e\u0442 \u0448\u0430\u0433 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0441\u0442 \u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u0438\u044f \u0438\u043b\u0438 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u0438.</p> </li> </ul> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u0430\u043a \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b, \u0442\u0430\u043a \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 \u0432 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445, \u0433\u0434\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b, \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0442\u0430\u043c\u0438, \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b. \u042d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442 \u0438\u0445 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0446\u0435\u043d\u043d\u044b\u043c\u0438 \u0432 \u0442\u0430\u043a\u0438\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445, \u043a\u0430\u043a \u0431\u0438\u043e\u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u043a\u0430, \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f \u0434\u0438\u0430\u0433\u043d\u043e\u0441\u0442\u0438\u043a\u0430 \u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439.</p> <p>Note</p> <p>\u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e <code>bamt</code> (\u0438 <code>applybn</code>) \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c K2 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u0430 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0438 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u0438\u0435 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432.</p>"},{"location":"user-guide/anomaly_detection_module/tDBN_data_formatter/","title":"\u0424\u043e\u0440\u043c\u0430\u0442\u0438\u0440\u043e\u0432\u0449\u0438\u043a \u0434\u0430\u043d\u043d\u044b\u0445 tDBN","text":""},{"location":"user-guide/anomaly_detection_module/tDBN_data_formatter/#_1","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u0414\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f <code>FastTimeSeriesDetector</code> \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0443\u0442\u0435\u043c \u0438\u0445 \u043d\u0430\u0440\u0435\u0437\u043a\u0438 \u043d\u0430 \u043e\u043a\u043d\u0430. \u041a\u0440\u0430\u0442\u043a\u043e \u044d\u0442\u043e\u0442 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043c\u043e\u0436\u043d\u043e \u043e\u043f\u0438\u0441\u0430\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u044d\u0442\u043e\u0433\u043e \u0440\u0438\u0441\u0443\u043d\u043a\u0430:</p> <p></p> <p>\u0422\u0435\u0445\u043d\u0438\u043a\u0430 \u0441\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0435\u0433\u043e \u043e\u043a\u043d\u0430. \u0418\u0441\u0442\u043e\u0447\u043d\u0438\u043a: \u0441\u0442\u0430\u0442\u044c\u044f</p> <p>Note</p> <p>\u0414\u043b\u0438\u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0435\u0433\u043e \u043e\u043a\u043d\u0430 - \u044d\u0442\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>window</code>, \u0430 <code>stride</code> - \u044d\u0442\u043e \u0448\u0430\u0433 \u0441\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0435\u0433\u043e \u043e\u043a\u043d\u0430.</p>"},{"location":"user-guide/anomaly_detection_module/tDBN_data_formatter/#_2","title":"\u0421\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0430\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u0438","text":"<p>\u041f\u043e\u0441\u043b\u0435 \u043d\u0430\u0440\u0435\u0437\u043a\u0438 \u043d\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u044f\u0441\u043d\u043e, \u043a\u0430\u043a \u0430\u0433\u0440\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0438 \u043f\u043e \u0441\u0443\u0431\u044a\u0435\u043a\u0442\u0430\u043c. \u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u0430 \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f \"any\", \u0442\u043e \u0435\u0441\u0442\u044c \u0430\u043d\u043e\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u0441\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f \u0441\u0443\u0431\u044a\u0435\u043a\u0442, \u0443 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0435\u0441\u0442\u044c \u0445\u043e\u0442\u044f \u0431\u044b 1 \u0430\u043d\u043e\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0448\u0430\u0433 \u0432\u043d\u0443\u0442\u0440\u0438.</p>"},{"location":"user-guide/anomaly_detection_module/tDBN_data_formatter/#_3","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435","text":"<p><code>TemporalDBNTransformer</code> \u0431\u044b\u043b \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d \u0434\u043b\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0442\u0430\u043a\u043e\u0439 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438:</p> <pre><code>import numpy as np\nfrom applybn.anomaly_detection.dynamic_anomaly_detector.data_formatter import TemporalDBNTransformer\nimport pandas as pd\n\nfrom tabulate import tabulate # \u043d\u0435 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\n\nnp.random.seed(51)\n\ndef print_df(df):\n    print(tabulate(df, tablefmt=\"github\", headers=\"keys\", showindex=\"always\"))\n\ndf = pd.DataFrame(\n    {\"col1\": np.linspace(0, 5, 10),\n     \"col2\": np.linspace(5, 10, 10),\n     \"anomaly\": np.random.choice([0, 1], p = [0.7, 0.3], size=10),}\n).astype(int)\n\nprint_df(df)\nprint(\"\\n\\n\")\nlabel = df.pop(\"anomaly\")\ntransformer = TemporalDBNTransformer(window=5, stride=2, include_label=True)\n\nprint_df(transformer.transform(df, label))\n</code></pre> <p>\u0427\u0442\u043e \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u0442:</p> col1 col2 anomaly 0 0 5 0 1 0 5 0 2 1 6 0 3 1 6 0 4 2 7 0 5 2 7 1 6 3 8 0 7 3 8 0 8 4 9 0 9 5 10 0 <p>\u0412:</p> subject_id col1__0 col2__0 col1__1 col2__1 col1__2 col2__2 col1__3 col2__3 col1__4 col2__4 anomaly 0 0 0 5 0 5 1 6 1 6 2 7 0 1 1 1 6 1 6 2 7 2 7 3 8 1 2 2 2 7 2 7 3 8 3 8 4 9 1"},{"location":"user-guide/anomaly_detection_module/tabular_detection/","title":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","text":""},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_2","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 <code>applybn</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043d\u0435\u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u043c\u043e\u0435 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432 \u043f\u0443\u0442\u0435\u043c \u043e\u0446\u0435\u043d\u043a\u0438 \u0431\u0430\u043b\u043b\u043e\u0432. \u0411\u0430\u043b\u043b\u044b \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u043c TabularDetector. \u041e\u0441\u043d\u043e\u0432\u0430 \u044d\u0442\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0431\u044b\u043b\u0430 \u0432\u0437\u044f\u0442\u0430 \u0438\u0437 \"Effective Outlier Detection based on Bayesian Network and Proximity\".</p> <p><code>applybn</code> \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439 \u044d\u0442\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0431\u0430\u043b\u043b\u044b, \u0441\u043c\u044f\u0433\u0447\u0435\u043d\u0438\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u0440\u0430\u0431\u043e\u0442\u0430 \u0441\u043e \u0441\u043c\u0435\u0448\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438.</p> <p>\u0421\u043f\u0438\u0441\u043e\u043a \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439:</p> <ul> <li>\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0430 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0441\u043c\u0435\u0448\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043e \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u043e\u0432\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a IQR, Cond Ratio Probability, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u043e\u0439 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438.</li> </ul> <p>\u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0439\u0442\u0435 \u044d\u0442\u0443 \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u043f\u0440\u0438 \u0432\u044b\u0431\u043e\u0440\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0435\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430:</p> \u0422\u0438\u043f \u0434\u0430\u043d\u043d\u044b\u0445 \u0420\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u041d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0435 <code>iqr</code>, <code>original_modified</code> \u0414\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u0435 <code>cond_ratio</code>, <code>original_modified</code> \u0421\u043c\u0435\u0448\u0430\u043d\u043d\u044b\u0435 <code>cond_ratio + iqr (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e)</code>, <code>original_modified</code>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_3","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_4","title":"\u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434","text":"<p>\u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 \u0441 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435\u043c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432.</p>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_5","title":"\u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438","text":"<p>\u0414\u043b\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \\( \\mathcal{D} = \\{\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)}\\} \\), \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u0430\u044f \u0441\u0442\u0440\u043e\u043a\u0430 \\( \\mathbf{x}^{(j)} = (x_1^{(j)}, x_2^{(j)}, \\dots, x_n^{(j)}) \\), \u0430\u0432\u0442\u043e\u0440\u044b \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0442 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438:</p> <ul> <li>\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: DAG \\( G \\), \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u044b\u0439 \u0443\u0437\u0435\u043b \\( X_i \\) \u0438\u043c\u0435\u0435\u0442 \u043d\u0430\u0431\u043e\u0440 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \\( \\text{Pa}(X_i) \\)</li> <li>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \u0423\u0420\u0412 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \\( X_i \\mid \\text{Pa}(X_i) \\), \u043e\u0431\u044b\u0447\u043d\u043e \u0433\u0430\u0443\u0441\u0441\u043e\u0432\u044b, \u0435\u0441\u043b\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b.</li> </ul>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_6","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u0441\u0442\u0440\u043e\u043a \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u0438\u044f","text":"<p>\u041f\u043e\u0441\u043b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438 \u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u0438\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438 \\( \\mathbf{x} \\) \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a:</p> \\[ P(\\mathbf{x}) = \\prod_{i=1}^{n} P(x_i \\mid \\text{Pa}_i(\\mathbf{x})) \\] <p>\u0413\u0434\u0435 \\( \\text{Pa}_i(\\mathbf{x}) \\) \u2014 \u044d\u0442\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \\( \\text{Pa}(X_i) \\) \u0432 \u0442\u043e\u0439 \u0436\u0435 \u0441\u0442\u0440\u043e\u043a\u0435.</p>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_7","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u0441\u0442\u0440\u043e\u043a","text":"<p>\u041e\u043d\u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u044e\u0442 \u0431\u0430\u043b\u043b \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0438 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c.</p> <p>\u0415\u0441\u043b\u0438 \u0423\u0420\u0412 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0433\u0430\u0443\u0441\u0441\u043e\u0432\u044b\u043c\u0438:</p> \\[ P(x_i \\mid \\text{Pa}_i(\\mathbf{x})) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left( -\\frac{(x_i - \\mu_i(\\text{Pa}_i))^2}{2\\sigma_i^2} \\right) \\] <p>\u0422\u043e\u0433\u0434\u0430 \u0431\u0430\u043b\u043b \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f:</p> \\[ S_{\\text{BN}}(\\mathbf{x}) = \\sum_{i=1}^{n} \\frac{(x_i - \\mu_i(\\text{Pa}_i(\\mathbf{x})))^2}{2\\sigma_i^2} \\]"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_8","title":"\u0423\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u0438","text":"<p>\u0414\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f (\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0434\u043b\u044f \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439) \u043e\u043d\u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044e\u0442 \u0431\u0430\u043b\u043b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u0438. \u041e\u043d\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u044e\u0442 \u043a\u043e\u043c\u0431\u0438\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0431\u0430\u043b\u043b \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0438 \u043a\u0430\u043a:</p> \\[ S_{\\text{Total}}(\\mathbf{x}) = S_{\\text{BN}}(\\mathbf{x}) + S_{\\text{prox}}(\\mathbf{x}) \\] <p>\u0413\u0434\u0435 \\( S_{prox}(x) \\) \u2014 \u044d\u0442\u043e \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0430\u043a\u0442\u043e\u0440\u044b \u0438\u0437 LOF.</p>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_9","title":"\u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043c\u043e\u0434\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434","text":"<p>\u0414\u043b\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \\( X = \\{X_1, X_2, \\dots, X_n\\} \\) \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0430\u044f \u0441\u0435\u0442\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u0430\u043a:</p> \\[ P(X) = \\prod_{i=1}^{n} P(X_i \\mid \\text{Pa}(X_i)) \\] <p>\u0413\u0434\u0435:</p> <ul> <li>\\( \\text{Pa}(X_i) \\) \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \u0443\u0437\u043b\u0430 \\( X_i \\) \u0432 DAG.</li> <li>\u041a\u0430\u0436\u0434\u043e\u0435 \\( P(X_i \\mid \\text{Pa}(X_i)) \\) \u2014 \u044d\u0442\u043e \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 (\u0423\u0420\u0412).</li> </ul> <p>\u041f\u0443\u0441\u0442\u044c:</p> <ul> <li>\\( \\mathcal{D} = \\{\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)}\\} \\)</li> <li>\\( \\text{pa}_i^{(j)} = \\text{pa}_i(\\mathbf{x}^{(j)}) \\), \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \\( X_i \\) \u0432 \u0441\u0442\u0440\u043e\u043a\u0435 \\( j \\)</li> </ul> <p>\u0422\u043e\u0433\u0434\u0430 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043b\u043b \u0434\u043b\u044f \u0443\u0437\u043b\u0430 \\( X_i \\) \u0432 \u0441\u0442\u0440\u043e\u043a\u0435 \\( j \\) \u0440\u0430\u0432\u0435\u043d:</p> \\[ s_i^{(j)} = \\text{Score}(x_i^{(j)} \\mid \\text{pa}_i^{(j)}) \\] <p>\u042d\u0442\u043e\u0442 \u0431\u0430\u043b\u043b \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442\u0441\u044f \u043f\u0443\u0442\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \\( x_i^{(j)} \\) \u0441 \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \\( X_i \\), \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u043c\u0435\u044e\u0442 \u0441\u0445\u043e\u0436\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \\( \\text{pa}_i^{(j)} \\). \u042d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u0440\u0430\u0437\u0438\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b:</p> \\[ s_i^{(j)} = \\phi \\left( x_i^{(j)}, \\{ x_i^{(k)} \\mid \\text{pa}_i^{(k)} = \\text{pa}_i^{(j)} \\} \\right) \\] <p>\u0413\u0434\u0435 \\( \\phi \\) \u2014 \u044d\u0442\u043e \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438.</p>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#iqr","title":"\u041c\u0435\u0442\u043e\u0434 IQR (\u043c\u0435\u0436\u043a\u0432\u0430\u0440\u0442\u0438\u043b\u044c\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0430\u0445)","text":"<p>\u0414\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438 \\( j \\):</p> <ul> <li>\\( y = x_i^{(j)} \\)</li> <li> <p>\u0423\u0441\u043b\u043e\u0432\u043d\u043e\u0435 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e:   \\(   \\mathcal{C}_i^{(j)} = \\{ x_i^{(k)} \\mid \\text{pa}_i^{(k)} = \\text{pa}_i^{(j)} \\}   \\)</p> </li> <li> <p>\u0413\u0440\u0430\u043d\u0438\u0446\u044b IQR \u0434\u043b\u044f \u0443\u0437\u043b\u0430 \\( i \\) \u043f\u0440\u0438 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \\( \\text{pa}_i^{(j)} \\):   \\(   Q_1 = \\text{Quantile}_{25\\%}(\\mathcal{C}_i^{(j)}), \\quad Q_3 = \\text{Quantile}_{75\\%}(\\mathcal{C}_i^{(j)})   \\)</p> </li> <li> <p>IQR:   \\(   \\text{IQR}_i^{(j)} = \\alpha \\cdot (Q_3 - Q_1)   \\), \u0433\u0434\u0435 \\(\\alpha\\) \u2014 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440</p> </li> <li> <p>\u0413\u0440\u0430\u043d\u0438\u0446\u044b:   \\(   \\text{Lower}_i^{(j)} = Q_1, \\quad \\text{Upper}_i^{(j)} = Q_3   \\)</p> </li> <li> <p>\u0420\u0435\u0444\u0435\u0440\u0435\u043d\u0441\u043d\u044b\u0435 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f:   \\(   d_{\\min} = \\min(\\mathcal{C}_i^{(j)}), \\quad d_{\\max} = \\max(\\mathcal{C}_i^{(j)})   \\)</p> </li> </ul>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#iqr_1","title":"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 IQR","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0431\u0430\u043b\u043b \u0443\u0437\u043b\u0430 \\( s_i^{(j)} \\) \u043a\u0430\u043a:</p> \\[ s_i^{(j)} = \\begin{cases} 0, &amp; \\text{\u0435\u0441\u043b\u0438 } \\text{Lower}_i^{(j)} &lt; x_i^{(j)} \\leq \\text{Upper}_i^{(j)} \\\\[6pt] \\min\\left(1, \\dfrac{|x_i^{(j)} - c|}{|d|} \\right), &amp; \\text{\u0438\u043d\u0430\u0447\u0435} \\end{cases} \\] <p>\u0413\u0434\u0435:</p> <ul> <li>\\( c = \\arg\\min_{b \\in \\{\\text{Lower}_i^{(j)}, \\text{Upper}_i^{(j)}\\}} |x_i^{(j)} - b| \\)</li> <li>\\( d = \\begin{cases} d_{\\max}, &amp; \\text{\u0435\u0441\u043b\u0438 } c = \\text{Upper}_i^{(j)} \\\\ d_{\\min}, &amp; \\text{\u0435\u0441\u043b\u0438 } c = \\text{Lower}_i^{(j)} \\\\ \\end{cases} \\)</li> </ul>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_10","title":"\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u0433\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f","text":"<p>\u0411\u0430\u043b\u043b\u044b, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0442\u043e\u043c, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435\u043e\u0436\u0438\u0434\u0430\u043d\u043d\u044b\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0438 \u0435\u0433\u043e \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0438 \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043c\u0430\u0440\u0436\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043c.</p> <p>\u041f\u0443\u0441\u0442\u044c:</p> <ul> <li>\\( X_i \\) \u2014 \u0443\u0437\u0435\u043b, \u0430 \\( x_i^{(j)} \\) \u2014 \u0435\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432 \u0441\u0442\u0440\u043e\u043a\u0435 \\( j \\)</li> <li>\\( \\text{pa}_i^{(j)} \\) \u2014 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u044d\u0442\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438</li> <li>\\( \\mathcal{D} \\) \u2014 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445</li> </ul> <p>\u041c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c:</p> <ul> <li>\u041c\u0430\u0440\u0436\u0438\u043d\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \\( x_i^{(j)} \\):</li> </ul> \\[ P(x_i^{(j)}) = \\frac{\\#(x_i = x_i^{(j)})}{N} \\] <ul> <li>\u0423\u0441\u043b\u043e\u0432\u043d\u0430\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438 \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044f\u0445:</li> </ul> \\[ P(x_i^{(j)} \\mid \\text{pa}_i^{(j)}) = \\frac{\\#(x_i = x_i^{(j)} \\land \\text{pa}_i = \\text{pa}_i^{(j)})}{\\#(\\text{pa}_i = \\text{pa}_i^{(j)})} \\] <p>\u0422\u0435\u043f\u0435\u0440\u044c \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043b\u043b \u0434\u043b\u044f \u0443\u0437\u043b\u0430 \\( X_i \\) \u0438 \u0441\u0442\u0440\u043e\u043a\u0438 \\( j \\) \u0440\u0430\u0432\u0435\u043d:</p> \\[ s_i^{(j)} = \\begin{cases} \\text{NaN}, &amp; \\text{\u0435\u0441\u043b\u0438 } \\frac{P(x_i^{(j)})}{P(x_i^{(j)} \\mid \\text{pa}_i^{(j)})} \\notin \\mathbb{R} \\\\ \\min\\left(1, \\frac{P(x_i^{(j)})}{P(x_i^{(j)} \\mid \\text{pa}_i^{(j)})} \\right), &amp; \\text{\u0438\u043d\u0430\u0447\u0435} \\end{cases} \\] <p>\u042d\u0442\u043e\u0442 \u0431\u0430\u043b\u043b \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0443\u0434\u0438\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044f \u0435\u0433\u043e \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442 \u0432 \u0441\u0435\u0442\u0438 \u2014 \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e\u0434\u0440\u0430\u0437\u0443\u043c\u0435\u0432\u0430\u044e\u0442 \u043c\u0435\u043d\u0435\u0435 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u043e\u0435 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0443\u0441\u043b\u043e\u0432\u043d\u043e.</p>"},{"location":"user-guide/anomaly_detection_module/tabular_detection/#_11","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/anomaly_detection/tabular.py<pre><code>\"\"\"\nExample usage script for the Tabular Detector class defined in tabular_detector.py.\n\nThis script demonstrates how to:\n1. Load the E.coli Adult dataset (as an example).\n2. Create and configure the Tabular Detector.\n3. Detect anomalies in the E.coli dataset, get scores, plot the result.\n\nRun this script to see how the methods can be chained together for end-to-end analysis.\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom applybn.anomaly_detection.static_anomaly_detector.tabular_detector import (\n    TabularDetector,\n)\n\n\ndef main():\n    # run from applybn root or change path here\n    data = pd.read_csv(\"applybn/anomaly_detection/data/tabular/ecoli.csv\")\n\n    detector_default = TabularDetector(target_name=\"y\")\n    detector_iqr = TabularDetector(target_name=\"y\", model_estimation_method=\"iqr\")\n    detector_IF = TabularDetector(target_name=\"y\", additional_score=\"IF\")\n\n    detector_default.fit(data)\n    detector_iqr.fit(data)\n    detector_IF.fit(data)\n\n    preds_default = detector_default.predict(data)\n    preds_iqr = detector_iqr.predict(data)\n    preds_IF = detector_IF.predict(data)\n\n    # let's compare the result of different methods\n    print(classification_report(data[\"y\"], preds_default))\n    print(\"___\")\n    print(classification_report(data[\"y\"], preds_iqr))\n    print(\"___\")\n    print(classification_report(data[\"y\"], preds_IF))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/","title":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445","text":""},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_2","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u0412 <code>applybn</code> \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043e \u043d\u0430 \u043e\u0441\u043e\u0431\u043e\u043c \u0442\u0438\u043f\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439 \u2014 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u044f\u0445. \u041e\u043d\u0438 \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u044b \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 \u0440\u044f\u0434\u0430\u043c\u0438.</p> <p></p> <p>\u041f\u0440\u0438\u043c\u0435\u0440 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438. \u0418\u0441\u0442\u043e\u0447\u043d\u0438\u043a: \u0441\u0442\u0430\u0442\u044c\u044f</p> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u0430\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439 \u043e\u0447\u0435\u043d\u044c \u0440\u0435\u0441\u0443\u0440\u0441\u043e\u0435\u043c\u043a\u043e, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u0438\u0437 \u0441\u0442\u0430\u0442\u044c\u0438 Outlier Detection for Multivariate Time Series Using Dynamic Bayesian Networks.</p> <p>Warning</p> <p>\u041c\u0435\u0442\u043e\u0434, \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0432 <code>applybn</code>, \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0441 \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u043c\u0438 MTS (\u043c\u043d\u043e\u0433\u043e\u043c\u0435\u0440\u043d\u044b\u043c\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 \u0440\u044f\u0434\u0430\u043c\u0438), \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0434\u043e\u043b\u0436\u0435\u043d \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0438\u0445 \u043b\u044e\u0431\u044b\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0430\u0446\u0438\u0438 (SAX, \u0431\u0438\u043d\u043d\u0438\u043d\u0433 \u0438 \u0442.\u0434.).</p>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_3","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_4","title":"\u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439","text":"<p>\u0414\u0411\u0421 \u0440\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u0442 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0443\u044e \u0441\u0435\u0442\u044c (\u0411\u0421), \u0432\u0432\u043e\u0434\u044f \u0432\u0440\u0435\u043c\u044f \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u044f\u0432\u043d\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439. \u041a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b:</p>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_5","title":"\u0412\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0441\u0440\u0435\u0437\u044b","text":"<p>\u0414\u0411\u0421 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u043f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u0438\u0439 \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438 \u043d\u0430 \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u0445 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0448\u0430\u0433\u0430\u0445 (\\(t=0, t=1, ..., t=T\\)). \u041a\u0430\u0436\u0434\u044b\u0439 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0441\u0440\u0435\u0437 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0442\u0435 \u0436\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u043d\u043e \u0441 \u0438\u0437\u043c\u0435\u043d\u044f\u044e\u0449\u0438\u043c\u0438\u0441\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c\u0438.</p>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_6","title":"\u0417\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0432\u043d\u0443\u0442\u0440\u0438 \u0438 \u043c\u0435\u0436\u0434\u0443 \u0441\u0440\u0435\u0437\u0430\u043c\u0438","text":"<ul> <li>\u0420\u0435\u0431\u0440\u0430 \u0432\u043d\u0443\u0442\u0440\u0438 \u0441\u0440\u0435\u0437\u0430 (\u0432 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445 \u043e\u0434\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0448\u0430\u0433\u0430) \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u0443\u044e\u0442 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \\(X_t \\rightarrow Y_t\\)).</li> <li>\u0420\u0435\u0431\u0440\u0430 \u043c\u0435\u0436\u0434\u0443 \u0441\u0440\u0435\u0437\u0430\u043c\u0438 (\u043c\u0435\u0436\u0434\u0443 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 \u0448\u0430\u0433\u0430\u043c\u0438) \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u0443\u044e\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \\(X_t \\rightarrow X_{t+1}\\)).</li> </ul>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#2-2-tbn","title":"2-\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0430\u044f \u0441\u0435\u0442\u044c (2-TBN)","text":"<p>\u041a\u043e\u043c\u043f\u0430\u043a\u0442\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435, \u0433\u0434\u0435 \u043f\u043e\u043b\u043d\u0430\u044f \u0414\u0411\u0421 \"\u0440\u0430\u0437\u0432\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f\" \u0438\u0437 \u0448\u0430\u0431\u043b\u043e\u043d\u0430 \u043d\u0430 2 \u0441\u0440\u0435\u0437\u0430. \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0430:   - \u0410\u043f\u0440\u0438\u043e\u0440\u043d\u0430\u044f \u0441\u0435\u0442\u044c (\\(t=0\\)): \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439.   - \u0421\u0435\u0442\u044c \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u043e\u0432 (\\(t \\rightarrow t+1\\)): \u043a\u0430\u043a \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u044e\u0442\u0441\u044f.</p>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_7","title":"\u041a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u0434\u043e\u043f\u0443\u0449\u0435\u043d\u0438\u044f \u0432 \u0414\u0411\u0421","text":""},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_8","title":"\u041c\u0430\u0440\u043a\u043e\u0432\u0441\u043a\u043e\u0435 \u0434\u043e\u043f\u0443\u0449\u0435\u043d\u0438\u0435","text":"<ul> <li> <p>\u0411\u0443\u0434\u0443\u0449\u0435\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e \u043e\u0442 \u043f\u0440\u043e\u0448\u043b\u043e\u0433\u043e \u043f\u0440\u0438 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u043c \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u043c:   \\( P(X_{t+1} | X_t, X_{t-1}, ..., X_0) = P(X_{t+1} | X_t) \\)</p> </li> <li> <p>\u0417\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u043e\u0433\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0430 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0441\u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043f\u0443\u0442\u0435\u043c \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u043f\u043e\u0440\u044f\u0434\u043a\u0430 \u041c\u0430\u0440\u043a\u043e\u0432\u0430.</p> </li> </ul>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_9","title":"\u0421\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u044c (\u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438)","text":"<ul> <li>\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0430 (\\(P(X_{t+1}|X_t)\\)) \u043d\u0435 \u0438\u0437\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0441\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c.</li> </ul>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_10","title":"\u0424\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044f","text":"<ul> <li>\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043e \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \\(X_t, Y_t, Z_t\\)), \u043a\u0430\u0436\u0434\u0430\u044f \u0441\u043e \u0441\u0432\u043e\u0435\u0439 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0434\u0438\u043d\u0430\u043c\u0438\u043a\u043e\u0439.</li> </ul>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_11","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0414\u0411\u0421","text":""},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_12","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","text":"<p>\u041f\u0440\u0438 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f CPT (\u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439) \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c. \u041c\u0435\u0442\u043e\u0434\u044b:</p> <ul> <li>\u041e\u0446\u0435\u043d\u043a\u0430 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u0438\u044f (MLE)</li> <li>\u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0430\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 (\u0441 \u0430\u043f\u0440\u0438\u043e\u0440\u043d\u044b\u043c\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f\u043c\u0438 \u0414\u0438\u0440\u0438\u0445\u043b\u0435)</li> </ul>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_13","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435","text":"<p>\u041e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u043c\u0435\u0436\u0441\u0440\u0435\u0437\u043e\u0432\u044b\u0435, \u0442\u0430\u043a \u0438 \u0432\u043d\u0443\u0442\u0440\u0438\u0441\u0440\u0435\u0437\u043e\u0432\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438. \u041c\u0435\u0442\u043e\u0434\u044b:</p> <ul> <li>\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u0446\u0435\u043d\u043e\u043a (BIC, AIC)</li> <li>\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439 (\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c PC, \u0430\u0434\u0430\u043f\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0434\u043b\u044f \u0432\u0440\u0435\u043c\u0435\u043d\u0438)</li> </ul>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#applybn","title":"\u041c\u0435\u0442\u043e\u0434 \u0432 <code>applybn</code>","text":"<p><code>applybn</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b. \u0412\u043c\u0435\u0441\u0442\u043e \u0434\u043e\u0440\u043e\u0433\u043e\u0441\u0442\u043e\u044f\u0449\u0438\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u043e\u043d \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u0430 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0441\u0442\u043e\u0432\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430.</p> <p>\u041e\u043d\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> <p>\u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043e\u0441\u0442\u043e\u0432\u043d\u043e\u0435 \u0434\u0435\u0440\u0435\u0432\u043e (MST) \u0438\u043b\u0438 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0432\u0435\u0441\u043e\u0432\u043e\u0435 \u043e\u0441\u0442\u043e\u0432\u043d\u043e\u0435 \u0434\u0435\u0440\u0435\u0432\u043e \u2014 \u044d\u0442\u043e \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0431\u0435\u0440 \u0441\u0432\u044f\u0437\u043d\u043e\u0433\u043e, \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u043e\u0433\u043e \u043f\u043e \u0440\u0435\u0431\u0440\u0430\u043c \u043d\u0435\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0430, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0441\u043e\u0435\u0434\u0438\u043d\u044f\u0435\u0442 \u0432\u0441\u0435 \u0432\u0435\u0440\u0448\u0438\u043d\u044b \u0432\u043c\u0435\u0441\u0442\u0435, \u0431\u0435\u0437 \u0446\u0438\u043a\u043b\u043e\u0432 \u0438 \u0441 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u043c \u043e\u0431\u0449\u0438\u043c \u0432\u0435\u0441\u043e\u043c \u0440\u0435\u0431\u0435\u0440. -- \u0412\u0438\u043a\u0438\u043f\u0435\u0434\u0438\u044f</p> <p>Warning</p> <p>\u0414\u043b\u044f \u043b\u044e\u0431\u043e\u0439 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0441\u0442\u0440\u043e\u0433\u043e\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u0430\u0446\u0438\u043a\u043b\u0438\u0447\u043d\u043e\u0441\u0442\u0438. \u0415\u0441\u043b\u0438 \u0432 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438 \u0435\u0441\u0442\u044c \u0446\u0438\u043a\u043b\u044b, \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e \u0447\u0442\u043e-\u0442\u043e \u043f\u043e\u0448\u043b\u043e \u043d\u0435 \u0442\u0430\u043a.</p> <p>\u042d\u0442\u043e\u0442 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u0448\u0430\u0433\u043e\u0432:</p> <ol> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043f\u043e\u043b\u043d\u044b\u0439 \u0433\u0440\u0430\u0444 \u0441 \u043c\u0430\u0440\u043a\u043e\u0432\u0441\u043a\u0438\u043c \u043b\u0430\u0433\u043e\u043c \\(m\\), \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0435 \u0441\u0435\u0442\u0438 \u0432\u043d\u0443\u0442\u0440\u0438 \u0441\u0440\u0435\u0437\u043e\u0432 \u0442\u0430\u043a\u0436\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u043e\u043b\u043d\u044b\u043c\u0438.</li> <li>\u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0432\u0435\u0441\u0430 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0440\u0435\u0431\u0440\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0433\u043e LL, \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0445\u043e\u0440\u043e\u0448\u043e    \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0443\u0437\u043b\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0435\u043c\u044b\u043c \u0434\u0430\u043d\u043d\u044b\u043c, \u043f\u0440\u0438 \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0433\u043e \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044f\u0445.</li> </ol> <p>\u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0441\u0440\u0435\u0437 (\\( t=0 \\)) (\u0442\u0430\u043a \u0436\u0435, \u043a\u0430\u043a \u0432 \u0441\u0442\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0411\u0421).</p> <p>\\(\\mathcal{L}_X^{(0)}(\\theta_X | \\mathcal{D}) = \\sum_{i=1}^{N} \\log P(X_0 = x_0^{(i)} | \\text{Pa}(X_0) = \\text{pa}_0^{(i)}, \\theta_X)\\)</p> <p>\u0421\u0440\u0435\u0437\u044b \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u043e\u0432 (\\( t \\geq 1 \\)). \u041b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u0438\u0435 \u0434\u043b\u044f \u0443\u0437\u043b\u0430 \\( X \\) \u0432 \u043c\u043e\u043c\u0435\u043d\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \\( t+1 \\) \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0435\u0433\u043e \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \u0432 \u043c\u043e\u043c\u0435\u043d\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \\( t \\).</p> <p>\\( \\mathcal{L}_X^{\\text{trans}}(\\theta_X | \\mathcal{D}) = \\sum_{t=0}^{T-1} \\sum_{i=1}^{N} \\log P(X_{t+1} = x_{t+1}^{(i)} | \\text{Pa}(X_{t+1}) = \\text{pa}_{t+1}^{(i)}, \\theta_X) \\)</p> <p>\u0433\u0434\u0435:</p> <ul> <li>\\( T \\) - \u043e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0448\u0430\u0433\u043e\u0432,</li> <li>\\(\\mathcal{D}\\) - \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\\( \\text{Pa}(X_{t+1}) \\) \u043c\u043e\u0436\u0435\u0442 \u0432\u043a\u043b\u044e\u0447\u0430\u0442\u044c \u043a\u0430\u043a \u0432\u043d\u0443\u0442\u0440\u0438\u0441\u0440\u0435\u0437\u043e\u0432\u044b\u0445, \u0442\u0430\u043a \u0438 \u043c\u0435\u0436\u0441\u0440\u0435\u0437\u043e\u0432\u044b\u0445 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439.</li> <li>\\(\\theta_x\\) - \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f.</li> </ul> <p>\u0420\u043e\u0434\u0438\u0442\u0435\u043b\u0438 \u0431\u044b\u043b\u0438 \u0432\u0437\u044f\u0442\u044b \u043a\u0430\u043a:</p> <p>&lt;...&gt; \u0434\u043e p \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \u0438\u0437 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 m \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0441\u0440\u0435\u0437\u043e\u0432 \u0438 \u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c \u0438\u0437 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0441\u0440\u0435\u0437\u0430 t.</p> <p>\u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0438 \u0431\u0435\u0440\u0443\u0442\u0441\u044f \u0432 \u0442\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043e\u043d\u0438 \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u0435\u0442\u0432\u043b\u0435\u043d\u0438\u044f.</p> <p>\u041f\u043e\u0441\u043b\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0430 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u0435\u0442\u0432\u043b\u0435\u043d\u0438\u044f, \u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0434\u043b\u044f \u0414\u0411\u0421.</p> <p>\u0414\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f MLE.</p>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#tdbn","title":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e tDBN","text":"<p>\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u0432 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439 \u043f\u043e \u043e\u0446\u0435\u043d\u043a\u0430\u043c. \u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0444\u043e\u0440\u043c\u043e\u0439 (1000, 56) \u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043e\u0446\u0435\u043d\u043e\u043a \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c (10, 1000), \u0433\u0434\u0435 10 \u2014 \u044d\u0442\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u043e\u0432, \u0430 1000 \u2014 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0443\u0431\u044a\u0435\u043a\u0442\u043e\u0432. \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0430 \u0435\u0441\u0442\u044c \u043c\u0435\u0440\u0430 \u0430\u043d\u043e\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u0435\u0440\u0432\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0430\u043d\u043e\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432 \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0435 \u043e\u0442 \\(X\\) \u043a \\(X_{t + 1}\\)).</p> <p>\u042d\u0442\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u0440\u043e\u0433\u043e\u0432\u044b\u043c\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>abs_threshold</code> \u0438 <code>rel_threshold</code>. \u041f\u0435\u0440\u0432\u044b\u0439 \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u0431\u0438\u043d\u0430\u0440\u043d\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441 \u0444\u043e\u0440\u043c\u043e\u0439 (10, 1000). \u0417\u0430\u0442\u0435\u043c <code>rel_threshold</code> (\u0447\u0438\u0441\u043b\u043e \u043e\u0442 0 \u0434\u043e 1) \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u0441\u0443\u0431\u044a\u0435\u043a\u0442 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b \u0430\u043d\u043e\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435, \u0438 \u0441 \u043a\u0430\u043a\u043e\u0439 \u0434\u043e\u043b\u0438 \u0441\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u044f.</p> <p>Note</p> <p>\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c, \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432 <code>rel_threshold</code> \u0438 \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0432 <code>abs_threshold</code>.</p>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_14","title":"\u0421\u043f\u0435\u0446\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","text":"<p>\u0427\u0442\u043e\u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>FastTimeSeriesDetector</code>, \u0432\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0438\u043c\u0435\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0444\u043e\u0440\u043c\u0430\u0442:</p> <pre><code>    subject_id f1__0  f2__0  f1__1  f2__1\n        0        0      10     1      11\n        1        1      11     2      12\n</code></pre> <p>\u0433\u0434\u0435</p> <ul> <li>subject_id \u2014 \u044d\u0442\u043e \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0441\u0443\u0431\u044a\u0435\u043a\u0442 (\u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0434\u0430\u0442\u0447\u0438\u043a\u0438, \u043b\u044e\u0434\u0438 \u0438 \u0442.\u0434.).</li> <li>\u041a\u0430\u0436\u0434\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u2014 \u044d\u0442\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0441\u0440\u0435\u0437\u0430 \u0441 \u0438\u043c\u0435\u043d\u0435\u043c feature_name__index (<code>__</code> \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e!).</li> </ul>"},{"location":"user-guide/anomaly_detection_module/time_series_detection/#_15","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/anomaly_detection/time_series.py<pre><code>from applybn.anomaly_detection.dynamic_anomaly_detector.fast_time_series_detector import (\n    FastTimeSeriesDetector,\n)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n\ndef add_anomalies(df, anomaly_fraction=0.05, random_state=None):\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    df_anomaly = df.copy()\n    total = df.shape[0]\n\n    # Initialize label matrix with zeros\n    anomaly_labels = np.zeros(total, dtype=int)\n\n    n_anomalies = int(df.shape[0] * anomaly_fraction)\n\n    # Generate random positions\n    rows = np.random.randint(0, df.shape[0], size=n_anomalies)\n\n    for row_idx in rows:\n        new_value = np.random.choice([\"a\", \"b\", \"c\"], size=df.shape[1] - 1)\n        anomaly_labels[row_idx] = 1\n\n        df_anomaly.iloc[row_idx, 1:] = new_value\n\n    return df_anomaly, anomaly_labels\n\n\ndf_discrete = pd.read_csv(\n    \"../../data/anomaly_detection/ts/meteor_discrete_example_data.csv\"\n)\n\ndf_anomaly, anomalies = add_anomalies(\n    df_discrete, anomaly_fraction=0.1, random_state=42\n)\n\ndetector = FastTimeSeriesDetector(markov_lag=1, num_parents=1)\n\ndetector.fit(df_anomaly)\ndetector.calibrate(anomalies)\npreds_cont = detector.predict(df_anomaly)\n\nprint(f1_score(anomalies, preds_cont))  # 0.7352941176470589 (may vary)\n</code></pre>"},{"location":"user-guide/explainable_module/cnn_filter_analysis/","title":"CausalCNNExplainer: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435","text":""},{"location":"user-guide/explainable_module/cnn_filter_analysis/#_1","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p><code>CausalCNNExplainer</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u044b\u0432\u043e\u0434\u0430 \u0434\u043b\u044f \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0445 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439, \u043a\u0430\u043a \u043e\u043f\u0438\u0441\u0430\u043d\u043e \u0432 \u0441\u0442\u0430\u0442\u044c\u0435 Explaining Deep Learning Models using Causal Inference, T. Narendra \u0438 \u0434\u0440.. \u042d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u0442 \u0444\u0438\u043b\u044c\u0442\u0440\u044b CNN \u043a\u0430\u043a \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0432 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u043c \u0433\u0440\u0430\u0444\u0435, \u0447\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0438\u0437\u043c\u0435\u0440\u044f\u0442\u044c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043d\u044b\u043c\u0438 \u0443\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f\u043c\u0438, \u0430 \u043d\u0435 \u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 \u043f\u043e\u0434\u0445\u043e\u0434\u0430\u043c\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438.</p>"},{"location":"user-guide/explainable_module/cnn_filter_analysis/#_2","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/explainable_module/cnn_filter_analysis/#cnn","title":"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 CNN","text":"<p>\u041c\u043e\u0434\u0443\u043b\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0444\u0438\u043b\u044c\u0442\u0440\u044b CNN \u043a\u0430\u043a \u0443\u0437\u043b\u044b \u0432 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u043c \u0430\u0446\u0438\u043a\u043b\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u0433\u0440\u0430\u0444\u0435 (DAG), \u0433\u0434\u0435 \u0440\u0435\u0431\u0440\u0430 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u043c\u0438 \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u043e\u044f\u0445:</p> \\[ G = (V, E) \\] <p>\u0433\u0434\u0435 \\(V\\) \u2014 \u044d\u0442\u043e \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 \u043f\u043e \u0432\u0441\u0435\u043c \u0441\u043b\u043e\u044f\u043c, \u0430 \\(E\\) \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u043d\u0438\u043c\u0438.</p>"},{"location":"user-guide/explainable_module/cnn_filter_analysis/#_3","title":"\u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043d\u044b\u043c\u0438 \u0443\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f\u043c\u0438","text":"<p>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0438\u043b\u044c\u0442\u0440\u0430 \\(i\\) \u0432 \u0441\u043b\u043e\u0435 \\(l\\) \u0435\u0433\u043e \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u044f \\(F_i^l\\) \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0435\u0433\u043e \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 \u0438\u0437 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0433\u043e \u0441\u043b\u043e\u044f:</p> \\[F_i^l = f_i(\\{F_j^{l-1} | j \\in \\text{parents}(i)\\}) + \\epsilon_i\\] <p>\u041c\u043e\u0434\u0443\u043b\u044c \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u044d\u0442\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, \u0433\u0434\u0435 \u0432\u044b\u0445\u043e\u0434 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0438\u043b\u044c\u0442\u0440\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0432\u044b\u0445\u043e\u0434\u043e\u0432 \u0435\u0433\u043e \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432:</p> \\[F_i^l = \\beta_0 + \\sum_{j \\in \\text{parents}(i)} \\beta_j \\cdot F_j^{l-1} + \\epsilon_i\\]"},{"location":"user-guide/explainable_module/cnn_filter_analysis/#_4","title":"\u0420\u0430\u0441\u0447\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0444\u0438\u043b\u044c\u0442\u0440\u0430","text":"<p>\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0444\u0438\u043b\u044c\u0442\u0440\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u0430\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0435\u0433\u043e \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u043f\u0440\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0438 \u0432\u0441\u0435\u0445 \u0434\u043e\u0447\u0435\u0440\u043d\u0438\u0445 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c \u0441\u043b\u043e\u0435:</p> \\[\\text{Importance}(F_j^l) = \\sum_{i \\in \\text{children}(j)} |\\beta_{j \\rightarrow i}|\\] <p>\u0433\u0434\u0435 \\(\\beta_{j \\rightarrow i}\\) \u2014 \u044d\u0442\u043e \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, \u043e\u0442\u0440\u0430\u0436\u0430\u044e\u0449\u0438\u0439, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0444\u0438\u043b\u044c\u0442\u0440 \\(j\\) \u0432 \u0441\u043b\u043e\u0435 \\(l\\) \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0444\u0438\u043b\u044c\u0442\u0440 \\(i\\) \u0432 \u0441\u043b\u043e\u0435 \\(l+1\\).</p>"},{"location":"user-guide/explainable_module/cnn_filter_analysis/#_5","title":"\u0421\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f \u043f\u0440\u0443\u043d\u0438\u043d\u0433\u0430 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432","text":"<p>\u041c\u043e\u0434\u0443\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u044d\u0442\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043f\u0440\u0443\u043d\u0438\u043d\u0433\u0430, \u0443\u0434\u0430\u043b\u044f\u044f \u0444\u0438\u043b\u044c\u0442\u0440\u044b \u0441 \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0438\u043c \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u043c \u0432\u043b\u0438\u044f\u043d\u0438\u0435\u043c:</p> \\[\\text{PruneSet} = \\{F_j^l | \\text{Importance}(F_j^l) \\leq \\text{threshold}_l\\}\\] <p>\u0433\u0434\u0435 \\(\\text{threshold}_l\\) \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u0436\u0435\u043b\u0430\u0435\u043c\u044b\u043c \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u043c \u043f\u0440\u0443\u043d\u0438\u043d\u0433\u0430.</p>"},{"location":"user-guide/explainable_module/cnn_filter_analysis/#_6","title":"\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f","text":"<p>\u042d\u0442\u043e\u0442 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 CNN \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442:</p> <ol> <li>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u044b\u0439 \u043f\u0440\u0443\u043d\u0438\u043d\u0433 - \u0443\u0434\u0430\u043b\u044f\u0435\u0442 \u0444\u0438\u043b\u044c\u0442\u0440\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u043b\u0438\u044f\u043d\u0438\u044f, \u0430 \u043d\u0435 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b \u0438\u043b\u0438 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438</li> <li>\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043f\u043b\u043e\u0432\u044b\u0445 \u043a\u0430\u0440\u0442 - \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043a\u0430\u043a\u0438\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0412\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 - \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u043c\u0438 \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0441\u043b\u043e\u044f\u0445</li> </ol> <p>\u041c\u0435\u0442\u043e\u0434 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442, \u0447\u0442\u043e \u043f\u0440\u0443\u043d\u0438\u043d\u0433 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0439 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u044b\u0447\u043d\u043e \u043b\u0443\u0447\u0448\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438, \u0447\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043f\u0440\u0443\u043d\u0438\u043d\u0433, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u043d \u0432\u044b\u044f\u0432\u043b\u044f\u0435\u0442 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u0444\u0438\u043b\u044c\u0442\u0440\u044b \u0441 \u0441\u0430\u043c\u044b\u043c \u0441\u0438\u043b\u044c\u043d\u044b\u043c \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u043c \u0432\u043b\u0438\u044f\u043d\u0438\u0435\u043c \u043d\u0430 \u0432\u044b\u0445\u043e\u0434 \u0441\u0435\u0442\u0438.</p>"},{"location":"user-guide/explainable_module/cnn_filter_analysis/#_7","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/explainable/cnn_filter_importance.py<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\n\n# For progress bar in loops (optional usage)\nfrom tqdm import tqdm\n\n# Import the causal_explainer module (adjust path as needed)\nfrom applybn.explainable.nn_layers_importance.cnn_filter_importance import (\n    CausalCNNExplainer,\n)\n\n\ndef train_model(model, train_loader, device, num_epochs=5, lr=0.0001):\n    \"\"\"Trains the given model on the provided data loader.\n\n    Args:\n        model (nn.Module):\n            The model to be trained.\n        train_loader (DataLoader):\n            DataLoader for the training data.\n        device (torch.device):\n            The device (CPU or CUDA) to train on.\n        num_epochs (int):\n            Number of epochs for training.\n        lr (float):\n            Learning rate for the optimizer.\n\n    Returns:\n        nn.Module: Trained model.\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model.train()\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        total = 0\n        correct = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = correct / total\n        print(\n            f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, \"\n            f\"Accuracy: {epoch_acc*100:.2f}%\"\n        )\n\n    return model\n\n\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    random.seed(42)\n    np.random.seed(42)\n\n    # Define transformations (as an example, e.g. CIFAR-10)\n    transform = transforms.Compose(\n        [\n            transforms.Resize(224),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ]\n    )\n\n    # Example: Load CIFAR-10 dataset\n    train_dataset = datasets.CIFAR10(\n        root=\"./data\", train=True, download=True, transform=transform\n    )\n    test_dataset = datasets.CIFAR10(\n        root=\"./data\", train=False, download=True, transform=transform\n    )\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n    # Load a pre-trained ResNet18 model\n    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n\n    # Modify the last layer to output 10 classes\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 10)\n\n    # Train the model (example uses fewer epochs for quick demonstration)\n    model = train_model(model, train_loader, device, num_epochs=1, lr=0.0001)\n\n    # Create a CausalCNNExplainer instance\n    explainer = CausalCNNExplainer(model=model, device=device)\n\n    # Collect data for structural equation modeling\n    explainer.collect_data(train_loader)\n\n    # Learn structural equations (computes filter importances)\n    explainer.learn_structural_equations()\n    filter_importances = explainer.get_filter_importances()\n    print(\"Filter importances collected.\")\n\n    # Visualizations (ADD THESE)\n    print(\"\\nVisualizing insights...\")\n\n    # 1. Input-space heatmap\n    sample_image, _ = next(iter(test_loader))\n    explainer.visualize_heatmap_on_input(sample_image[0])\n\n    # 2. First-layer filters\n    explainer.visualize_first_layer_filters(n_filters=16)\n\n    # 3. Importance distribution across layers\n    explainer.plot_importance_distribution()\n\n    # 4. t-SNE of filter weights (e.g., for layer 3)\n    explainer.visualize_filter_tsne(layer_idx=3)\n\n    # Evaluate baseline accuracy\n    baseline_acc = explainer.evaluate_model(explainer.model, test_loader)\n    print(f\"Baseline Accuracy: {baseline_acc*100:.2f}%\")\n\n    # Demonstrate pruning by importance vs random pruning\n    prune_percentages = [5, 10, 20]  # Example percentages\n    importance_accuracies = []\n    random_accuracies = []\n\n    for percent in prune_percentages:\n        # Pruning by importance\n        pruned_model_importance = explainer.prune_filters_by_importance(percent)\n        acc_imp = explainer.evaluate_model(pruned_model_importance, test_loader)\n        importance_accuracies.append(acc_imp * 100)\n        print(f\"Accuracy after pruning {percent}% by importance: {acc_imp*100:.2f}%\")\n\n        # Random pruning\n        pruned_model_random = explainer.prune_random_filters(percent)\n        acc_rand = explainer.evaluate_model(pruned_model_random, test_loader)\n        random_accuracies.append(acc_rand * 100)\n        print(f\"Accuracy after pruning {percent}% randomly: {acc_rand*100:.2f}%\")\n\n    # Plot the results (optional, showing example of usage)\n    try:\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(\n            [0] + prune_percentages,\n            [baseline_acc * 100] + importance_accuracies,\n            marker=\"o\",\n            label=\"Pruning by Importance\",\n        )\n        plt.plot(\n            [0] + prune_percentages,\n            [baseline_acc * 100] + random_accuracies,\n            marker=\"s\",\n            label=\"Random Pruning\",\n        )\n        plt.title(\"Accuracy vs. Percentage of Filters Pruned\")\n        plt.xlabel(\"Percentage of Filters Pruned\")\n        plt.ylabel(\"Accuracy (%)\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    except ImportError:\n        print(\"Matplotlib is not installed. Skipping plots.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user-guide/explainable_module/concept_explainer/","title":"ConceptCausalExplainer: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435","text":""},{"location":"user-guide/explainable_module/concept_explainer/#_1","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p><code>ConceptCausalExplainer</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043d\u043e\u0432\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0447\u0435\u0440\u0435\u0437 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432, \u043a\u0430\u043a \u043e\u043f\u0438\u0441\u0430\u043d\u043e \u0432 \u0441\u0442\u0430\u0442\u044c\u0435 \"Concept-Level Model Interpretation From the Causal Aspect\". \u0412\u043c\u0435\u0441\u0442\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u044d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c \u0432\u044b\u044f\u0432\u043b\u044f\u0435\u0442 \u0432\u044b\u0441\u043e\u043a\u043e\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u044b\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u044b \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442 \u0438\u0445 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044f \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u043c \u0434\u043b\u044f \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c.</p>"},{"location":"user-guide/explainable_module/concept_explainer/#_2","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/explainable_module/concept_explainer/#_3","title":"\u0418\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0438 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432","text":"<p>\u041c\u043e\u0434\u0443\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0434\u0432\u0443\u0445\u044d\u0442\u0430\u043f\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044e \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432:</p> <ol> <li>\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438: \u041f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u0434\u0430\u043d\u043d\u044b\u0445 \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 KMeans:</li> </ol> <p>\\( C_i = \\{x_j \\in D \\mid \\arg\\min_k \\|x_j - \\mu_k\\|^2 = i\\} \\)</p> <p>\u0433\u0434\u0435:</p> <ul> <li>\\(C_i\\) \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043a\u043b\u0430\u0441\u0442\u0435\u0440 \\(i\\),</li> <li>\\(D\\) \u2014 \u044d\u0442\u043e \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f,</li> <li>\\(\\mu_k\\) \u2014 \u044d\u0442\u043e \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432.</li> </ul> <p>\u00a02. \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432: \u041a\u0430\u0436\u0434\u044b\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043f\u0443\u0442\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e SVM:</p> <p>\\( S_i(x) = \\text{sign}(w_i^T x + b_i) \\)</p> <p>\u041a\u043b\u0430\u0441\u0442\u0435\u0440 \u0441\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f \u0432\u0430\u043b\u0438\u0434\u043d\u044b\u043c \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u043c, \u0435\u0441\u043b\u0438 \u0435\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u043e\u0442\u043b\u0438\u0447\u0438\u0442\u044c \u043e\u0442 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 AUC &gt; \u043f\u043e\u0440\u043e\u0433\u0430.</p>"},{"location":"user-guide/explainable_module/concept_explainer/#_4","title":"\u041f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432","text":"<p>\u041f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u043e\u0432: \\( A(x) = [A_1(x), A_2(x), ..., A_m(x)] \\) \u0433\u0434\u0435 \\(A_i(x) = 1\\), \u0435\u0441\u043b\u0438 \\(x\\) \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0438\u0442 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443 \\(i\\), \u0438 0 \u0432 \u043f\u0440\u043e\u0442\u0438\u0432\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435.</p>"},{"location":"user-guide/explainable_module/concept_explainer/#_5","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0430","text":"<p>\u0414\u043b\u044f \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0433\u043e \u0438\u0441\u0445\u043e\u0434\u0430 \\(L_f\\) \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u044d\u0444\u0444\u0435\u043a\u0442 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430 \\(A_i\\) \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e: \\(\\tau_i = \\mathbb{E}[L_f \\mid do(A_i = 1)] - \\mathbb{E}[L_f \\mid do(A_i = 0)]\\)</p> <p>\u0414\u043b\u044f \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0445 \u0438\u0441\u0445\u043e\u0434\u043e\u0432, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a Double Machine Learning: \\(\\tau_i(x) = \\mathbb{E}[Y \\mid do(A_i = 1), X = x] - \\mathbb{E}[Y \\mid do(A_i = 0), X = x]\\)</p> <p>\u0433\u0434\u0435 \\(Y\\) \u2014 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0449\u0438\u0439 \u0438\u0441\u0445\u043e\u0434 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438), \u0430 \\(X\\) \u2014 \u0434\u0440\u0443\u0433\u0438\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u044b, \u0432\u044b\u0441\u0442\u0443\u043f\u0430\u044e\u0449\u0438\u0435 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445.</p>"},{"location":"user-guide/explainable_module/concept_explainer/#_6","title":"\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432:</p> <ol> <li>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c: \u041a\u043e\u043d\u0446\u0435\u043f\u0442\u044b \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u043c \u0434\u043b\u044f \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u0430\u043c \u0432 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435: \u041e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432, \u0430 \u043d\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0439</li> <li>\u0414\u0438\u0430\u0433\u043d\u043e\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0441\u0438\u043b\u0430: \u0412\u044b\u044f\u0432\u043b\u044f\u0435\u0442, \u043a\u0430\u043a\u0438\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u044b \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u041f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c\u043e\u0441\u0442\u044c: \u041a\u043e\u043d\u0446\u0435\u043f\u0442\u044b \u043c\u043e\u0433\u0443\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u043a \u0440\u0430\u0437\u043d\u044b\u043c \u043c\u043e\u0434\u0435\u043b\u044f\u043c \u043d\u0430 \u043e\u0434\u043d\u0438\u0445 \u0438 \u0442\u0435\u0445 \u0436\u0435 \u0434\u0430\u043d\u043d\u044b\u0445</li> </ol> <p>\u041f\u043e\u043d\u0438\u043c\u0430\u044f \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c\u0438 \u0438 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438, \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u043c\u043e\u0433\u0443\u0442 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043e\u0431 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f\u0445 \u043c\u043e\u0434\u0435\u043b\u0438, \u0441\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f\u0445 \u0438\u043d\u0436\u0438\u043d\u0438\u0440\u0438\u043d\u0433\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</p>"},{"location":"user-guide/explainable_module/concept_explainer/#_7","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/explainable/concept_explainer.py<pre><code>\"\"\"\nExample usage script for the CausalModelExplainer class defined in causal_explanator.py.\n\nThis script demonstrates how to:\n1. Load and preprocess the UCI Adult dataset (as an example).\n2. Create and configure the CausalModelExplainer.\n3. Extract concepts, generate a concept space, train a predictive model, and estimate causal effects.\n\nRun this script to see how the methods can be chained together for end-to-end analysis.\n\"\"\"\n\nimport pandas as pd\nfrom rich import print as rprint\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom applybn.explainable.causal_analysis import ConceptCausalExplainer\n\n\ndef load_and_preprocess_data():\n    \"\"\"Load and preprocess the UCI Adult dataset.\n\n    Returns:\n        tuple: (X_processed, y, X_original) where:\n            X_processed (pd.DataFrame): Processed features, ready for modeling.\n            y (pd.Series): Binary labels (income &gt;50K or &lt;=50K).\n            X_original (pd.DataFrame): Original features before encoding/scaling.\n    \"\"\"\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n    column_names = [\n        \"age\",\n        \"workclass\",\n        \"fnlwgt\",\n        \"education\",\n        \"education-num\",\n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n        \"sex\",\n        \"capital-gain\",\n        \"capital-loss\",\n        \"hours-per-week\",\n        \"native-country\",\n        \"income\",\n    ]\n    data = pd.read_csv(url, names=column_names, header=None, na_values=\" ?\")\n\n    data.dropna(inplace=True)\n    data.reset_index(drop=True, inplace=True)\n\n    X_original = data.drop(\"income\", axis=1).reset_index(drop=True)\n    y = (\n        data[\"income\"]\n        .apply(lambda x: 1 if x.strip() == \"&gt;50K\" else 0)\n        .reset_index(drop=True)\n    )\n\n    # One-hot encode categorical columns\n    categorical_cols = X_original.select_dtypes(include=[\"object\"]).columns\n    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n    X_encoded = pd.DataFrame(\n        encoder.fit_transform(X_original[categorical_cols]),\n        columns=encoder.get_feature_names_out(categorical_cols),\n    )\n    X_numeric = X_original.select_dtypes(exclude=[\"object\"]).reset_index(drop=True)\n    X_processed = pd.concat(\n        [X_numeric.reset_index(drop=True), X_encoded.reset_index(drop=True)], axis=1\n    )\n\n    # Scale numeric columns\n    numeric_cols = X_numeric.columns\n    scaler = StandardScaler()\n    X_processed[numeric_cols] = scaler.fit_transform(X_processed[numeric_cols])\n    X_processed.reset_index(drop=True, inplace=True)\n\n    return X_processed, y, X_original\n\n\ndef main():\n    \"\"\"Demonstration of using CausalModelExplainer on a sample dataset.\"\"\"\n    # Load and preprocess data\n    X, y, original_X = load_and_preprocess_data()\n\n    # Create discovery (D) and natural (N) datasets\n    D, N = train_test_split(X, test_size=0.3, random_state=42, shuffle=False)\n    D.reset_index(drop=False, inplace=True)\n    N.reset_index(drop=False, inplace=True)\n\n    # Instantiate the explainer\n    explainer = ConceptCausalExplainer()\n\n    # Extract concepts\n    cluster_concepts = explainer.extract_concepts(D, N)\n\n    # Generate concept space\n    A = explainer.generate_concept_space(X, cluster_concepts)\n\n    # Train a random forest classifier for demonstration\n    predictive_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    predictive_model.fit(X, y)\n\n    # Calculate confidence and uncertainty\n    confidence, uncertainty = explainer.calculate_confidence_uncertainty(\n        X, y, predictive_model\n    )\n\n    # Prepare data for causal effect estimation\n    D_c_confidence = A.copy()\n    D_c_confidence[\"confidence\"] = confidence\n\n    D_c_uncertainty = A.copy()\n    D_c_uncertainty[\"uncertainty\"] = uncertainty\n\n    # Estimate causal effects\n    effects_confidence = explainer.estimate_causal_effects_on_continuous_outcomes(\n        D_c_confidence, outcome_name=\"confidence\"\n    )\n\n    effects_uncertainty = explainer.estimate_causal_effects_on_continuous_outcomes(\n        D_c_uncertainty, outcome_name=\"uncertainty\"\n    )\n\n    # Generate visualizations\n    explainer.plot_tornado(\n        effects_confidence, title=\"Causal Effects on Model Confidence\", figsize=(10, 8)\n    )\n\n    explainer.plot_tornado(\n        effects_uncertainty,\n        title=\"Causal Effects on Model Uncertainty\",\n        figsize=(10, 8),\n    )\n\n    # Extract and log concept meanings\n    selected_features_per_concept = explainer.extract_concept_meanings(\n        D, cluster_concepts, original_X\n    )\n    rprint(f\"\\nConcept feature details: {selected_features_per_concept}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user-guide/explainable_module/interventional_analysis/","title":"InterventionCausalExplainer: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435","text":""},{"location":"user-guide/explainable_module/interventional_analysis/#_1","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p><code>InterventionCausalExplainer</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u044b\u0432\u043e\u0434\u0430 \u0434\u043b\u044f \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u044f\u043c\u043e\u0435 \u0432\u043c\u0435\u0448\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e, \u0430 \u043d\u0435 \u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438. \u042d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u0442 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441 \u0442\u0435\u0445\u043d\u0438\u043a\u0430\u043c\u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u044b\u0432\u043e\u0434\u0430, \u0447\u0442\u043e\u0431\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043a\u0430\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438.</p>"},{"location":"user-guide/explainable_module/interventional_analysis/#_2","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/explainable_module/interventional_analysis/#_3","title":"\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u043b\u0435\u0441 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u044d\u0444\u0444\u0435\u043a\u0442\u0430 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f","text":"<p>\u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a Double Machine Learning (DML) \u0441 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u043b\u0435\u0441\u0430\u043c\u0438. \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \\(X_j\\) \u043c\u044b \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u0435\u0433\u043e \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u044d\u0444\u0444\u0435\u043a\u0442 \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438:</p> \\[\\tau(X_j) = \\mathbb{E}[C | do(X_j = x)] - \\mathbb{E}[C]\\] <p>\u0433\u0434\u0435 \\(C\\) \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438, \u0430 \\(do(X_j = x)\\) \u2014 \u044d\u0442\u043e do-\u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440 \u041f\u0438\u0440\u043b\u0430, \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u043d\u0430 \u0432\u043c\u0435\u0448\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e. \u041f\u043e\u0434\u0445\u043e\u0434 DML \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043e\u0440\u0442\u043e\u0433\u043e\u043d\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u0443\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u044e\u0449\u0438\u0445 \u044d\u0444\u0444\u0435\u043a\u0442\u043e\u0432:</p> \\[\\tau(X_j) = \\mathbb{E}[C - \\mathbb{E}[C|X_{-j}] | X_j = x] - \\mathbb{E}[X_j - \\mathbb{E}[X_j|X_{-j}]]\\]"},{"location":"user-guide/explainable_module/interventional_analysis/#_4","title":"\u041a\u0432\u0430\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0430\u043b\u0435\u0430\u0442\u043e\u0440\u043d\u043e\u0439 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438","text":"<p>\u041c\u043e\u0434\u0443\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 Data-IQ \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0430\u043b\u0435\u0430\u0442\u043e\u0440\u043d\u043e\u0439 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438. \u0410\u043b\u0435\u0430\u0442\u043e\u0440\u043d\u0430\u044f \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0448\u0443\u043c \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a:</p> \\[U_A = \\mathbb{E}[\\text{Var}(Y|X)]\\] <p>\u0433\u0434\u0435 \\(Y\\) \u2014 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f, \u0430 \\(X\\) \u2014 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438.</p>"},{"location":"user-guide/explainable_module/interventional_analysis/#_5","title":"\u0410\u043d\u0430\u043b\u0438\u0437 \u0432\u043c\u0435\u0448\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432","text":"<p>\u041c\u043e\u0434\u0443\u043b\u044c \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043f\u0440\u044f\u043c\u044b\u0435 \u0432\u043c\u0435\u0448\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430 \u043f\u0443\u0442\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u0432\u044b\u0441\u043e\u043a\u0438\u043c \u0432\u043b\u0438\u044f\u043d\u0438\u0435\u043c \u0438\u0437 \u0438\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f:</p> \\[X_j^{new} \\sim \\text{Uniform}(\\min(X_j), \\max(X_j))\\] <p>\u042d\u0442\u043e \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u043a\u043e\u043d\u0442\u0440\u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043e \u0438 \u043f\u043e\u0441\u043b\u0435, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044f \u0431\u043e\u043b\u0435\u0435 \u043d\u0430\u0434\u0435\u0436\u043d\u043e\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0447\u0435\u043c \u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b.</p>"},{"location":"user-guide/explainable_module/interventional_analysis/#_6","title":"\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f","text":"<p>\u042d\u0442\u043e\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0446\u0435\u043d\u0435\u043d \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0445 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0441 \u0432\u044b\u0441\u043e\u043a\u0438\u043c\u0438 \u0441\u0442\u0430\u0432\u043a\u0430\u043c\u0438, \u0433\u0434\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u0432\u044f\u0437\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u0438 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0432\u0430\u0436\u043d\u044b\u043c. \u0412\u044b\u044f\u0432\u043b\u044f\u044f, \u043a\u0430\u043a\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438, \u043e\u043d \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u0446\u0435\u043b\u0435\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0441\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438.</p>"},{"location":"user-guide/explainable_module/interventional_analysis/#_7","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/explainable/intervention_explainer.py<pre><code>import logging\n\nimport pandas as pd\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n)\n\nfrom applybn.explainable.causal_analysis import InterventionCausalExplainer\n\n\n# Example data loading function (replace with actual data)\ndef load_data():\n    from sklearn.datasets import load_breast_cancer\n    from sklearn.model_selection import train_test_split\n\n    data = load_breast_cancer()\n    X = pd.DataFrame(data.data, columns=data.feature_names)\n    y = pd.Series(data.target)\n    return train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Load data\nX_train, X_test, y_train, y_test = load_data()\n\n# Initialize and run ModelInterpreter\ninterpreter = InterventionCausalExplainer()\ninterpreter.interpret(RandomForestClassifier(), X_train, y_train, X_test, y_test)\n</code></pre>"},{"location":"user-guide/feature_extraction/feature_extraction/","title":"BNFeatureGenerator: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435","text":""},{"location":"user-guide/feature_extraction/feature_extraction/#_1","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p><code>BNFeatureGenerator</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043d\u043e\u0432\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u0438\u043d\u0436\u0438\u043d\u0438\u0440\u0438\u043d\u0433\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439, \u043a\u0430\u043a \u043e\u043f\u0438\u0441\u0430\u043d\u043e \u0432 \u0441\u0442\u0430\u0442\u044c\u0435 \"Bayesian feature construction for the improvement of classification performance\" \u041c\u0430\u043d\u043e\u043b\u0438\u0441\u0430 \u041c\u0430\u0440\u0430\u0433\u0443\u0434\u0430\u043a\u0438\u0441\u0430. \u042d\u0442\u043e\u0442 \u043c\u043e\u0434\u0443\u043b\u044c \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u043b\u044f\u043c\u0431\u0434\u0430-\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0438\u0437 \u0432\u044b\u0432\u043e\u0434\u0430 \u0432 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438, \u0447\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 \u0438 \u043c\u043e\u0436\u0435\u0442 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438.</p>"},{"location":"user-guide/feature_extraction/feature_extraction/#_2","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/feature_extraction/feature_extraction/#_3","title":"\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0435\u0442\u0438","text":"<p>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u0448\u0430\u0433\u043e\u0432:</p> <ol> <li> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435: \u041e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u0430\u0446\u0438\u043a\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0433\u0440\u0430\u0444 (DAG) \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0445 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438.</p> </li> <li> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c: \u041c\u043e\u0434\u0443\u043b\u044c \u043f\u043e\u0434\u0431\u0438\u0440\u0430\u0435\u0442 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0443\u0437\u043b\u0430.</p> </li> </ol>"},{"location":"user-guide/feature_extraction/feature_extraction/#_4","title":"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0447\u0435\u0440\u0435\u0437 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0439 \u0432\u044b\u0432\u043e\u0434","text":"<p>\u041f\u0440\u043e\u0446\u0435\u0441\u0441 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u043b\u044f\u043c\u0431\u0434\u0430-\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> <ol> <li>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \\(X_i\\) \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u043b\u044f\u043c\u0431\u0434\u0430-\u043f\u0440\u0438\u0437\u043d\u0430\u043a \\(\\lambda_i\\).</li> <li>\u0414\u043b\u044f \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: \\(\\lambda_i = P(X_i = x_i | Pa(X_i) = pa_i)\\)</li> <li>\u0414\u043b\u044f \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: \\(\\lambda_i = P(X_i \\leq x_i | Pa(X_i) = pa_i)\\)</li> </ol> <p>\u042d\u0442\u0438 \u043b\u044f\u043c\u0431\u0434\u0430-\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u044e\u0442 \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u043c\u043e\u0433\u0443\u0442 \u0432\u044b\u0440\u0430\u0437\u0438\u0442\u044c, \u043a\u043e\u0434\u0438\u0440\u0443\u044f \u0443\u0441\u043b\u043e\u0432\u043d\u0443\u044e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0435\u043c\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0443\u0437\u043b\u0430\u0445.</p>"},{"location":"user-guide/feature_extraction/feature_extraction/#_5","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"<p>\u0421\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442, \u043a\u0430\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c BNFeatureGenerator \u043d\u0430 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0430\u0443\u0442\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0431\u0430\u043d\u043a\u043d\u043e\u0442:</p> examples/feature_extraction/banknote-authentication_example.py<pre><code>import pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom applybn.feature_extraction.bn_feature_extractor import BNFeatureGenerator\nimport ssl\nfrom sklearn.linear_model import LogisticRegression\n\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Load the banknote-authentication dataset\nprint(\"Loading banknote-authentication dataset...\")\ndata = fetch_openml(name=\"banknote-authentication\", version=1, as_frame=True)\nX = pd.DataFrame(data.data)\ny = pd.Series(data.target, name=\"target\")\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Reset indices to ensure proper alignment\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n\n# First approach: Using only original features\nprint(\"\\n1. Training Decision Tree with original features...\")\ndt_original = DecisionTreeClassifier(random_state=42)\ndt_original.fit(X_train, y_train)\ny_pred_original = dt_original.predict(X_test)\n\n# Second approach: Using Bayesian Network features\nprint(\"\\n2. Generating Bayesian Network features...\")\nbn_feature_generator = BNFeatureGenerator()\nbn_feature_generator.fit(X=X_train, y=y_train)  # Fit the generator with training data\n\n# Transform both training and testing data\nX_train_bn = bn_feature_generator.transform(X_train).reset_index(drop=True)\nX_test_bn = bn_feature_generator.transform(X_test).reset_index(drop=True)\n\n\n# Train with combined features\nprint(\"\\nTraining Decision Tree with combined features...\")\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train_bn, y_train)\ny_pred = dt.predict(X_test_bn)\nprint(\"\\nClassification Report with Original Features:\")\nprint(classification_report(y_test, y_pred_original))\nprint(\"\\nClassification Report with Combined Features:\")\nprint(classification_report(y_test, y_pred))\n</code></pre> <p>BNFeatureGenerator \u043f\u043e\u0432\u044b\u0448\u0430\u0435\u0442 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438, \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u044f \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0447\u0442\u043e \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u0430\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u044b, \u043a\u0430\u043a \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043e \u0432 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0432\u044b\u0448\u0435. </p>"},{"location":"user-guide/feature_selection/causal_feature_selection/","title":"CausalFeatureSelector","text":""},{"location":"user-guide/feature_selection/causal_feature_selection/#_1","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u041a\u043b\u0430\u0441\u0441 <code>CausalFeatureSelector</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043c\u0435\u0442\u043e\u0434 \u043e\u0442\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0432\u0434\u043e\u0445\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438, \u0434\u043b\u044f \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441\u043e \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u043c \u0432\u043b\u0438\u044f\u043d\u0438\u0435\u043c \u043d\u0430 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e. \u042d\u0442\u043e\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u043e\u0441\u043d\u043e\u0432\u0430\u043d \u043d\u0430 \u043c\u0435\u0442\u043e\u0434\u043e\u043b\u043e\u0433\u0438\u0438, \u043e\u043f\u0438\u0441\u0430\u043d\u043d\u043e\u0439 \u0432 \u0441\u0442\u0430\u0442\u044c\u0435 \"A Causal Model-Inspired Automatic Feature-Selection Method for Developing Data-Driven Soft Sensors in Complex Industrial Processes\". \u0412\u043c\u0435\u0441\u0442\u043e \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0430\u0433\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044e, \u043e\u043d \u043e\u0442\u0431\u0438\u0440\u0430\u0435\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u044f \u0438\u0445 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u0447\u0435\u0440\u0435\u0437 \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044f \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u044b\u0435 \u0438 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0435 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</p>"},{"location":"user-guide/feature_selection/causal_feature_selection/#_2","title":"\u041a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438","text":"<ul> <li>\u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0430: \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438\u0445 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u0441\u043d\u0438\u0436\u0430\u0442\u044c \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439.</li> <li>\u0414\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e IQR: \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442 \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043d\u0430 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u044b \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u043c\u0435\u0436\u043a\u0432\u0430\u0440\u0442\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0430\u0445\u0430 (IQR).</li> <li>\u0418\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044f \u0441\u043e Scikit-Learn: \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c \u0441 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u0430\u043c\u0438 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438 scikit-learn.</li> </ul>"},{"location":"user-guide/feature_selection/causal_feature_selection/#_3","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435","text":""},{"location":"user-guide/feature_selection/causal_feature_selection/#_4","title":"\u0414\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0430\u0446\u0438\u044f","text":"<p>\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 IQR. \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u043e\u0432 <code>n_bins</code> \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a:</p> \\[ n_{\\text{bins}} = \\max\\left(2, \\left\\lceil \\frac{R}{2 \\cdot \\text{IQR} \\cdot n^{1/3}} \\cdot \\log_2(n + 1) \\right\\rceil \\right) \\] <p>\u0433\u0434\u0435 \\( R \\) \u2014 \u044d\u0442\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0434\u0430\u043d\u043d\u044b\u0445, \\( \\text{IQR} \\) \u2014 \u043c\u0435\u0436\u043a\u0432\u0430\u0440\u0442\u0438\u043b\u044c\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0430\u0445, \u0430 \\( n \\) \u2014 \u0440\u0430\u0437\u043c\u0435\u0440 \u0432\u044b\u0431\u043e\u0440\u043a\u0438.</p>"},{"location":"user-guide/feature_selection/causal_feature_selection/#_5","title":"\u0420\u0430\u0441\u0447\u0435\u0442 \u043a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0430","text":"<p>\u041a\u0430\u0443\u0437\u0430\u043b\u044c\u043d\u044b\u0439 \u044d\u0444\u0444\u0435\u043a\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \\( X_i \\) \u043d\u0430 \u0446\u0435\u043b\u044c \\( Y \\) \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u0435 \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u0439 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438:</p> \\[\\text{CE}(X_i \\rightarrow Y) = H(Y \\mid \\text{\u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438}) - H(Y \\mid X_i, \\text{\u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438})\\] <p>\u0433\u0434\u0435 \\( H \\) \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u044e. \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441 \\( \\text{CE} &gt; 0 \\).</p>"},{"location":"user-guide/feature_selection/causal_feature_selection/#_6","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/feature_selection/causal_fs_example.py<pre><code># Example: Causal Feature Selection on High-Dimensional Synthetic Data\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom applybn.feature_selection.ce_feature_selector import CausalFeatureSelector\n\n# Generate synthetic dataset with 1000 features (50 informative, 950 noise)\nX, y = make_classification(\n    n_samples=2000, n_features=50, n_informative=5, n_redundant=10\n)\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Initialize causal feature selector\ncausal_selector = CausalFeatureSelector(n_bins=10)\n\n# Fit on training data and transform features\ncausal_selector.fit(X_train, y_train)\nX_train_selected = causal_selector.transform(X_train)\nX_test_selected = causal_selector.transform(X_test)\n\n# Verify feature reduction\nprint(f\"Original features: {X_train.shape[1]}\")\nprint(f\"Selected features: {X_train_selected.shape[1]}\")\n\n# Train classifier on full features\nclf_full = LogisticRegression(max_iter=1000, random_state=42)\nclf_full.fit(X_train, y_train)\ny_pred_full = clf_full.predict(X_test)\naccuracy_full = accuracy_score(y_test, y_pred_full)\n\n# Train classifier on causal-selected features\nclf_selected = LogisticRegression(max_iter=1000, random_state=42)\nclf_selected.fit(X_train_selected, y_train)\ny_pred_selected = clf_selected.predict(X_test_selected)\naccuracy_selected = accuracy_score(y_test, y_pred_selected)\n\n# Compare performance\nprint(f\"\\nAccuracy with all features: {accuracy_full:.4f}\")\nprint(f\"Accuracy with causal features: {accuracy_selected:.4f}\")\n\n# Show mask of selected features\nprint(\"\\nSelected feature indices:\", np.where(causal_selector.support_)[0])\n</code></pre>"},{"location":"user-guide/feature_selection/nmi_feature_selection/","title":"NMIFeatureSelector: \u041e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0432\u0437\u0430\u0438\u043c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438","text":""},{"location":"user-guide/feature_selection/nmi_feature_selection/#_1","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p>\u041a\u043b\u0430\u0441\u0441 <code>NMIFeatureSelector</code> \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u0443\u0442\u0435\u043c \u043e\u0446\u0435\u043d\u043a\u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0432\u0437\u0430\u0438\u043c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 (NMI) \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u0438 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0441\u0442\u0430\u0442\u044c\u0438  <code>Local Bayesian Network Structure Learning for High-Dimensional Data</code>. \u042d\u0442\u043e\u0442 \u043c\u0435\u0442\u043e\u0434 \u0432\u044b\u044f\u0432\u043b\u044f\u0435\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441 \u0441\u0438\u043b\u044c\u043d\u044b\u043c\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044f\u043c\u0438 \u043e\u0442 \u0446\u0435\u043b\u0438, \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u044f \u0438\u0437\u0431\u044b\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0441\u0440\u0435\u0434\u0438 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \u041e\u043d \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u0435\u043d \u0434\u043b\u044f \u0443\u043b\u0430\u0432\u043b\u0438\u0432\u0430\u043d\u0438\u044f \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0435\u0439 \u0438 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c \u0441 API <code>SelectorMixin</code> \u0438\u0437 scikit-learn.</p>"},{"location":"user-guide/feature_selection/nmi_feature_selection/#_2","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/feature_selection/nmi_feature_selection/#nmi","title":"\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u0432\u0437\u0430\u0438\u043c\u043d\u0430\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f (NMI)","text":"<p>NMI \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043c\u0435\u0436\u0434\u0443 \u0434\u0432\u0443\u043c\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438, \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u0443\u044e \u0432 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435 [0, 1]. \u0414\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \\(a\\) \u0438 \\(b\\) NMI \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u0430\u043a:</p> \\[ \\text{NMI}(a, b) = \\frac{H(a) + H(b) - H(a, b)}{\\min(H(a), H(b))} \\] <p>\u0433\u0434\u0435:</p> <ul> <li>\\(H(a)\\) \u0438 \\(H(b)\\) \u2014 \u044d\u0442\u043e \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438 \\(a\\) \u0438 \\(b\\),</li> <li>\\(H(a, b)\\) \u2014 \u0438\u0445 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u0430\u044f \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u044f.</li> </ul> <p>\u0411\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f NMI \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u0441\u0438\u043b\u044c\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438.</p>"},{"location":"user-guide/feature_selection/nmi_feature_selection/#_3","title":"\u0414\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","text":"<p>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438:</p> <ul> <li>\u0426\u0435\u043b\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435/\u0441\u0442\u0440\u043e\u043a\u043e\u0432\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b: \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441 \u043f\u0435\u0440\u0435\u0447\u0438\u0441\u043b\u0438\u043c\u044b\u043c\u0438 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\u043c\u0438.</li> <li>\u0421\u0442\u043e\u043b\u0431\u0446\u044b \u0441 \u043f\u043b\u0430\u0432\u0430\u044e\u0449\u0435\u0439 \u0442\u043e\u0447\u043a\u043e\u0439: \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0439 \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0441 \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0431\u0438\u043d\u043e\u0432.</li> </ul>"},{"location":"user-guide/feature_selection/nmi_feature_selection/#_4","title":"\u041f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u0442\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","text":"<ol> <li> <p>\u041f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f:</p> <ul> <li>\u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c NMI \u043c\u0435\u0436\u0434\u0443 \u043a\u0430\u0436\u0434\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c \u0438 \u0446\u0435\u043b\u044c\u044e.</li> <li>\u0412\u044b\u0431\u0440\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441 NMI &gt; <code>threshold</code>.</li> </ul> </li> <li> <p>\u0412\u0442\u043e\u0440\u043e\u0439 \u044d\u0442\u0430\u043f:</p> <ul> <li>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043f\u0430\u0440\u044b \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u043f\u043e\u043f\u0430\u0440\u043d\u043e\u0435 NMI.</li> <li>\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a \\(f_j\\), \u0435\u0441\u043b\u0438:<ul> <li>\u0414\u0440\u0443\u0433\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \\(f_i\\) \u0438\u043c\u0435\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u043e\u0435 NMI \u0441 \u0446\u0435\u043b\u044c\u044e, \u0438</li> <li>NMI(\\(f_i\\), \\(f_j\\)) &gt; NMI(\\(f_j\\), \u0446\u0435\u043b\u044c).</li> </ul> </li> </ul> </li> </ol> <p>\u042d\u0442\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442 \u0438\u0437\u0431\u044b\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c, \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0438\u0437\u0438\u0440\u0443\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0431\u043e\u043b\u0435\u0435 \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0435 \u0434\u043b\u044f \u0446\u0435\u043b\u0438.</p>"},{"location":"user-guide/feature_selection/nmi_feature_selection/#_5","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/feature_selection/nmi_fs_example.py<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom applybn.feature_selection.bn_nmi_feature_selector import NMIFeatureSelector\n\n# Load dataset\ncancer = load_breast_cancer()\nX = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ny = cancer.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Initialize and apply feature selector\nselector = NMIFeatureSelector(threshold=0.5, n_bins=20)\nselector.fit(X_train, y_train)  # Fixed version captures feature names before conversion\n\n# Transform datasets\nX_train_selected = selector.transform(X_train)\nX_test_selected = selector.transform(X_test)\n\n# Get selected feature names\nselected_features = selector.feature_names_in_[selector.selected_features_]\nprint(f\"Selected features ({len(selected_features)}):\\n{selected_features}\")\n\n# Train classifier and compare results\nclf_full = RandomForestClassifier(random_state=42)\nclf_full.fit(X_train, y_train)\nacc_full = clf_full.score(X_test, y_test)\n\nclf_selected = RandomForestClassifier(random_state=42)\nclf_selected.fit(X_train_selected, y_train)\nacc_selected = clf_selected.score(X_test_selected, y_test)\n\nprint(f\"\\nOriginal features: {X_train.shape[1]}\")\nprint(f\"Selected features: {X_train_selected.shape[1]}\")\nprint(f\"Accuracy with all features: {acc_full:.4f}\")\nprint(f\"Accuracy with selected features: {acc_selected:.4f}\")\n</code></pre>"},{"location":"user-guide/oversampling_module/bn_oversampling/","title":"\u0420\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439","text":""},{"location":"user-guide/oversampling_module/bn_oversampling/#_2","title":"\u041e\u0431\u0437\u043e\u0440","text":"<p><code>BNOverSampler</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0445 \u0441\u0435\u0442\u0435\u0439 \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043f\u0443\u0442\u0435\u043c \u0438\u0437\u0443\u0447\u0435\u043d\u0438\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u043c\u0435\u0441\u0435\u0439 \u0413\u0430\u0443\u0441\u0441\u0438\u0430\u043d\u043e\u0432 \u0438 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a. \u042d\u0442\u043e\u0442 \u043c\u0435\u0442\u043e\u0434 \u0443\u043b\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u0438 \u043c\u0443\u043b\u044c\u0442\u0438\u043c\u043e\u0434\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f, \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0441\u0442\u0440\u043e\u0433\u0438\u0439 \u0440\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433.</p>"},{"location":"user-guide/oversampling_module/bn_oversampling/#_3","title":"\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0441\u043d\u043e\u0432\u0430","text":""},{"location":"user-guide/oversampling_module/bn_oversampling/#_4","title":"\u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0430\u044f \u0441\u0435\u0442\u044c \u0441\u043e \u0441\u043c\u0435\u0448\u0430\u043d\u043d\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438","text":"<p>\u0420\u0435\u0441\u0435\u043c\u043f\u043b\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0431\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u0443\u044e \u0441\u0435\u0442\u044c (\u0411\u0421), \u0433\u0434\u0435 \u0443\u0437\u043b\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0430 \u0440\u0435\u0431\u0440\u0430 \u043a\u043e\u0434\u0438\u0440\u0443\u044e\u0442 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438. \u0414\u043b\u044f \u0433\u0438\u0431\u043a\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439:</p> <ol> <li>\u041d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0441\u043c\u0435\u0441\u0435\u0439 \u0433\u0430\u0443\u0441\u0441\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 (GMM):</li> </ol> <p>\\( P(X_i | \\text{Pa}(X_i)) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(X_i | \\mu_k, \\Sigma_k) \\)</p> <p>\u0433\u0434\u0435:</p> <ul> <li>\\(\\pi_k\\): \u0432\u0435\u0441\u0430 \u0441\u043c\u0435\u0441\u0438 (\\(\\sum \\pi_k = 1\\))</li> <li>\\(\\mu_k, \\Sigma_k\\): \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438 \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u044f \\(k\\)-\u0433\u043e \u0433\u0430\u0443\u0441\u0441\u043e\u0432\u0430 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430</li> <li>\\(\\text{Pa}(X_i)\\): \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0435 \u0443\u0437\u043b\u044b \\(X_i\\) \u0432 \u0411\u0421</li> </ul> <p>\u00a02. \u0414\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u043c\u0443\u043b\u044c\u0442\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f, \u043e\u0431\u0443\u0441\u043b\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044f\u043c\u0438 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0443\u0437\u043b\u043e\u0432.</p>"},{"location":"user-guide/oversampling_module/bn_oversampling/#_5","title":"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","text":"<p>\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0411\u0421 \u043e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e:</p> <ul> <li>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435: \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \"\u0432\u043e\u0441\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435\" (Hill Climbing)</li> <li>\u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432: \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \"\u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435-\u043c\u0430\u043a\u0441\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f\" (EM) \u0434\u043b\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442 GMM:   \\( \\theta^* = \\arg\\max_\\theta \\mathbb{E}_{Z|X,\\theta^{old}}[\\log P(X,Z|\\theta)] \\)   \u0433\u0434\u0435 \\(Z\\) \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043a\u0440\u044b\u0442\u044b\u0435 \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442 \u0441\u043c\u0435\u0441\u0438.</li> </ul>"},{"location":"user-guide/oversampling_module/bn_oversampling/#_6","title":"\u0421\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f","text":"<p>\u0414\u043b\u044f \u043c\u0438\u043d\u043e\u0440\u0438\u0442\u0430\u0440\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \\(C\\) \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u0438\u0437 \u0443\u0441\u043b\u043e\u0432\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f: \\( P(\\mathbf{X} | C) = \\prod_{i=1}^d P(X_i | \\text{Pa}(X_i), C) \\) \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0441\u0435\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e \u043f\u0440\u0435\u0434\u043a\u0430\u043c \u0447\u0435\u0440\u0435\u0437 \u0411\u0421, \u0441 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0441\u0432\u0438\u0434\u0435\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430\u043c\u0438 \u0434\u043b\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430.</p>"},{"location":"user-guide/oversampling_module/bn_oversampling/#_7","title":"\u041a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438","text":"<ul> <li>\u041c\u0443\u043b\u044c\u0442\u0438\u043c\u043e\u0434\u0430\u043b\u044c\u043d\u043e\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435: GMM \u0443\u043b\u0430\u0432\u043b\u0438\u0432\u0430\u044e\u0442 \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0438\u043a\u043e\u0432 \u0432 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432)</li> <li>\u0423\u0441\u043b\u043e\u0432\u043d\u043e\u0435 \u0441\u0435\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435: \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u0442 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438, \u0447\u0435\u0440\u0435\u0437 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u0411\u0421</li> <li>\u0413\u0438\u0431\u0440\u0438\u0434 \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u043d\u044b\u0445 \u0438 \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445: \u043d\u0430\u0442\u0438\u0432\u043d\u043e \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u0441\u043c\u0435\u0448\u0430\u043d\u043d\u044b\u0435 \u0442\u0438\u043f\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0447\u0435\u0440\u0435\u0437:<ul> <li>\u041a\u0432\u0430\u043d\u0442\u0438\u043b\u044c\u043d\u0443\u044e \u0434\u0438\u0441\u043a\u0440\u0435\u0442\u0438\u0437\u0430\u0446\u0438\u044e (\u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430)</li> <li>\u0421\u0435\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0442\u0438\u043f\u0430 (\u0441\u0442\u043e\u043b\u0431\u0446\u044b disc_num \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u044e\u0442\u0441\u044f \u0432 \u0446\u0435\u043b\u044b\u0435 \u0447\u0438\u0441\u043b\u0430 \u043f\u043e\u0441\u043b\u0435 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438)</li> </ul> </li> </ul>"},{"location":"user-guide/oversampling_module/bn_oversampling/#_8","title":"\u041f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0430 \u043f\u0435\u0440\u0435\u0434 \u0442\u0440\u0430\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 \u043c\u0435\u0442\u043e\u0434\u0430\u043c\u0438","text":"<ol> <li>\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438</li> <li>\u0418\u0437\u0431\u0435\u0433\u0430\u0435\u0442 \u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442\u043e\u0432 \u0438\u043d\u0442\u0435\u0440\u043f\u043e\u043b\u044f\u0446\u0438\u0438 (\u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u043d\u044b\u0445 \u0434\u043b\u044f SMOTE) \u0437\u0430 \u0441\u0447\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0433\u043e \u0441\u0435\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</li> <li>\u0410\u0434\u0430\u043f\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043a \u0433\u0435\u0442\u0435\u0440\u043e\u0433\u0435\u043d\u043d\u044b\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f\u043c \u0447\u0435\u0440\u0435\u0437 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b \u0441\u043c\u0435\u0441\u0438</li> </ol>"},{"location":"user-guide/oversampling_module/bn_oversampling/#_9","title":"\u041f\u0440\u0438\u043c\u0435\u0440","text":"examples/imbalanced/iris_example.py<pre><code>\"\"\"Bayesian Network Oversampling Example with Iris Dataset.\n\nThis example demonstrates using BNOverSampler to address class imbalance while preserving feature\nrelationships through Bayesian network modeling. Includes artificial imbalance creation, resampling,\nand logging integration.\n\nAttributes:\n    logger: Configured logger instance.\n    oversampler: Bayesian network-based resampler.\n    X_res: Resampled feature matrix.\n    y_res: Balanced target vector.\n\nNotes:\n    - Requires applybn package components\n    - Uses sklearn's Iris dataset as base data\n    - Implements custom logging configuration\n    - Preserves original data structure and types\n\"\"\"\n\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport logging\nfrom applybn.imbalanced.over_sampling.bn_over_sampler import BNOverSampler\nfrom applybn.core.logger import Logger\n\n\ndef create_imbalanced_data(\n    X: pd.DataFrame, y: pd.DataFrame, ratios: dict\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create artificial class imbalance by subsampling majority classes\n\n    Args:\n        X: Original feature matrix\n        y: Original target vector\n        ratios: Dictionary mapping class labels to desired sample counts\n\n    Returns:\n        tuple with specified class distribution\n\n    \"\"\"\n    imbalanced_X = pd.DataFrame()\n    imbalanced_y = pd.Series(dtype=y.dtype)\n\n    for cls, n_samples in ratios.items():\n        cls_X = X[y == cls].sample(n=n_samples, random_state=42)\n        cls_y = y[y == cls].sample(n=n_samples, random_state=42)\n\n        imbalanced_X = pd.concat([imbalanced_X, cls_X], ignore_index=True)\n        imbalanced_y = pd.concat([imbalanced_y, cls_y], ignore_index=True)\n\n    return imbalanced_X, imbalanced_y\n\n\nif __name__ == \"__main__\":\n    # Configure logging using updated Logger class\n    logger = Logger(\"bn_oversample_demo\", level=logging.INFO)\n    logger.info(\"Initializing BN oversampling demonstration\")\n\n    # Load and prepare data\n    iris = load_iris()\n    X = pd.DataFrame(\n        iris.data,\n        columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n    )\n    y = pd.Series(iris.target, name=\"species\")\n    logger.debug(\"Loaded base Iris dataset with %d samples\", len(X))\n\n    # Create artificial imbalance\n    imbalance_ratios = {0: 5, 1: 10, 2: 30}  # Class: sample_count\n    imbalanced_X, imbalanced_y = create_imbalanced_data(X, y, imbalance_ratios)\n    logger.info(\n        \"Created imbalanced dataset:\\n%s\",\n        imbalanced_y.value_counts().sort_index().to_string(),\n    )\n\n    # Initialize and apply oversampler\n    oversampler = BNOverSampler(\n        class_column=\"species\", strategy=\"max_class\", shuffle=True\n    )\n    logger.debug(\"Initialized BNOverSampler with parameters: %s\", vars(oversampler))\n\n    # Perform resampling\n    try:\n        X_res, y_res = oversampler.fit_resample(imbalanced_X, imbalanced_y)\n        logger.info(\"Resampling completed successfully\")\n    except Exception as e:\n        logger.error(\"Resampling failed: %s\", str(e))\n        raise\n\n    # Log final results\n    logger.info(\n        \"Resampled class distribution:\\n%s\",\n        y_res.value_counts().sort_index().to_string(),\n    )\n    logger.info(\"Total samples after resampling: %d\", len(X_res))\n    logger.debug(\"Final feature matrix shape: %s\", str(X_res.shape))\n</code></pre>"}]}